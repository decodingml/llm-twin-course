[
    {
        "instruction": "Summarize in a Medium article the Full Stack 7-Steps MLOps Framework, providing a preview of what to expect from an end-to-end machine learning batch architecture course, including course lessons and a drill down of the batch architecture.",
        "post": "The Full Stack 7-Steps MLOps Framework | by Paul Iusztin | Towards AIOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyThe Full Stack 7-Steps MLOps FrameworkPreview: Everything you must know about an end-to-end machine learning batch architecturePaul Iusztin\u00b7FollowPublished inTowards AI\u00b714 min read\u00b7Jul 26, 20231621ListenSharePhoto by Hassan Pasha on UnsplashThis article represents an overview of a 7-lesson FREE course entitled \u201cThe Full Stack 7-Steps MLOps Framework\u201d that will walk you step-by-step through how to design, implement, train, deploy, and monitor an ML system using MLOps good practices.\u2192 If you don\u2019t want to do the course, this article also works as a standalone framework for an overall picture of an end-to-end ML batch architecture.During the course, you will build a production-ready end-to-end batch ML system to forecast energy consumption levels for the next 24 hours across multiple consumer types from Denmark.Nowadays, certificates are everywhere. Building advanced end-to-end projects that you can later show off is the best way to get recognition as a professional engineer.If you are busy and want to check out the curriculum of my free course quickly, this is the right place to be.Table of Contents:Course IntroductionCourse LessonsThe Batch ArchitectureThe 7-Steps Drill DownHow Can We Adapt the Batch Architecture to an Online ML System?ConclusionCourse IntroductionAt the end of this 7 lessons course, you will know how to:design a batch-serving architectureuse Hopsworks as a feature storedesign a feature engineering pipeline that reads data from an APIbuild a training pipeline with hyper-parameter tunninguse W&B as an ML Platform to track your experiments, models, and metadataimplement a batch prediction pipelineuse Poetry to build your own Python packagesdeploy your own private PyPi serverorchestrate everything with AirflowCreate an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0in1621621FollowWritten by Paul Iusztin1.5K Followers\u00b7Writer for Towards AI\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul Iusztin and Towards AIPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Ignacio de GregorioinTowards AIIs Mamba the End of ChatGPT As We Know It?The Great New Question\u00b78 min read\u00b7Jan 116.2K62IVAN ILINinTowards AIAdvanced RAG Techniques: an Illustrated OverviewA comprehensive study of the advanced retrieval augmented generation techniques and algorithms, systemising various approaches. The article\u202619 min read\u00b7Dec 17, 20234.3K27Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111See all from Paul IusztinSee all from Towards AIRecommended from MediumPaul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111SemaphoreMLOps: From Jupyter to ProductionJupyter notebooks are great for learning and running experiments on Machine Learning. They, however, fall short when it comes to\u20266 min read\u00b7Feb 127ListsPredictive Modeling w/ Python20 stories\u00b7891 savesPractical Guides to Machine Learning10 stories\u00b71032 savesNatural Language Processing1179 stories\u00b7656 savesThe New Chatbots: ChatGPT, Bard, and Beyond12 stories\u00b7301 savesDeepanshu tyagiMLOps 2024 Roadmap: From Zero to ProIn the rapidly evolving landscape of artificial intelligence and machine learning, the significance of operationalizing machine learning\u20264 min read\u00b7Jan 36034DatadriftersinDev GeniusDeploy XGBoost to Local Kubernetes Cluster using FastAPI: Movie Recommendation Use CaseEfficient deployment and scaling of Machine Learning models is critical for user experience in many real-world applications.\u00b711 min read\u00b7Oct 20, 2023761Moez AliEasily deploy machine learning models from the comfort of your NotebookStreamline your Machine Learning model deployment with Modelbit.com. Seamlessly move from Jupyter notebooks to production-ready REST APIs\u20269 min read\u00b7Sep 8, 20232621QwakinInferCI/CD for Machine Learning in 2024: Best Practices to Build, Test, and DeployExplore best practices for CI/CD in Machine Learning in 2024. Learn to build, test, and deploy ML models efficiently with expert\u202618 min read\u00b7Dec 27, 20231K9See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Discuss the top 6 ML Platform features that enhance a mature ML system in a LinkedIn post. Highlight experiment tracking, metadata storage, visualizations, reports, artifacts, and model registry as essential components.",
        "post": "\ud835\udde7\ud835\uddfc\ud835\uddfd \ud835\udff2 \ud835\udde0\ud835\udddf \ud835\udde3\ud835\uddf9\ud835\uddee\ud835\ude01\ud835\uddf3\ud835\uddfc\ud835\uddff\ud835\uddfa \ud835\uddf3\ud835\uddf2\ud835\uddee\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2\ud835\ude00 you must know and use in your ML system.\nHere they are \u2193\n#\ud835\udfed. \ud835\uddd8\ud835\ude05\ud835\uddfd\ud835\uddf2\ud835\uddff\ud835\uddf6\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01 \ud835\udde7\ud835\uddff\ud835\uddee\ud835\uddf0\ud835\uddf8\ud835\uddf6\ud835\uddfb\ud835\uddf4\nIn your ML development phase, you generate lots of experiments.\nTracking and comparing the metrics between them is crucial in finding the optimal model.\n#\ud835\udfee. \ud835\udde0\ud835\uddf2\ud835\ude01\ud835\uddee\ud835\uddf1\ud835\uddee\ud835\ude01\ud835\uddee \ud835\udde6\ud835\ude01\ud835\uddfc\ud835\uddff\ud835\uddf2\nIts primary purpose is reproducibility.\nTo know how a model was generated, you need to know:\n- the version of the code\n- the version of the packages\n- hyperparameters/config\n- total compute\n- version of the dataset\n... and more\n#\ud835\udfef. \ud835\udde9\ud835\uddf6\ud835\ude00\ud835\ude02\ud835\uddee\ud835\uddf9\ud835\uddf6\ud835\ude00\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb\ud835\ude00\nMost of the time, along with the metrics, you must log a set of visualizations for your experiment.\nSuch as:\n- images\n- videos\n- prompts\n- t-SNE graphs\n- 3D point clouds\n... and more\n#\ud835\udff0. \ud835\udde5\ud835\uddf2\ud835\uddfd\ud835\uddfc\ud835\uddff\ud835\ude01\ud835\ude00\nYou don't work in a vacuum.\nYou have to present your work to other colleges or clients.\nA report lets you take the metadata and visualizations from your experiment...\n...and create, deliver and share a targeted presentation for your clients or peers.\n#\ud835\udff1. \ud835\uddd4\ud835\uddff\ud835\ude01\ud835\uddf6\ud835\uddf3\ud835\uddee\ud835\uddf0\ud835\ude01\ud835\ude00\nThe most powerful feature out of them all.\nAn artifact is a versioned object that is an input or output for your task.\nEverything can be an artifact, but the most common cases are:\n- data\n- model\n- code\nWrapping your assets around an artifact ensures reproducibility.\nFor example, you wrap your features into an artifact (e.g., features:3.1.2), which you can consume into your ML development step.\nThe ML development step will generate config (e.g., config:1.2.4) and code (e.g., code:1.0.2) artifacts used in the continuous training pipeline.\nDoing so lets you quickly respond to questions such as \"What I used to generate the model?\" and \"What Version?\"\n#\ud835\udff2. \ud835\udde0\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddf9 \ud835\udde5\ud835\uddf2\ud835\uddf4\ud835\uddf6\ud835\ude00\ud835\ude01\ud835\uddff\ud835\ude06\nThe model registry is the ultimate way to make your model accessible to your production ecosystem.\nFor example, in your continuous training pipeline, after the model is trained, you load the weights as an artifact into the model registry (e.g., model:1.2.4).\nYou label this model as \"staging\" under a new version and prepare it for testing. If the tests pass, mark it as \"production\" under a new version and prepare it for deployment (e.g., model:2.1.5).\n.\nAll of these features are used in a mature ML system. What is your favorite one?\nYou can see all these features in action in my \ud835\udde7\ud835\uddf5\ud835\uddf2 \ud835\uddd9\ud835\ude02\ud835\uddf9\ud835\uddf9 \ud835\udde6\ud835\ude01\ud835\uddee\ud835\uddf0\ud835\uddf8 \ud835\udff3-\ud835\udde6\ud835\ude01\ud835\uddf2\ud835\uddfd\ud835\ude00 \ud835\udde0\ud835\udddf\ud835\udde2\ud835\uddfd\ud835\ude00 \ud835\uddd9\ud835\uddff\ud835\uddee\ud835\uddfa\ud835\uddf2\ud835\ude04\ud835\uddfc\ud835\uddff\ud835\uddf8 FREE course. Link in the comments \u2193\nhashtag\n#\nmlops\nhashtag\n#\nmachinelearning\nhashtag\n#\ndata\n-----\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps."
    },
    {
        "instruction": "Write a Medium article in which you outline the essential knowledge needed to build an MLOps end-to-end architecture, covering principles from ML development to continuous monitoring. Reference workflows used by leading companies and include a visual representation of an MLOps workflow.",
        "post": "This Is What You Need to Know to Build an MLOps End-To-End Architecture | by Paul Iusztin | MLearning.ai | MediumOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyThis Is What You Need to Know to Build an MLOps End-To-End ArchitecturePaul Iusztin\u00b7FollowPublished inMLearning.ai\u00b77 min read\u00b7Aug 11, 2022783ListenShare7 principles to quickly bring MLOps to your machine learning projects.The end-to-end MLOps workflow. Image by Author, inspired by source.MLOps is a new field that was born due to the high failure rate of most machine learning projects. By failure, we understand the inability to generalize your freshly cooked model and ship it into a production environment to bring value to a specific group of people. Don't get me wrong, there are many excellent machine learning projects/algorithms, but the main issue is that a few have passed the \"laboratory\" stage of development.As we all know, machine learning addresses problems that cannot be well specified programmatically. Machine Learning systems are entangled with various dependencies, such as the size of the dataset, the distribution of features within the dataset, data scaling and splitting techniques, the type of optimizer being used, etc.Using a standardized end-to-end MLOps framework, we can show our incredible models to the world and show people how impressive AI can be.\u201cReproducibility is a kindness to your future self and everyone else who might want to build upon your work, which is an excellent goal all on its own.\u201d\ud83c\udfc6 This article will summarize the core components used in almost any MLOps infrastructure. It is inspired by workflows used by companies such as Google Cloud and Comet.Table of ContentsML developmentTraining operationalizationContinuous trainingModel deploymentPrediction servingContinuous monitoringData & model management1\ufe0f\u20e3 ML developmentSummary: experimenting and developing a robust and reproducible model training procedure (training pipeline code).The experimentation process aims to understand how incremental changes affect the system. Keeping track of experimental metadata is the starting point for adopting a consistent and repeatable experiment process.Create an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0in78783FollowWritten by Paul Iusztin1.5K Followers\u00b7Writer for MLearning.ai\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul Iusztin and MLearning.aiPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Fabio MatricardiinMLearning.aiHow I Built a Chatbot that Crushed ChatGPT with Zero Cost AI ToolsChallenge Accepted! How I created a chatbot that surpassed the performance of the famous ChatGPT model using free and open source AI tools\u2026\u00b79 min read\u00b76 days ago1.4K5Maximilian VogelinMLearning.aiThe ChatGPT list of lists: A collection of 3000+ prompts, GPTs, use-cases, tools, APIs, extensions\u2026Updated Jan-11, 2024. Added GPT Store, new resources.11 min read\u00b7Feb 7, 202311.5K141Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111See all from Paul IusztinSee all from MLearning.aiRecommended from MediumPaul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111Nicolas PogeantinMLearning.aiBuilding a Scalable MLOps Stack\u200a\u2014\u200a From Data to DeploymentThis article explores the development of a streamlined ML system, showcasing the journey from data to deployment, simplifying complex\u2026\u00b75 min read\u00b7Sep 8, 202354ListsPredictive Modeling w/ Python20 stories\u00b7891 savesPractical Guides to Machine Learning10 stories\u00b71032 savesNatural Language Processing1179 stories\u00b7656 savesGeneral Coding Knowledge20 stories\u00b7905 savesAnkush k SingalinILLUMINATIONCrafting Quality Python Projects: A Unique Template for Success \u2014 MLOPS-IAnkush k Singal\u00b716 min read\u00b7Oct 25, 20231692Deepanshu tyagiMLOps 2024 Roadmap: From Zero to ProIn the rapidly evolving landscape of artificial intelligence and machine learning, the significance of operationalizing machine learning\u20264 min read\u00b7Jan 36034Moez AliEasily deploy machine learning models from the comfort of your NotebookStreamline your Machine Learning model deployment with Modelbit.com. Seamlessly move from Jupyter notebooks to production-ready REST APIs\u20269 min read\u00b7Sep 8, 20232621DatadriftersinDev GeniusDeploy XGBoost to Local Kubernetes Cluster using FastAPI: Movie Recommendation Use CaseEfficient deployment and scaling of Machine Learning models is critical for user experience in many real-world applications.\u00b711 min read\u00b7Oct 20, 2023761See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Explain why traditional k-fold cross-validation fails in finance due to non-IID data and serial correlation. Introduce in the Medium post concepts like purge and embargo as solutions to avoid lookahead bias and data leakage in financial machine learning models.",
        "post": "Why k-fold Cross-Validation is Failing in Finance? | by Paul Iusztin | MLearning.ai | MediumOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyWhy k-fold Cross-Validation is Failing in Finance?Purge and EmbargoPaul Iusztin\u00b7FollowPublished inMLearning.ai\u00b74 min read\u00b7Jan 22, 20221381ListenSharePhoto by Shubham Dhage on UnsplashThe method is described from Advances in Financial Machine Learning by Marcos L\u00f3pez de Prado.What is Cross-Validation?When we want to train a Machine Learning model usually we have to split the dataset into three parts:trainvalidationtestFor the sake of the example let\u2019s say that we take 20% of the data and put it in the test split. Now we are left with 80% which we have to split into train and validation. The easiest way to go would be just to take another 20% into the validation split and train with 60% of the data. There are two reasons why this would not work:We have too little dataWe could overfit the hyper-parameters on the validation split.Cross-validation is trying to solve those problems. The basic approach is called k-fold CV. The algorithm consists in partitioning the train-validation split into k groups. Afterward, take the first group as the validation split and the rest of k-1 as the training split. We repeat this process until every group has been used as a validation split. For our final metrics, take all the k results and average them together. This method is more computationally intensive, but it helps the algorithm to generalize well even with small amounts of data. In the figure below we can see the visual representation of a 5-Fold CV :5-Fold Cross-Validation Example; sourceNote: In some cases, we only have two splits: train and validation , but the same principles apply.Why does K-Fold Cross-Validation Fail in Finance?The k-fold method usually works with the standard data, but in price time series the main issue is that the observations are not independent and identically distributed ( IID). In simpler terms the features are serially correlated, therefore the labels are derived from overlapping data points.A feature from time t is highly correlated with the feature from\u2026Create an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0in1381381FollowWritten by Paul Iusztin1.5K Followers\u00b7Writer for MLearning.ai\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul Iusztin and MLearning.aiPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Fabio MatricardiinMLearning.aiHow I Built a Chatbot that Crushed ChatGPT with Zero Cost AI ToolsChallenge Accepted! How I created a chatbot that surpassed the performance of the famous ChatGPT model using free and open source AI tools\u2026\u00b79 min read\u00b76 days ago1.4K5Maximilian VogelinMLearning.aiThe ChatGPT list of lists: A collection of 3000+ prompts, GPTs, use-cases, tools, APIs, extensions\u2026Updated Jan-11, 2024. Added GPT Store, new resources.11 min read\u00b7Feb 7, 202311.5K141Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111See all from Paul IusztinSee all from MLearning.aiRecommended from MediumPaul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111Umar ZaibA Deep Dive into Optuna vs. Hyperopt for Hyperparameter Optimization ExcellenceMachine learning, with its relentless march into various domains, demands precision in model performance. At the heart of this precision\u20263 min read\u00b7Oct 15, 202311ListsPredictive Modeling w/ Python20 stories\u00b7891 savesPractical Guides to Machine Learning10 stories\u00b71032 savesNatural Language Processing1179 stories\u00b7656 savesdata science and AI40 stories\u00b767 savesDeniz Kenan KilicTuning Hyperparameters in Machine Learning Models 101A straightforward and important question\u202623 min read\u00b7Sep 8, 202382Niranjan AppajiA Guide to Handling High Cardinality in Categorical VariablesHigh cardinality refers to a situation in a dataset where a particular feature has a large number of distinct values. In other words, a\u20265 min read\u00b7Dec 28, 20232Ankush k SingalinILLUMINATIONCrafting Quality Python Projects: A Unique Template for Success \u2014 MLOPS-IAnkush k Singal\u00b716 min read\u00b7Oct 25, 20231692TheDataScienceProUnderstanding TimeSeriesSplit Cross-Validation for Time Series DataIntroduction2 min read\u00b7Dec 20, 2023See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Highlight the four pillars necessary to build production ML systems.",
        "post": "The \ud835\udff0 \ud835\uddfd\ud835\uddf6\ud835\uddf9\ud835\uddf9\ud835\uddee\ud835\uddff\ud835\ude00 to \ud835\uddef\ud835\ude02\ud835\uddf6\ud835\uddf9\ud835\uddf1 \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddf1\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\udde0\ud835\udddf \ud835\ude00\ud835\ude06\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfa\ud835\ude00 \u2193\n\nAs highlighted by Alex Vesa, before building a production-ready system, it is critical to go through a set of questions that will later decide the nature of your ML system architecture.\n\n\ud835\ude0f\ud835\ude26\ud835\ude33\ud835\ude26 \ud835\ude22\ud835\ude33\ud835\ude26 \ud835\ude35\ud835\ude29\ud835\ude26 4 \ud835\ude31\ud835\ude2a\ud835\ude2d\ud835\ude2d\ud835\ude22\ud835\ude33\ud835\ude34 \ud835\ude35\ud835\ude29\ud835\ude22\ud835\ude35 \ud835\ude3a\ud835\ude30\ud835\ude36 \ud835\ude22\ud835\ude2d\ud835\ude38\ud835\ude22\ud835\ude3a\ud835\ude34 \ud835\ude29\ud835\ude22\ud835\ude37\ud835\ude26 \ud835\ude35\ud835\ude30 \ud835\ude24\ud835\ude30\ud835\ude2f\ud835\ude34\ud835\ude2a\ud835\ude25\ud835\ude26\ud835\ude33 \ud835\ude23\ud835\ude26\ud835\ude27\ud835\ude30\ud835\ude33\ud835\ude26 \ud835\ude25\ud835\ude26\ud835\ude34\ud835\ude2a\ud835\ude28\ud835\ude2f\ud835\ude2a\ud835\ude2f\ud835\ude28 \ud835\ude22\ud835\ude2f\ud835\ude3a \ud835\ude34\ud835\ude3a\ud835\ude34\ud835\ude35\ud835\ude26\ud835\ude2e \u2193\n\n\u2794 \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee\n\n- What data types do you have? (e.g., tabular data, images, text, etc.)\n- What does the data look like? (e.g., for text data, is it in a single language or multiple?)\n- How do you collect the data?\n- At what frequency do you have to collect the data?\n- How do you collect labels for the data? (crucial for how you plan to evaluate and monitor the model in production)\n\n\u2794 \ud835\udde7\ud835\uddf5\ud835\uddff\ud835\uddfc\ud835\ude02\ud835\uddf4\ud835\uddf5\ud835\uddfd\ud835\ude02\ud835\ude01\n\n- What are the throughput requirements? You must know at least the throughput's minimum, average, and maximum statistics.\n- How many requests the system must handle simultaneously? (1, 10, 1k, 1 million, etc.)\n\n\u2794 \ud835\udddf\ud835\uddee\ud835\ude01\ud835\uddf2\ud835\uddfb\ud835\uddf0\ud835\ude06\n\n- What are the latency requirements? (1 millisecond, 10 milliseconds, 1 second, etc.)\n- Throughput vs. latency trade-off\n- Accuracy vs. speed trade-off\n\n\u2794 \ud835\udddc\ud835\uddfb\ud835\uddf3\ud835\uddff\ud835\uddee\ud835\ude00\ud835\ude01\ud835\uddff\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2\n\n- Batch vs. real-time architecture (closely related to the throughput vs. latency trade-off)\n- How should the system scale? (e.g., based on CPU workload, # of requests, queue size, data size, etc.)\n- Cost requirements\n\n.\n\nDo you see how we shifted the focus from model performance towards how it is integrated into a more extensive system?\n\nWhen building production-ready ML, the model's accuracy is no longer the holy grail but a bullet point in a grander scheme.\n\n. \n\n\ud835\udde7\ud835\uddfc \ud835\ude00\ud835\ude02\ud835\uddfa\ud835\uddfa\ud835\uddee\ud835\uddff\ud835\uddf6\ud835\ude07\ud835\uddf2, the 4 pillars to keep in mind before designing an ML architecture are:\n- Data\n- Throughput\n- Latency\n- Infrastructure\n\nLet me know in the comments below if you want to dive deeper into each topic. \u21b4\n\nhashtag\n#machinelearning \nhashtag\n#mlops \nhashtag\n#datascience\n\n.\n\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps.\nVizualiza\u0163i traducerea\n"
    },
    {
        "instruction": "Create a light-hearted LinkedIn post with an amusing image, intended to engage the audience humorously.",
        "post": "Who fell for it raise your hand \ud83d\ude02\ud83d\ude02\ud83d\ude02"
    },
    {
        "instruction": "Explain in a Medium Article the Singular Value Decomposition (SVD) in a manner understandable to a young audience. Focus on the intuition behind the algorithm and its practical applications, such as recommendation systems, without delving into complex mathematics.",
        "post": "This Is How You Can Explain SVD to a 7-Year-Old | by Paul Iusztin | Towards AIOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyThis Is How You Can Explain SVD to a 7-Year-OldUnderstand the intuition behind the Singular Value Decomposition (SVD) algorithm.Paul Iusztin\u00b7FollowPublished inTowards AI\u00b76 min read\u00b7Sep 15, 202238ListenSharePhoto by Ben White on UnsplashI am writing this article because, at first, I struggled to understand the mathematical intuition behind the SVD algorithm. But if you throw away all the fancy math terms, the idea behind the algorithm is easy to understand.After reading this article, you will get an intuition about why and how to use SVD in various applications.I could throw the definition of SVD such as:In linear algebra, the singular value decomposition (SVD) is a factorization of a real or complex matrix. It generalizes the eigendecomposition of a square normal matrix with an orthonormal eigenbasis to any m x n matrix. [1]Hmm. What is that? This definition won\u2019t make any sense for a beginner or a person who isn\u2019t much in math. Many people are coming from software into machine learning engineering and are struggling to understand some of the advanced math concepts used in ML. At least I did.Don\u2019t get me wrong, to fully understand what SVD is, you need to understand the math behind it, but we focused in this article on getting the intuition on how it works and why we even care about SVD. We will not focus on the actual computation of the algorithm.Famous use cases for SVD are:Pseudoinverse \u2014 inverse for matrices of any size (the Moore-Penrose Inverse)Dimensionality Reduction (images, tabular data, etc.)Recommendation engines (the FunkSVD algorithm and its derivatives)The Intuition Behind SVDLet\u2019s reduce the problem to people and their favorite food. We assume we have a m x n matrix A, where m represents the number of people and n is the number of possible food types. The matrix values represent how much a person likes a food type, encoded as a rating from 1 to 10.Here is an example of a 3x4 matrix with three people and four food types:Create an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0in3838FollowWritten by Paul Iusztin1.5K Followers\u00b7Writer for Towards AI\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul Iusztin and Towards AIPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Ignacio de GregorioinTowards AIIs Mamba the End of ChatGPT As We Know It?The Great New Question\u00b78 min read\u00b7Jan 116.2K62IVAN ILINinTowards AIAdvanced RAG Techniques: an Illustrated OverviewA comprehensive study of the advanced retrieval augmented generation techniques and algorithms, systemising various approaches. The article\u202619 min read\u00b7Dec 17, 20234.3K27Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111See all from Paul IusztinSee all from Towards AIRecommended from MediumCristian LeoinTowards Data ScienceThe Math behind Adam OptimizerWhy is Adam the most popular optimizer in Deep Learning? Let\u2019s understand it by diving into its math, and recreating the algorithm.16 min read\u00b7Jan 301.3K9Paul IusztininDecoding MLThe ultimate guide on installing PyTorch with CUDA support in all possible ways\u2192 Using Pip, Conda, Poetry, Docker, or directly on the system\u00b710 min read\u00b7Jan 42231ListsPredictive Modeling w/ Python20 stories\u00b7891 savesNatural Language Processing1179 stories\u00b7656 savesAI Regulation6 stories\u00b7311 savesPractical Guides to Machine Learning10 stories\u00b71032 savesCoursesteachNatural Language Processing (Part 29)-Cosine Similarity\ud83d\udcdaChapter 3: Vector Space Model4 min read\u00b7Feb 453Everton Gomede, PhDin\ud835\udc00\ud835\udc08 \ud835\udc26\ud835\udc28\ud835\udc27\ud835\udc24\ud835\udc2c.\ud835\udc22\ud835\udc28Understanding the Causes of Overfitting: A Mathematical PerspectiveIntroduction6 min read\u00b7Jan 2731Nithin ManiChoosing Between Matrix Factorization and Neural Matrix Factorization: A Guide Based on Data\u2026Neural Matrix Factorization (NMF) is an extension of traditional matrix factorization methods that incorporates neural networks to capture\u20263 min read\u00b7Feb 261J RosserDemystifying GCNs: A Step-by-Step Guide to Building a Graph Convolutional Network Layer in PyTorchGraph Convolutional Networks (GCNs) are essential in GNNs. Understand the core concepts and create your GCN layer in PyTorch!6 min read\u00b7Jan 183155See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Compose a Medium article discussing the significance of fractionally differentiated features for maintaining memory in stationary time series and their application in financial machine learning. Highlight the method's importance in preprocessing data for ML trading bots, ensuring the data retains crucial historical information for accurate prediction models.",
        "post": "Fractionally Differentiated Features to Preserve Memory in Stationary Time Series | by Paul Iusztin | MLearning.ai | MediumOpen in appSign upSign inWriteSign upSign inMastodonTop highlightMember-only storyFractionally Differentiated Features to Preserve Memory in Stationary Time SeriesHow to Properly Preprocess Data for Machine Learning Trading BotsPaul Iusztin\u00b7FollowPublished inMLearning.ai\u00b79 min read\u00b7Jan 20, 20221401ListenSharePhoto by Dan-Cristian P\u0103dure\u021b on UnsplashThe method is described from Advances in Financial Machine Learning by Marcos L\u00f3pez de Prado.MotivationThe problem of stationarity is often found in time-series data. In this article, I want to give you an intuition on the problem and how to solve it properly without losing you in too many details. If you are interested to dive deep into the topic, I suggest you read: Advances in Financial Machine Learning by Marcos L\u00f3pez de Prado. It is really a great book if you are interested in applying Machine Learning in Finance.A Machine Learning algorithm needs the data to be stationary so it can work properly. Before throwing the definition of stationarity I want to give you an intuition about it. Let\u2019s say that we have the price of an asset over a period of time. The training data has prices that vary within the interval [90$; 100$]. The algorithm will learn that everything that is around 90$ is cheap and everything that is around 100$ is expensive. Because the price of an asset moves its mean price over time, the test split will have the price within [100$; 110$]. Now the algorithm will think that the asset is always expensive, even though that is not true because now 100$ is the lowest point. This will mess with the algorithm and it will probably make very strange predictions.Now let's see the definition of stationarity: A time series has stationarity if a shift in time doesn\u2019t cause a change in the shape of the distribution. Basic properties of the distribution like the mean , variance and covariance are constant over time.As an example, we can see in the figure below the prices of Apple from 2021 to 2022. It doesn\u2019t look very stationary right?Apple prices from 2021 to 2022Also, in this article, I will present to you why the standard integer differentiation is not good enough for memory preservation.Theoretical DemonstrationCreate an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0in1401401FollowWritten by Paul Iusztin1.5K Followers\u00b7Writer for MLearning.ai\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul Iusztin and MLearning.aiPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Fabio MatricardiinMLearning.aiHow I Built a Chatbot that Crushed ChatGPT with Zero Cost AI ToolsChallenge Accepted! How I created a chatbot that surpassed the performance of the famous ChatGPT model using free and open source AI tools\u2026\u00b79 min read\u00b76 days ago1.4K5Maximilian VogelinMLearning.aiThe ChatGPT list of lists: A collection of 3000+ prompts, GPTs, use-cases, tools, APIs, extensions\u2026Updated Jan-11, 2024. Added GPT Store, new resources.11 min read\u00b7Feb 7, 202311.5K141Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111See all from Paul IusztinSee all from MLearning.aiRecommended from MediumPaul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111Moez AliEasily deploy machine learning models from the comfort of your NotebookStreamline your Machine Learning model deployment with Modelbit.com. Seamlessly move from Jupyter notebooks to production-ready REST APIs\u20269 min read\u00b7Sep 8, 20232621ListsPredictive Modeling w/ Python20 stories\u00b7891 savesPractical Guides to Machine Learning10 stories\u00b71032 savesCoding & Development11 stories\u00b7442 savesNatural Language Processing1179 stories\u00b7656 savesAserdargunAdvanced OOP in PythonClasses and objects: Class instantiation, self, data attributes, UML, methods, __str__, __repr__72 min read\u00b7Jan 146136SerokellTime Series Analysis in MLThe chronology of events, the ebb and flow of stock markets, the rhythm of the seasons-the world, as we know it, is rich with patterns and\u202616 min read\u00b7Nov 10, 202348Ankush k SingalinILLUMINATIONCrafting Quality Python Projects: A Unique Template for Success \u2014 MLOPS-IAnkush k Singal\u00b716 min read\u00b7Oct 25, 20231692Marcello DichierainData And BeyondDesign Patterns in Python for machine learning and data engineer: Factory PatternA journey to understand the most common design patterns for data science and data engineering in Python.\u00b76 min read\u00b7Sep 30, 2023134See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Provide a comprehensive tutorial for a Medium post on building a churn prediction model using Apache Spark. Focus on end-to-end pipeline construction, from data loading and cleaning to feature engineering and model evaluation, using a music streaming company dataset.",
        "post": "This Is How You Can Build a Churn Prediction Model Using\u00a0Spark | Towards AIOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyThis Is How You Can Build a Churn Prediction Model Using Apache SparkAn end-to-end tutorial on how to build a churn prediction pipeline using only Apache Spark.Paul Iusztin\u00b7FollowPublished inTowards AI\u00b711 min read\u00b7Sep 20, 202299ListenShareImage by the Author created with Stable Diffusion.This article is a tutorial on how to build a churn prediction classifier using the ML stack from Spark.We will use data from a fictive company called Sparkify, a music streaming company. The dataset contains all kinds of events created by the users who interacted with the platform.We will take a closer look at the data during the tutorial. But, I want to thank Udacity for making this data public. Without them, I couldn't make this tutorial.Here you can download the mini version of the dataset and here the entire dataset.What is customer churn?Customer churn is when someone chooses to stop using your products or services.Why is such a model useful?By building such a model, we can understand why the users are leaving the company. Therefore, this is one way to improve customer experience and retention based on user activity data.Table of ContentsDefine a Spark SessionLoad the DatasetClean the DatasetDefine the Customer Churn LabelExploratory Data AnalysisFeature EngineeringModelingResultsConclusionNote: We ran all the code in a single Notebook.1\ufe0f\u20e3 Define a Spark Sessionspark = SparkSession.\\    builder.\\    appName(\"Sparkify Churn Prediction\").\\    getOrCreate()We created a spark session called Sparkify Churn Prediction. With this session, we can load and run all the computations.2\ufe0f\u20e3 Load the DatasetEVENT_DATA_LINK = \"mini_sparkify_event_data.json\"df = spark.read.json(EVENT_DATA_LINK)df.persist()Create an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0in9999FollowWritten by Paul Iusztin1.5K Followers\u00b7Writer for Towards AI\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul Iusztin and Towards AIPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Ignacio de GregorioinTowards AIIs Mamba the End of ChatGPT As We Know It?The Great New Question\u00b78 min read\u00b7Jan 116.2K62IVAN ILINinTowards AIAdvanced RAG Techniques: an Illustrated OverviewA comprehensive study of the advanced retrieval augmented generation techniques and algorithms, systemising various approaches. The article\u202619 min read\u00b7Dec 17, 20234.3K27Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111See all from Paul IusztinSee all from Towards AIRecommended from MediumPaul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111Niranjan AppajiA Guide to Handling High Cardinality in Categorical VariablesHigh cardinality refers to a situation in a dataset where a particular feature has a large number of distinct values. In other words, a\u20265 min read\u00b7Dec 28, 20232ListsPredictive Modeling w/ Python20 stories\u00b7891 savesNatural Language Processing1179 stories\u00b7656 savesAI Regulation6 stories\u00b7311 savesPractical Guides to Machine Learning10 stories\u00b71032 savesMarcello DichierainData And BeyondAnomaly Detection Data Pipeline and Dashboard with Mage AI and StreamlitEasy steps to build a data pipeline and dashboard for anomaly detection with Mage AI and Streamlit\u00b78 min read\u00b7Aug 30, 2023235DatadriftersinDev GeniusDeploy XGBoost to Local Kubernetes Cluster using FastAPI: Movie Recommendation Use CaseEfficient deployment and scaling of Machine Learning models is critical for user experience in many real-world applications.\u00b711 min read\u00b7Oct 20, 2023761Darren WhitwoodTelco Customer Churn PredictionNearly every business, and other entities as well, is concerned about some form of customer or client retention. Customer churn can be\u202613 min read\u00b7Nov 30, 202310Moez AliEasily deploy machine learning models from the comfort of your NotebookStreamline your Machine Learning model deployment with Modelbit.com. Seamlessly move from Jupyter notebooks to production-ready REST APIs\u20269 min read\u00b7Sep 8, 20232621See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Write a Medium article in which you introduce yourself, emphasizing your journey into machine learning, your passion for building ML systems, and your mission to help others in the field through content creation. Share personal anecdotes to make the introduction engaging.",
        "post": "Hi, I am Paul. Yet Not Another LinkedIn \u201cAbout Me.\u201d A\u2026 | by Paul Iusztin | MediumOpen in appSign upSign inWriteSign upSign inMastodonAbout Me\ud83e\udd16 Hi, I am PaulYet Not Another LinkedIn \u201cAbout Me.\u201d A short introduction of myself.Paul Iusztin\u00b7Follow3 min read\u00b7Oct 17, 20221722ListenShareHi, I am Paul Iusztin, a passionate Senior ML Engineer working as a contractor and content creator.I aim to help machine learning engineers level up through hands-on practical posts, articles, and tutorials on designing and productionizing ML systems.I find beauty in simplicity. Therefore, my life\u2019s mission is simple:1. To always follow my curiosity. I am hooked on the sweet feeling of learning something new daily.2. To build machine learning products that make our daily lives better.Photo of me.My StoryAs a kid, I always loved to break and build stuff. But, I spent my entire childhood playing video games and experiencing to the fullest the story called \u201cadolescence.\u201dWhen I started my bachelor\u2019s degree in SW Engineering, that story finished, and a new one began. A tale that defined, and still does, the current version of \u201cPaul.\u201dThe truth is that before starting my university, I sucked at programming, math, and even writing.I didn\u2019t know exactly if I had the potential to learn software engineering, and I didn\u2019t even know what machine learning was. I was always stuck between \u201cthe cool kids\u201d and \u201cthe nerds,\u201d never being able to find my tribe.My passion for building software started in my second year of college when I participated in my first hackathon. I did not know much about developing complex software applications. Still, I said, \u201cwhat the heck\u201d I want to build a mobile application (more, I want to try). Of course, I didn\u2019t manage to do much. Still, this event sparked my desire to learn more about Java, Python, Object-Oriented Programming, Mobile & Web Development, etc.After a year, I participated in an optional course about Deep Learning. Since then, I have fallen in love with Machine Learning and knew what I wanted to do for the rest of my life.Long story short, after a few ML jobs, a failed start-up, a master\u2019s degree in Machine Learning, and lots of ML projects, I still feel I have much to learn. But I can proudly say something that, 5 years ago didn\u2019t know was possible: I am a self-thought senior ML engineer, contractor, and content creator.Let\u2019s connect if you want to level up in productionizing ML systems:I post almost daily AI content on \ud83d\udc47\ud83c\udffc\ud83d\udcd8 LinkedIn\ud83d\udd8b\ufe0f Newsletter\ud83d\udcbb GitHub\u2709\ufe0f Email\ud83d\udc26 TwitterCreating content takes me a lot of time. If you enjoyed my work, you could support me by:joining Medium through my referral link, you can support me without any extra cost while enjoying limitless access to Medium\u2019s rich collection of stories:Join Medium with my referral link - Paul IusztinJoin to get exclusive content about ML, MLOps, and freelancing from other writers and me. \ud83d\ude80 Unlock full access to\u2026medium.com2. buying me a coffee:Paul Iusztin is writing about Machine Learning, Computer Vision, and MLOps.\ud83e\udd16 My mission is to build practical intuition about professional ML and MLOps applications. \ud83d\ude80 Your support will\u2026www.buymeacoffee.comThank you \u270c\ud83c\udffc !Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipAccess the best member-only stories.Support independent authors.Listen to audio narrations.Read offline.Join the Partner Program and earn for your writing.Try for $5/monthAbout Me1721722FollowWritten by Paul Iusztin1.5K Followers\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul IusztinPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111Paul IusztininDecoding MLThe ultimate guide on installing PyTorch with CUDA support in all possible ways\u2192 Using Pip, Conda, Poetry, Docker, or directly on the system\u00b710 min read\u00b7Jan 42231Paul IusztininDecoding MLPrepare your RAG LangChain application for productionLesson 8: LLMOps. Serverless infrastructure. RESTful API. Gradio.\u00b719 min read\u00b7Feb 3208See all from Paul IusztinRecommended from MediumCassie KozyrkovWhat are: Embeddings? Vector Databases? Vector Search? k-NN? ANN?A simple explainer to debuzz these AI buzzwords\u00b76 min read\u00b7Feb 21.6K15Nikhil AdithyaninLevel Up CodingStock Market Sentiment Prediction with OpenAI and PythonAn interesting exploration of the power of LLMs in stock analysis11 min read\u00b76 days ago4621ListsStaff Picks577 stories\u00b7738 savesStories to Help You Level-Up at Work19 stories\u00b7469 savesSelf-Improvement 10120 stories\u00b71327 savesProductivity 10120 stories\u00b71211 savesThu VuinTowards Data ScienceHow to Learn AI on Your Own (a self-study guide)If your hands touch a keyboard for work, Artificial Intelligence is going to change your job in the next few years.\u00b712 min read\u00b7Jan 53.2K27Paul Is PositiveinILLUMINATIONHow To Detect An AI Written ArticleWith the emergence of AI comes the rise of the fake writer\u00b74 min read\u00b7Jan 245.3K129Deepanshu tyagiMLOps 2024 Roadmap: From Zero to ProIn the rapidly evolving landscape of artificial intelligence and machine learning, the significance of operationalizing machine learning\u20264 min read\u00b7Jan 36034C\u00e1ssio BolbainAI MindBecome a Senior Engineer: Tech skills are not enoughThe Crucial Soft Skills for Senior Engineer Success\u00b74 min read\u00b75 days ago1.1K16See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Introduce the concept of retry policies for enhancing code reliability in ML systems on LinkedIn. Highlight the use of the Tenacity Python package as a simple yet powerful solution for implementing customizable retry strategies.",
        "post": "One strategy that makes the \ud835\uddf1\ud835\uddf6\ud835\uddf3\ud835\uddf3\ud835\uddf2\ud835\uddff\ud835\uddf2\ud835\uddfb\ud835\uddf0\ud835\uddf2 \ud835\uddef\ud835\uddf2\ud835\ude01\ud835\ude04\ud835\uddf2\ud835\uddf2\ud835\uddfb \ud835\uddf4\ud835\uddfc\ud835\uddfc\ud835\uddf1 \ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2 \ud835\uddee\ud835\uddfb\ud835\uddf1 \ud835\uddf4\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\ude01 \ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2 is adding \ud835\uddff\ud835\uddf2\ud835\ude01\ud835\uddff\ud835\ude06 \ud835\uddfd\ud835\uddfc\ud835\uddf9\ud835\uddf6\ud835\uddf0\ud835\uddf6\ud835\uddf2\ud835\ude00.\nTo manually implement them can get tedious and complicated.\nRetry policies are a must when you:\n- make calls to an external API\n- read from a queue, etc.\n.\n\ud835\udde8\ud835\ude00\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\udde7\ud835\uddf2\ud835\uddfb\ud835\uddee\ud835\uddf0\ud835\uddf6\ud835\ude01\ud835\ude06 \ud835\udde3\ud835\ude06\ud835\ude01\ud835\uddf5\ud835\uddfc\ud835\uddfb \ud835\uddfd\ud835\uddee\ud835\uddf0\ud835\uddf8\ud835\uddee\ud835\uddf4\ud835\uddf2...\n\ud835\ude20\ud835\ude30\ud835\ude36 \ud835\ude24\ud835\ude22\ud835\ude2f \ud835\ude32\ud835\ude36\ud835\ude2a\ud835\ude24\ud835\ude2c\ud835\ude2d\ud835\ude3a \ud835\ude25\ud835\ude26\ud835\ude24\ud835\ude30\ud835\ude33\ud835\ude22\ud835\ude35\ud835\ude26 \ud835\ude3a\ud835\ude30\ud835\ude36\ud835\ude33 \ud835\ude27\ud835\ude36\ud835\ude2f\ud835\ude24\ud835\ude35\ud835\ude2a\ud835\ude30\ud835\ude2f\ud835\ude34 \ud835\ude22\ud835\ude2f\ud835\ude25 \ud835\ude22\ud835\ude25\ud835\ude25 \ud835\ude24\ud835\ude36\ud835\ude34\ud835\ude35\ud835\ude30\ud835\ude2e\ud835\ude2a\ud835\ude3b\ud835\ude22\ud835\ude23\ud835\ude2d\ud835\ude26 \ud835\ude33\ud835\ude26\ud835\ude35\ud835\ude33\ud835\ude3a \ud835\ude31\ud835\ude30\ud835\ude2d\ud835\ude2a\ud835\ude24\ud835\ude2a\ud835\ude26\ud835\ude34, \ud835\ude34\ud835\ude36\ud835\ude24\ud835\ude29 \ud835\ude22\ud835\ude34:\n1. Add fixed and random wait times between multiple retries.\n2. Add a maximum number of attempts or computation time.\n3. Retry only when specific errors are thrown (or not thrown).\n... as you can see, you easily compose these policies between them.\nThe cherry on top is that you can access the statistics of the retries of a specific function:\n\"\nprint(raise_my_exception.retry.statistics)\n\"\n.\nWhat is your current strategy for adding retry policies to your Python code?\nhashtag\n#\nmachinelearning\nhashtag\n#\npython\nhashtag\n#\nsoftwareengineering\n-----\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps."
    },
    {
        "instruction": "Write a post for Medium platform in which you provide an overview of decision trees and their evolution to XGBoost and LightGBM, focusing on their significance in machine learning, especially for tabular data. Emphasize the intuition behind these models and their applications in competitions like Kaggle.",
        "post": "Decision Trees: from 0 to XGBoost & LightGBM | by Paul Iusztin | MLearning.ai | MediumOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyDecision Trees: from 0 to XGBoost & LightGBMPaul Iusztin\u00b7FollowPublished inMLearning.ai\u00b714 min read\u00b7May 22, 2022100ListenShareDecision Tree, Gradient Boosting, XGBoost, LightGBM.SourceAs a machine learning engineer, I worked until now only with images, on systems using deep learning models. In the last months, I had the chance to work with some tabular data. I started learning by looking at different solutions on Kaggle. To my surprise almost every solution was based on decision trees based models, more precisely on XGBoost and LightGBM, which are both gradient boosting decision tree methods. To understand XGBoost and LightGBM first you have to understand previous iterations of the algorithm. Therefore, I thought it would be nice to learn everything from one place.In this article, I packed together everything you need to know starting from a simple decision tree until the most known algorithms like XGBoost and LightGBM. I want to give you an intuition on what is a decision tree and how it is optimized. Also, this blog is for people who don\u2019t have a lot of time, but still want to understand the whole thought process starting from a simple decision tree to the most popular models like XGBoost and LightGBM.Table of content:Decision Trees2. Future Approaches Built on Top of Decision Trees3. Gradient Boosting4. XGBoost5. LightGBM1. Decision TreesWhat is a Decision Tree?As shown in Fig. 1 a decision tree is an acyclic binary graph. This translates to a graph where every node has exactly two children and the logic flows in only one direction: from top to bottom (there are no cycles). It has three main components: the node, the root node, and the leaf. A node is a component that contains a boolean condition, the root node is the first node within the graph, and the leaf is the last element within the graph where the flow ends. In our case, the value within the leaf represents the final prediction of the model.Let\u2019s say that we have a person to which we know the height (meters) and country of residence and we want to predict their weight (kg). If its height is 1.75 meters and it lives in Canada then the logic, relative to the decision tree from Fig. 1, will be the\u2026Create an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0in100100FollowWritten by Paul Iusztin1.5K Followers\u00b7Writer for MLearning.ai\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul Iusztin and MLearning.aiPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Fabio MatricardiinMLearning.aiHow I Built a Chatbot that Crushed ChatGPT with Zero Cost AI ToolsChallenge Accepted! How I created a chatbot that surpassed the performance of the famous ChatGPT model using free and open source AI tools\u2026\u00b79 min read\u00b76 days ago1.4K5Maximilian VogelinMLearning.aiThe ChatGPT list of lists: A collection of 3000+ prompts, GPTs, use-cases, tools, APIs, extensions\u2026Updated Jan-11, 2024. Added GPT Store, new resources.11 min read\u00b7Feb 7, 202311.5K141Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111See all from Paul IusztinSee all from MLearning.aiRecommended from MediumShreya RaoinTowards Data ScienceCatBoost Regression: Break It Down For MeA comprehensive (and illustrated) breakdown of the inner workings of CatBoost\u00b714 min read\u00b7Sep 2, 2023160Data OverloadMastering Hyperparameter Tuning for XGBoost: Boosting Your Model\u2019s PerformanceXGBoost, or eXtreme Gradient Boosting, has emerged as a powerful and popular machine learning algorithm, particularly in the realm of\u2026\u00b74 min read\u00b7Dec 23, 202317ListsPredictive Modeling w/ Python20 stories\u00b7891 savesPractical Guides to Machine Learning10 stories\u00b71032 savesNatural Language Processing1179 stories\u00b7656 savesdata science and AI40 stories\u00b767 savesTurkish TechnologyLight GBM Light and Powerful Gradient Boost AlgorithmLightGBM, developed by Microsoft, is a gradient-boosting algorithm that has rapidly gained popularity and secured a robust position among\u20265 min read\u00b7Nov 28, 20236Amit ChauhaninTowards AIIn-depth Understanding of XGBoost Introduction in Machine LearningAlgorithm to improve the speed and performance\u00b74 min read\u00b7Nov 12, 20231632AmbikaIntroduction to CatBoost in Machine Learning\u201c CatBoost is a gradient boosting framework that is specifically designed for categorical feature support and is known for its excellent\u20266 min read\u00b7Sep 11, 202344Kasun DissanayakeinTowards DevMachine Learning Algorithms(11)\u200a\u2014\u200aEnsemble techniques (Boosting\u200a\u2014\u200aGradient Boosting)In this tutorial, we learn another Ensemble technique algorithm called Gradient boosting. It\u2019s a boosting technique that I\u2019ve covered in\u202612 min read\u00b7Dec 2, 20233023See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Write a comprehensive guide for Medium platform on the top 10 statistical concepts essential for success in data science and machine learning. The post should explain the importance of each concept in understanding and interpreting data, with examples of their application in real-world scenarios. Aim to demystify complex statistical theories for beginners while providing valuable insights for more experienced practitioners. Include practical tips on how to apply these concepts using popular data science tools and programming languages like Python and R.",
        "post": "Mastering the Top 10 Statistical Concepts: The Key to Success in Data Science | by Paul Iusztin | Towards AIOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyStatisticsMastering the Top 10 Statistical Concepts: The Key to Success in Data ScienceUnlock the full potential of your data with a deep understanding of these fundamental statistical conceptsPaul Iusztin\u00b7FollowPublished inTowards AI\u00b74 min read\u00b7Dec 28, 2022811ListenSharePhoto by Alex Chambers on UnsplashAs a data scientist, it is essential to have a strong foundation in statistical concepts and methods. These concepts and methods provide the tools and techniques necessary for analyzing and interpreting data, making informed decisions, and communicating results effectively.In this blog, we will explore the top 10 most interesting statistical concepts that a data scientist should know.From the Central Limit Theorem to feature selection, these concepts are fundamental to the field of data science and will serve as a strong foundation for any data scientist. Whether you are new to the field or an experienced professional, mastering these methods will undoubtedly improve your ability to extract insights from data and make data-driven decisions.#1. Central Limit TheoremThis theorem states that given a sufficiently large sample size, the distribution of sample means will approach a normal distribution, regardless of the shape of the underlying population distribution. This is an important concept in statistical inference, as it allows us to use normal distribution-based methods to make inferences about a population based on a sample.#2. Correlation and CausationCorrelation refers to a statistical relationship between two variables, where an increase or decrease in one variable is associated with an increase or decrease in the other. However, just because two variables are correlated does not necessarily mean that one causes the other. Establishing causation requires additional evidence and experimentation.Create an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0in81811FollowWritten by Paul Iusztin1.5K Followers\u00b7Writer for Towards AI\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul Iusztin and Towards AIPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Ignacio de GregorioinTowards AIIs Mamba the End of ChatGPT As We Know It?The Great New Question\u00b78 min read\u00b7Jan 116.2K62IVAN ILINinTowards AIAdvanced RAG Techniques: an Illustrated OverviewA comprehensive study of the advanced retrieval augmented generation techniques and algorithms, systemising various approaches. The article\u202619 min read\u00b7Dec 17, 20234.3K27Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111See all from Paul IusztinSee all from Towards AIRecommended from MediumPaul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111Anamika SinghinCodeXA Comprehensive Guide to Master the Data Science WorkflowIn this dynamic field of data science, a data science workflow consists of skeletal stages that back data scientists with their data\u20265 min read\u00b7Oct 23, 202341ListsPredictive Modeling w/ Python20 stories\u00b7891 savesPractical Guides to Machine Learning10 stories\u00b71032 savesNatural Language Processing1179 stories\u00b7656 savesdata science and AI40 stories\u00b767 savesAsad iqbalTop 20 Data Scientist Interview QuestionsExplore the top data science interview questions with answers professionals looking for jobs.6 min read\u00b7Jan 162493Anjolaoluwa Ajayiin\ud835\udc00\ud835\udc08 \ud835\udc26\ud835\udc28\ud835\udc27\ud835\udc24\ud835\udc2c.\ud835\udc22\ud835\udc28130+ Data Science Terms Every Data Scientist Should Know in 2024Most Data Science Jargon explained in plain English11 min read\u00b7Jan 53.6K33Cassie KozyrkovWhat are: Embeddings? Vector Databases? Vector Search? k-NN? ANN?A simple explainer to debuzz these AI buzzwords\u00b76 min read\u00b7Feb 21.6K15Deepanshu tyagiMLOps 2024 Roadmap: From Zero to ProIn the rapidly evolving landscape of artificial intelligence and machine learning, the significance of operationalizing machine learning\u20264 min read\u00b7Jan 36034See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Discuss the essentials of fine-tuning an open-source LLM to create a financial advisor.",
        "post": "\ud835\uddea\ud835\uddf5\ud835\uddee\ud835\ude01 do you need to \ud835\uddf3\ud835\uddf6\ud835\uddfb\ud835\uddf2-\ud835\ude01\ud835\ude02\ud835\uddfb\ud835\uddf2 an open-source \ud835\udddf\ud835\udddf\ud835\udde0 to create your own \ud835\uddf3\ud835\uddf6\ud835\uddfb\ud835\uddee\ud835\uddfb\ud835\uddf0\ud835\uddf6\ud835\uddee\ud835\uddf9 \ud835\uddee\ud835\uddf1\ud835\ude03\ud835\uddf6\ud835\ude00\ud835\uddfc\ud835\uddff?\n\nThis is the \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\uddf3\ud835\uddf6\ud835\uddfb\ud835\uddf2-\ud835\ude01\ud835\ude02\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddf8\ud835\uddf6\ud835\ude01 you must know \u2193\n\n\ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude01\n\nThe key component of any successful ML project is the data.\n\nYou need a 100 - 1000 sample Q&A (questions & answers) dataset with financial scenarios.\n\nThe best approach is to hire experts to create it manually.\n\nBut, for a PoC, that might get expensive & slow.\n\nThe good news is that a method called \"\ud835\ude0d\ud835\ude2a\ud835\ude2f\ud835\ude26\ud835\ude35\ud835\ude36\ud835\ude2f\ud835\ude2a\ud835\ude2f\ud835\ude28 \ud835\ude38\ud835\ude2a\ud835\ude35\ud835\ude29 \ud835\ude25\ud835\ude2a\ud835\ude34\ud835\ude35\ud835\ude2a\ud835\ude2d\ud835\ude2d\ud835\ude22\ud835\ude35\ud835\ude2a\ud835\ude30\ud835\ude2f\" exists.\n\nIn a nutshell, this is how it works: \"Use a big & powerful LLM (e.g., GPT4) to generate your fine-tuning data. After, use this data to fine-tune a smaller model (e.g., Falcon 7B).\"\n\nFor specializing smaller LLMs on specific use cases (e.g., financial advisors), this is an excellent method to kick off your project.\n\n\n\ud835\udde3\ud835\uddff\ud835\uddf2-\ud835\ude01\ud835\uddff\ud835\uddee\ud835\uddf6\ud835\uddfb\ud835\uddf2\ud835\uddf1 \ud835\uddfc\ud835\uddfd\ud835\uddf2\ud835\uddfb-\ud835\ude00\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\uddf0\ud835\uddf2 \ud835\udddf\ud835\udddf\ud835\udde0\n\nYou never want to start training your LLM from scratch (or rarely).\n\nWhy? Because you need trillions of tokens & millions of $$$ in compute power.\n\nYou want to fine-tune your LLM on your specific task.\n\nThe good news is that you can find a plethora of open-source LLMs on HuggingFace (e.g., Falcon, LLaMa, etc.)\n\n\n\ud835\udde3\ud835\uddee\ud835\uddff\ud835\uddee\ud835\uddfa\ud835\uddf2\ud835\ude01\ud835\uddf2\ud835\uddff \ud835\uddf2\ud835\uddf3\ud835\uddf3\ud835\uddf6\ud835\uddf0\ud835\uddf6\ud835\uddf2\ud835\uddfb\ud835\ude01 \ud835\uddf3\ud835\uddf6\ud835\uddfb\ud835\uddf2-\ud835\ude01\ud835\ude02\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4\n\nAs LLMs are big... duh...\n\n... they don't fit on a single GPU.\n\nAs you want only to fine-tune the LLM, the community invented clever techniques that quantize the LLM (to fit on a single GPU) and fine-tune only a set of smaller adapters.\n\nOne popular approach is QLoRA, which can be implemented using HF's `\ud835\ude31\ud835\ude26\ud835\ude27\ud835\ude35` Python package.\n\n\n\ud835\udde0\ud835\udddf\ud835\udde2\ud835\uddfd\ud835\ude00\n\nAs you want your project to get to production, you have to integrate the following MLOps components:\n\n- experiment tracker to monitor & compare your experiments\n- model registry to version & share your models between the FTI pipelines\n- prompts monitoring to debug & track complex chains\n\n\u21b3 Available on ML platforms, such as Comet ML.\n\n\n\ud835\uddd6\ud835\uddfc\ud835\uddfa\ud835\uddfd\ud835\ude02\ud835\ude01\ud835\uddf2 \ud835\uddfd\ud835\uddf9\ud835\uddee\ud835\ude01\ud835\uddf3\ud835\uddfc\ud835\uddff\ud835\uddfa\n\nThe most common approach is to train your LLM on your on-prem Nivida GPUs cluster or rent them on cloud providers such as AWS, Paperspace, etc.\n\nBut what if I told you that there is an easier way?\n\nThere is! It is called serverless.\n\nFor example, Beam is a GPU serverless provider that makes deploying your training pipeline as easy as decorating your Python function with `@\ud835\ude22\ud835\ude31\ud835\ude31.\ud835\ude33\ud835\ude36\ud835\ude2f()`.\n\nAlong with ease of deployment, you can easily add your training code to your CI/CD to add the final piece of the MLOps puzzle, called CT (continuous training).\n\n.\n\n\u21b3 To see everything in action, check out my FREE \ud835\udddb\ud835\uddee\ud835\uddfb\ud835\uddf1\ud835\ude00-\ud835\uddfc\ud835\uddfb \ud835\udddf\ud835\udddf\ud835\udde0\ud835\ude00 \ud835\uddf0\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\ude00\ud835\uddf2 & give it a \u2b50: \ud83d\udd17 https://lnkd.in/dZgqtf8f"
    },
    {
        "instruction": "Craft a post to inform your network about a recent publication or article related to your field. Provide a brief summary of the article's key points and your personal insights or opinions.",
        "post": "Electromyography has the potential to revolutionize how we interact with computers. Imagine typing without a keyboard, controlling visual interfaces without a mouse, and navigating AR/VR environments much more intuitively. The signals in our wrists are so clear that EMG can detect finger motion of just a millimeter. This video highlights some of the groundbreaking research by the incredible CTRL-Labs team who joined us last year. \ud83e\udd29 Mind-blowing potential. #innovation #technology #facebook"
    },
    {
        "instruction": "Initiate a conversation on LinkedIn about the distinct roles within ML engineering, drawing from personal observations. Encourage sharing of experiences to explore how these roles intersect and contribute to the broader field of AI and ML.",
        "post": "\ud835\udddd\ud835\uddfc\ud835\uddef \ud835\uddff\ud835\uddfc\ud835\uddf9\ud835\uddf2\ud835\ude00 tell you there is just \ud835\uddfc\ud835\uddfb\ud835\uddf2 \ud835\ude01\ud835\ude06\ud835\uddfd\ud835\uddf2 \ud835\uddfc\ud835\uddf3 \ud835\udde0\ud835\udddf \ud835\uddf2\ud835\uddfb\ud835\uddf4\ud835\uddf6\ud835\uddfb\ud835\uddf2\ud835\uddf2\ud835\uddff\ud835\uddf6\ud835\uddfb\ud835\uddf4, but there are \ud835\uddee\ud835\uddf0\ud835\ude01\ud835\ude02\ud835\uddee\ud835\uddf9\ud835\uddf9\ud835\ude06 \ud835\udfef\nHere they are \u2193\nThese are the 3 ML engineering personas I found while working with different teams in the industry:\n#\ud835\udfed. \ud835\udde5\ud835\uddf2\ud835\ude00\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddf0\ud835\uddf5\ud835\uddf2\ud835\uddff\ud835\ude00 \ud835\ude02\ud835\uddfb\ud835\uddf1\ud835\uddf2\ud835\uddff\ud835\uddf0\ud835\uddfc\ud835\ude03\ud835\uddf2\ud835\uddff\nThey like to stay in touch with the latest papers, understand the architecture of models, optimize them, run experiments, etc.\nThey are great at picking the best models but not that great at writing clean code and scaling the solution.\n#\ud835\udfee. \ud835\udde6\ud835\uddea\ud835\uddd8 \ud835\ude02\ud835\uddfb\ud835\uddf1\ud835\uddf2\ud835\uddff\ud835\uddf0\ud835\uddfc\ud835\ude03\ud835\uddf2\ud835\uddff\nThey pretend they read papers but don't (maybe only when they have to). They are more concerned with writing modular code and data quality than the latest hot models. Usually, these are the \"data-centric\" people.\nThey are great at writing clean code & processing data at scale but lack deep mathematical skills to develop complex DL solutions.\n#\ud835\udfef. \ud835\udde0\ud835\udddf\ud835\udde2\ud835\uddfd\ud835\ude00 \ud835\uddf3\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddf8\ud835\ude00\nThey ultimately don't care about the latest research & hot models. They are more into the latest MLOps tools and building ML systems. They love to automate everything and use as many tools as possible.\nGreat at scaling the solution and building ML pipelines, but not great at running experiments & tweaking ML models. They love to treat the ML model as a black box.\n.\nI started as #1. , until I realized I hated it - now I am a mix of:\n\u2192 #\ud835\udfed. 20%\n\u2192 #\ud835\udfee. 40%\n\u2192 #\ud835\udfef. 40%\nBut that doesn't mean one is better - these types are complementary.\nA great ML team should have at least one of each persona.\nWhat do you think? Did I get it right?\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning\n-----\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps."
    },
    {
        "instruction": "Invite LinkedIn followers to consider the implications of searching for oneself on ChatGPT. Spark a discussion on the role of generative AI in personal and professional discovery, questioning its potential in hiring and dating contexts.",
        "post": "Have you ever searched for yourself on ChatGPT?\nDo you think this is the next generation of hiring and dating?\nThis is what it returned after prompting: \"You are a private detective. Tell me everything you know about Paul Iusztin.\"\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\nchatgpt\n-----\n\ud83d\udca1 Follow me for weekly insights about designing ML systems."
    },
    {
        "instruction": "On LinkedIn, share a strategy for managing information overload by describing a personalized 'second brain' system. Highlight the process from collection to storage, emphasizing the importance of organizing information for productivity and learning.",
        "post": "I never forget anything. Said no one but only your second brain.\nAfter 6+ months of refinement, this is my second brain strategy \ud83d\udc47\nTiago's Forte book inspired me, but I adapted his system to my needs.\n.\n#\ud835\udfec. \ud835\uddd6\ud835\uddfc\ud835\uddf9\ud835\uddf9\ud835\uddf2\ud835\uddf0\ud835\ude01\nThis is where you are bombarded with information from all over the place.\n#\ud835\udfed. \ud835\udde7\ud835\uddf5\ud835\uddf2 \ud835\uddda\ud835\uddff\ud835\uddee\ud835\ude03\ud835\uddf2\ud835\ude06\ud835\uddee\ud835\uddff\ud835\uddf1\nThis is where I save everything that looks interesting.\nI won't use 90% of what is here, but it satisfied my urge to save that \"cool article\" I saw on LinkedIn.\nTools: Mostly Browser Bookmarks, but I rarely use GitHub stars, Medium lists, etc.\n#\ud835\udfee. \ud835\udde7\ud835\uddf5\ud835\uddf2 \ud835\uddd5\ud835\uddfc\ud835\uddee\ud835\uddff\ud835\uddf1\nHere, I start converging the information and planning what to do next.\nTools: Notion\n#\ud835\udfef. \ud835\udde7\ud835\uddf5\ud835\uddf2 \ud835\uddd9\ud835\uddf6\ud835\uddf2\ud835\uddf9\ud835\uddf1\nHere is where I express myself through learning, coding, writing, etc.\nTools: whatever you need to express yourself.\n2 & 3 are iterative processes. Thus I often bounce between them until the information is distilled.\n#\ud835\udff0. \ud835\udde7\ud835\uddf5\ud835\uddf2 \ud835\uddea\ud835\uddee\ud835\uddff\ud835\uddf2\ud835\uddf5\ud835\uddfc\ud835\ude02\ud835\ude00\ud835\uddf2\nHere is where I take the distilled information and write it down for cold storage.\nTools: Notion, Google Drive\n.\nWhen I want to search for a piece of information, I start from the Warehouse and go backward until I find what I need.\nAs a minimalist, I  kept my tools to a minimum. I primarily use only: Brave, Notion, and Google Drive.\nYou don't need 100+ tools to be productive. They just want to take your money from you.\nSo remember...\nYou have to:\n- collect\n- link\n- plan\n- distill\n- store\nWhat is your second brain strategy? Leave your thoughts in the comments.\nhashtag\n#\nproductivity\nhashtag\n#\nsecondbrain\nhashtag\n#\nmachinelearning\n-----\n\ud83d\udca1 Follow me for weekly insights about designing ML systems."
    },
    {
        "instruction": "Publish a LinkedIn post listing top Python tools for code quality, including links and a note on automating their use with pre-commit hooks.",
        "post": "Top 3 tools to keep your Python code professional without any additional effort \ud83d\udc47\n.\n#\ud835\udfcf. \ud835\udc01\ud835\udc25\ud835\udc1a\ud835\udc1c\ud835\udc24\nA tool for automatically formatting your code.\nCheck it out:\nhttps://lnkd.in/dR_RdjT9\n#\ud835\udfd0. \ud835\udc05\ud835\udc25\ud835\udc1a\ud835\udc24\ud835\udc1e\ud835\udfd6\nA tool that checks and validates your coding style.\nCheck it out:\nhttps://lnkd.in/dBmyuECV\n#\ud835\udfd1. \ud835\udc11\ud835\udc1e\ud835\udc1f\ud835\udc2e\ud835\udc2b\ud835\udc1b\nA tool for automatically refurbishing and modernizing your Python codebases.\nCheck it out:\nhttps://lnkd.in/d67J9hbt\n.\nAn important note is that you can automate the trigger of these tools using pre-commit. Thus, these tools will be triggered every time you run a commit.\nhashtag\n#\npython\nhashtag\n#\ncleancode\nhashtag\n#\ncodingstyle\n-----\n\ud83d\udca1 My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n."
    },
    {
        "instruction": "Write a Medium tutorial post introducing the first lesson of a seven-part course on MLOps, focusing on building a production-ready feature engineering pipeline, including batch serving, using feature stores like Hopsworks, and designing pipelines for data processing and model training. The post should target mid to advanced machine learning engineers, emphasizing hands-on project building to forecast energy consumption in Denmark.",
        "post": "A Framework for Building a Production-Ready Feature Engineering Pipeline | by Paul Iusztin | Towards Data ScienceOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyThe Full Stack 7-Steps MLOps FrameworkA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.Paul Iusztin\u00b7FollowPublished inTowards Data Science\u00b713 min read\u00b7Apr 28, 202376111ListenSharePhoto by Hassan Pasha on UnsplashThis tutorial represents lesson 1 out of a 7-lesson course that will walk you step-by-step through how to design, implement, and deploy an ML system using MLOps good practices. During the course, you will build a production-ready model to forecast energy consumption levels for the next 24 hours across multiple consumer types from Denmark.By the end of this course, you will understand all the fundamentals of designing, coding and deploying an ML system using a batch-serving architecture.This course targets mid/advanced machine learning engineers who want to level up their skills by building their own end-to-end projects.Nowadays, certificates are everywhere. Building advanced end-to-end projects that you can later show off is the best way to get recognition as a professional engineer.Table of Contents:Course IntroductionCourse LessonsData SourceLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.Lesson 1: CodeConclusionReferencesIntroductionAt the end of this 7 lessons course, you will know how to:design a batch-serving architectureuse Hopsworks as a feature storedesign a feature engineering pipeline that reads data from an APIbuild a training pipeline with hyper-parameter tunninguse W&B as an ML Platform to track your experiments, models, and metadataimplement a batch prediction pipelineuse Poetry to build your own Python packagesdeploy your own private PyPi serverCreate an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0in76176111FollowWritten by Paul Iusztin1.5K Followers\u00b7Writer for Towards Data Science\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul Iusztin and Towards Data SciencePaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Sheila TeoinTowards Data ScienceHow I Won Singapore\u2019s GPT-4 Prompt Engineering CompetitionA deep dive into the strategies I learned for harnessing the power of Large Language Models (LLMs)\u00b723 min read\u00b7Dec 29, 202311.2K125Cristian LeoinTowards Data ScienceThe Math behind Adam OptimizerWhy is Adam the most popular optimizer in Deep Learning? Let\u2019s understand it by diving into its math, and recreating the algorithm.16 min read\u00b7Jan 301.3K9Paul IusztininDecoding MLThe ultimate guide on installing PyTorch with CUDA support in all possible ways\u2192 Using Pip, Conda, Poetry, Docker, or directly on the system\u00b710 min read\u00b7Jan 42231See all from Paul IusztinSee all from Towards Data ScienceRecommended from MediumPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Deepanshu tyagiMLOps 2024 Roadmap: From Zero to ProIn the rapidly evolving landscape of artificial intelligence and machine learning, the significance of operationalizing machine learning\u20264 min read\u00b7Jan 36034ListsPredictive Modeling w/ Python20 stories\u00b7891 savesPractical Guides to Machine Learning10 stories\u00b71032 savesNatural Language Processing1179 stories\u00b7656 savesThe New Chatbots: ChatGPT, Bard, and Beyond12 stories\u00b7301 savesAnkush k SingalinILLUMINATIONCrafting Quality Python Projects: A Unique Template for Success \u2014 MLOPS-IAnkush k Singal\u00b716 min read\u00b7Oct 25, 20231692Moez AliEasily deploy machine learning models from the comfort of your NotebookStreamline your Machine Learning model deployment with Modelbit.com. Seamlessly move from Jupyter notebooks to production-ready REST APIs\u20269 min read\u00b7Sep 8, 20232621DatadriftersinDev GeniusDeploy XGBoost to Local Kubernetes Cluster using FastAPI: Movie Recommendation Use CaseEfficient deployment and scaling of Machine Learning models is critical for user experience in many real-world applications.\u00b711 min read\u00b7Oct 20, 2023761Nicolas PogeantinMLearning.aiBuilding a Scalable MLOps Stack\u200a\u2014\u200a From Data to DeploymentThis article explores the development of a streamlined ML system, showcasing the journey from data to deployment, simplifying complex\u2026\u00b75 min read\u00b7Sep 8, 202354See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Explain key insights from David Sinclair's book 'Lifespan' in simple terms suitable for a young audience, focusing on practical tips for living a longer, healthier life. Write it in a Medium post, including a summary of the book's main ideas and how they apply to daily routines.",
        "post": "This Is How You Can Live Up to 120-Years Old | by Paul Iusztin | MediumOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyWellbeingThis Is How You Can Live Up to 120-Years OldExplaining Lifespan by David Sinclair to a 7-years old.Paul Iusztin\u00b7Follow9 min read\u00b7Nov 9, 2022122ListenSharePhoto by Ashraful Islam on UnsplashThe purpose of this article is to document my key takeaways from [1] Lifespan by David Sinclair.The book completely changed my perspective on aging and life in general. I don\u2019t usually write about this topic, but it contains valuable information that everybody should know.Disclaimer: I am not a biotech expert, but I am an enthusiastic engineer who likes to read about wellbeing, technology, and biotech. Therefore, this summary is my perspective on the book. If you want to get the most out of the book, I highly suggest you read it.Through my notes, I tried to synthesize the main ideas of the book. I overly simplified everything, intending to take practical tips & tricks that we can introduce in our daily lives on how to live longer and happier.Therefore, if, by chance, any expert reads this article, this is a piece of information that ideally can be read by a seven years old child. In my opinion, it is everything we need to know at the level of adding good practices in our daily routine that might keep us healthy to live 100+ years old.Photo of my [1] Lifespan book and its wanna-be index system [Image by the author].Table of Contents:Why Do We Age?Should We Age?Practical Tips & Tricks1. Why Do We Age?Every cell contains a copy of the DNA. We can split the DNA into two logical components:The information that defines us as species,An operating system that can manipulate that information accordingly is called the Epigenome. It has the role of turning on/off specific genes for various cells.The computer is a perfect analogy (at least for me as a machine learning software engineer). When the computer is off, everything is stored on the disk (the DNA). When you turn it on, the operation system (Epigenome) is the first component that wakes up. Afterward, the OS (Epigenome)\u2026Create an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0in122122FollowWritten by Paul Iusztin1.5K Followers\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul IusztinPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111Paul IusztininDecoding MLThe ultimate guide on installing PyTorch with CUDA support in all possible ways\u2192 Using Pip, Conda, Poetry, Docker, or directly on the system\u00b710 min read\u00b7Jan 42231Paul IusztininDecoding MLPrepare your RAG LangChain application for productionLesson 8: LLMOps. Serverless infrastructure. RESTful API. Gradio.\u00b719 min read\u00b7Feb 3208See all from Paul IusztinRecommended from MediumJames Presbitero Jr.inPractice in PublicThese Words Make it Obvious That Your Text is Written By AIThese 7 words are painfully obvious. They make me cringe. They will make your reader cringe.4 min read\u00b7Dec 31, 202336K952Thu VuinTowards Data ScienceHow to Learn AI on Your Own (a self-study guide)If your hands touch a keyboard for work, Artificial Intelligence is going to change your job in the next few years.\u00b712 min read\u00b7Jan 53.2K27Lists6 Science-Backed Health Stories on Covid, Sleep, and More6 stories\u00b798 savesA Guide to Choosing, Planning, and Achieving Personal Goals13 stories\u00b7949 savesStories to Help You Live Better19 stories\u00b71169 savesProductivity 10120 stories\u00b71211 savesIain StanleyinDigital Global TravelerI Went Speed-Dating in Tokyo: It Explains Japan\u2019s Sexless Population Explosion!Exclusive conversations with a bevy of beautiful, single girls? What could go wrong\u2026\u00b78 min read\u00b7Jan 1710.1K126Jacob BennettinLevel Up CodingThe 5 paid subscriptions I actually use in 2024 as a software engineerTools I use that are cheaper than Netflix\u00b75 min read\u00b7Jan 48.2K96Cassie KozyrkovWhat are: Embeddings? Vector Databases? Vector Search? k-NN? ANN?A simple explainer to debuzz these AI buzzwords\u00b76 min read\u00b7Feb 21.6K15Dare Obasanjo5 Things I Learned About Leadership from the Death & Rebirth of MicrosoftIn 2007 Paul Graham, the founder of Y-Combinator, penned an essay titled Microsoft is Dead. The thesis of the article was that Microsoft\u20268 min read\u00b76 days ago5.3K113See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "On LinkedIn, share a personal experience with Hopsworks while developing The Full Stack 7-Steps MLOps Framework. Praise its features, documentation, and the support received, underscoring the platform's contribution to the course's success.",
        "post": "Don't know what \ud835\uddd9\ud835\uddf2\ud835\uddee\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2 \ud835\udde6\ud835\ude01\ud835\uddfc\ud835\uddff\ud835\uddf2 to use?\nI recommend you check out\nHopsworks\n.\nI had an excellent time using it while implementing \ud835\udde7\ud835\uddf5\ud835\uddf2 \ud835\uddd9\ud835\ude02\ud835\uddf9\ud835\uddf9 \ud835\udde6\ud835\ude01\ud835\uddee\ud835\uddf0\ud835\uddf8 \ud835\udff3-\ud835\udde6\ud835\ude01\ud835\uddf2\ud835\uddfd\ud835\ude00 \ud835\udde0\ud835\udddf\ud835\udde2\ud835\uddfd\ud835\ude00 \ud835\uddd9\ud835\uddff\ud835\uddee\ud835\uddfa\ud835\uddf2\ud835\ude04\ud835\uddfc\ud835\uddff\ud835\uddf8 free course.\nThey provide:\n- an intuitive Python package to interface with their platform\n- a robust documentation\n- all the features you need from a feature store\nThey are still a fast-growing company, so the tool is evolving, making it more robust & feature-rich.\nWhile developing the course, I didn't know the team or have any connections with them, but they were kind enough to approach me and send me a gift.\nI love the logo & branding on their items. \ud83d\udd25\nI guess I have no other option but to drink this weekend \ud83d\ude02\nThank you,\nHopsworks\n!\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\nfeaturestore\n-----\n\ud83d\udca1 Follow me if you want to level up in designing ML systems using MLOps good practices."
    },
    {
        "instruction": "Please write me a LinkedIn Post of about 400 words about fine tuning open source llms: and about fine-tuning vs prompt engineering. Highlight the need to fine tune, why do we need it and the difference between it and prompt engineering.",
        "post": "\ud835\uddea\ud835\uddf5\ud835\ude06 & \ud835\ude04\ud835\uddf5\ud835\uddf2\ud835\uddfb do you need to \ud835\uddf3\ud835\uddf6\ud835\uddfb\ud835\uddf2-\ud835\ude01\ud835\ude02\ud835\uddfb\ud835\uddf2 \ud835\uddfc\ud835\uddfd\ud835\uddf2\ud835\uddfb-\ud835\ude00\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\uddf0\ud835\uddf2 \ud835\udddf\ud835\udddf\ud835\udde0\ud835\ude00? What about \ud835\uddf3\ud835\uddf6\ud835\uddfb\ud835\uddf2-\ud835\ude01\ud835\ude02\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\ude03\ud835\ude00. \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddfa\ud835\uddfd\ud835\ude01 \ud835\uddf2\ud835\uddfb\ud835\uddf4\ud835\uddf6\ud835\uddfb\ud835\uddf2\ud835\uddf2\ud835\uddff\ud835\uddf6\ud835\uddfb\ud835\uddf4? \n \nFine-tuning is the process of taking a pre-trained model and further refining it on a specific task. \n \n\ud835\uddd9\ud835\uddf6\ud835\uddff\ud835\ude00\ud835\ude01, \ud835\uddf9\ud835\uddf2\ud835\ude01'\ud835\ude00 \ud835\uddf0\ud835\uddf9\ud835\uddee\ud835\uddff\ud835\uddf6\ud835\uddf3\ud835\ude06 \ud835\ude04\ud835\uddf5\ud835\uddee\ud835\ude01 \ud835\uddfa\ud835\uddf2\ud835\ude01\ud835\uddf5\ud835\uddfc\ud835\uddf1\ud835\ude00 \ud835\uddfc\ud835\uddf3 \ud835\uddf3\ud835\uddf6\ud835\uddfb\ud835\uddf2-\ud835\ude01\ud835\ude02\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddee\ud835\uddfb \ud835\uddfc\ud835\uddfd\ud835\uddf2\ud835\uddfb-\ud835\ude00\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\uddf0\ud835\uddf2 \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\uddf2\ud835\ude05\ud835\uddf6\ud835\ude00t \u2193 \n \n- \ud835\ude0a\ud835\ude30\ud835\ude2f\ud835\ude35\ud835\ude2a\ud835\ude2f\ud835\ude36\ud835\ude26\ud835\ude25 \ud835\ude31\ud835\ude33\ud835\ude26-\ud835\ude35\ud835\ude33\ud835\ude22\ud835\ude2a\ud835\ude2f\ud835\ude2a\ud835\ude2f\ud835\ude28: utilize domain-specific data to apply the same pre-training process (next token prediction) on the pre-trained (base) model \n- \ud835\ude10\ud835\ude2f\ud835\ude34\ud835\ude35\ud835\ude33\ud835\ude36\ud835\ude24\ud835\ude35\ud835\ude2a\ud835\ude30\ud835\ude2f \ud835\ude27\ud835\ude2a\ud835\ude2f\ud835\ude26-\ud835\ude35\ud835\ude36\ud835\ude2f\ud835\ude2a\ud835\ude2f\ud835\ude28: the pre-trained (base) model is fine-tuned on a Q&A dataset to learn to answer questions \n- \ud835\ude1a\ud835\ude2a\ud835\ude2f\ud835\ude28\ud835\ude2d\ud835\ude26-\ud835\ude35\ud835\ude22\ud835\ude34\ud835\ude2c \ud835\ude27\ud835\ude2a\ud835\ude2f\ud835\ude26-\ud835\ude35\ud835\ude36\ud835\ude2f\ud835\ude2a\ud835\ude2f\ud835\ude28: the pre-trained model is refined for a specific task, such as toxicity detection, coding, medicine advice, etc. \n- \ud835\ude19\ud835\ude13\ud835\ude0f\ud835\ude0d: It requires collecting human preferences (e.g., pairwise comparisons), which are then used to train a reward model. The reward model is used to fine-tune the LLM via RL techniques such as PPO. \n \nCommon approaches are to take a pre-trained LLM (next-word prediction) and apply instruction & single-task fine-tuning. \n \n\ud835\uddea\ud835\uddf5\ud835\ude06 \ud835\uddf1\ud835\uddfc \ud835\ude06\ud835\uddfc\ud835\ude02 \ud835\uddfb\ud835\uddf2\ud835\uddf2\ud835\uddf1 \ud835\ude01\ud835\uddfc \ud835\uddf3\ud835\uddf6\ud835\uddfb\ud835\uddf2-\ud835\ude01\ud835\ude02\ud835\uddfb\ud835\uddf2 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\udddf\ud835\udddf\ud835\udde0? \n \nYou do instruction fine-tuning to make the LLM learn to answer your questions. \n \nThe exciting part is when you want to fine-tune your LLM on a single task. \n \nHere is why \u2193 \n \n\ud835\ude31\ud835\ude26\ud835\ude33\ud835\ude27\ud835\ude30\ud835\ude33\ud835\ude2e\ud835\ude22\ud835\ude2f\ud835\ude24\ud835\ude26: it will improve your LLM performance on given use cases (e.g., coding, extracting text, etc.). Mainly, the LLM will specialize in a given task (a specialist will always beat a generalist in its domain) \n \n\ud835\ude24\ud835\ude30\ud835\ude2f\ud835\ude35\ud835\ude33\ud835\ude30\ud835\ude2d: you can refine how your model should behave on specific inputs and outputs, resulting in a more robust product \n \n\ud835\ude2e\ud835\ude30\ud835\ude25\ud835\ude36\ud835\ude2d\ud835\ude22\ud835\ude33\ud835\ude2a\ud835\ude3b\ud835\ude22\ud835\ude35\ud835\ude2a\ud835\ude30\ud835\ude2f: you can create an army of smaller models, where each is specialized on a particular task, increasing the overall system's performance. Usually, when you fine-tune one task, it reduces the performance of the other tasks (known as the alignment tax). Thus, having an expert system of multiple smaller models can improve the overall performance. \n \n\ud835\uddea\ud835\uddf5\ud835\uddee\ud835\ude01 \ud835\uddee\ud835\uddef\ud835\uddfc\ud835\ude02\ud835\ude01 \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddfa\ud835\uddfd\ud835\ude01 \ud835\uddf2\ud835\uddfb\ud835\uddf4\ud835\uddf6\ud835\uddfb\ud835\uddf2\ud835\uddf2\ud835\uddff\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\ude03\ud835\ude00 \ud835\uddf3\ud835\uddf6\ud835\uddfb\ud835\uddf2-\ud835\ude01\ud835\ude02\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4? \n \n\ud835\ude25\ud835\ude22\ud835\ude35\ud835\ude22: use prompting when you don't have data available (~2 examples are enough). Fine-tuning needs at least >=100 examples to work. \n \n\ud835\ude24\ud835\ude30\ud835\ude34\ud835\ude35: prompting forces you to write long & detailed prompts to achieve your level of performance. You pay per token (API or compute-wise). Thus, when a prompt gets bigger, your costs increase. But, when fine-tuning an LLM, you incorporate all that knowledge inside the model. Hence, you can use smaller prompts with similar performance. \n \nhashtag\n#machinelearning \nhashtag\n#mlops \nhashtag\n#datascience"
    },
    {
        "instruction": "Share personal experiences in a Medium post with GitHub Copilot on machine learning projects, highlighting its capabilities, benefits, and limitations. Include screenshots of Copilot suggestions in your code and discuss how it can improve coding efficiency.",
        "post": "Should You Use GitHub Copilot?. GitHub Copilot. Python. PyCharm. | by Paul Iusztin | MLearning.ai | MediumOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyShould You Use GitHub Copilot?Paul Iusztin\u00b7FollowPublished inMLearning.ai\u00b77 min read\u00b7May 26, 202252ListenShareGitHub Copilot. Python. PyCharm.Photo by Praveen Thirumurugan on UnsplashIn the past months, I had the chance to test GitHub Copilot on one of my machine learning projects. Lately, I had to write data analysis and feature engineering code, therefore I used it together with the following technologies:IDE: P\u0332y\u0332C\u0332h\u0332a\u0332r\u0332m\u0332Programming language: P\u0332y\u0332t\u0332h\u0332o\u0332n\u0332Libraries: P\u0332a\u0332n\u0332d\u0332a\u0332s\u0332I made some screenshots from my own codebase. I wanted to share with you my experience of how this new AI is performing out there in the wild. I think it is really interesting to see how technology evolved and where is headed.Table of content:What is GitHub CopilotHow You Can Use ItPersonal Impressions & Screenshots from Using the Copilot1\ufe0f\u20e3. What is GitHub CopilotGitHub Copilot is the latest autocompletion tool developed by OpenAI. It uses deep learning to understand your current state of the program. It is more than just a simple auto-completion tool because it can give you complex suggestions based on the whole state of the function/class/program you are into. In some cases, it will suggest you multiple lines of code and sometimes even the whole function.From my perspective, this tool will change the game of programming. We will switch from ctrl-c + ctrl-v to ctrl + tab (this is how you accept a suggestion from the tool).SourceI will not describe how the algorithm works and how it makes the suggestions, because, honestly, I did not have the chance to understand it so far. I will focus just on how to use it and give you some screenshots from my coding session to see it in action.To be honest, I was pretty impressed with what it can do. It can have a deep understanding of your code by making multi-line suggestions and picking up on your coding style. By using it I had the chance to learn new tricks about Python and Pandas functionalities. Even though I think it has a lot of potentials, it is not yet perfect\u2026Create an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0in5252FollowWritten by Paul Iusztin1.5K Followers\u00b7Writer for MLearning.ai\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul Iusztin and MLearning.aiPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Fabio MatricardiinMLearning.aiHow I Built a Chatbot that Crushed ChatGPT with Zero Cost AI ToolsChallenge Accepted! How I created a chatbot that surpassed the performance of the famous ChatGPT model using free and open source AI tools\u2026\u00b79 min read\u00b76 days ago1.4K5Maximilian VogelinMLearning.aiThe ChatGPT list of lists: A collection of 3000+ prompts, GPTs, use-cases, tools, APIs, extensions\u2026Updated Jan-11, 2024. Added GPT Store, new resources.11 min read\u00b7Feb 7, 202311.5K141Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111See all from Paul IusztinSee all from MLearning.aiRecommended from MediumAnkush k SingalinILLUMINATIONCrafting Quality Python Projects: A Unique Template for Success \u2014 MLOPS-IAnkush k Singal\u00b716 min read\u00b7Oct 25, 20231692AserdargunAdvanced OOP in PythonClasses and objects: Class instantiation, self, data attributes, UML, methods, __str__, __repr__72 min read\u00b7Jan 146136ListsPredictive Modeling w/ Python20 stories\u00b7891 savesPractical Guides to Machine Learning10 stories\u00b71032 savesCoding & Development11 stories\u00b7442 savesNatural Language Processing1179 stories\u00b7656 savesThu VuinTowards Data ScienceHow to Learn AI on Your Own (a self-study guide)If your hands touch a keyboard for work, Artificial Intelligence is going to change your job in the next few years.\u00b712 min read\u00b7Jan 53.2K27Pouya HallajPython Asyncio: A Guide to Asynchronous ProgrammingA Guide to Python\u2019s Asynchronous Programming.4 min read\u00b7Sep 18, 202311Nikhil AdithyaninLevel Up CodingStock Market Sentiment Prediction with OpenAI and PythonAn interesting exploration of the power of LLMs in stock analysis11 min read\u00b76 days ago4621Saverio MazzainStackademicFastAPI & Parallel ProcessingIt is possible for an HTTP request in FastAPI to trigger a parallel process that utilizes multiple CPU cores. However, since FastAPI and\u2026\u00b74 min read\u00b7Jan 7159See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Compose a Medium post in which you discuss the importance of model serving in deploying machine learning models, comparing Model-as-Dependency and Model-as-Service approaches. Include practical advice for using web APIs for model serving and the advantages of this method.",
        "post": "This Is What You Need to Know to Serve a Machine Learning Model | by Paul Iusztin | FAUN\u200a\u2014\u200aDeveloper Community \ud83d\udc3eOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyThis Is What You Need to Know to Serve a Machine Learning ModelPaul Iusztin\u00b7FollowPublished inFAUN\u200a\u2014\u200aDeveloper Community \ud83d\udc3e\u00b75 min read\u00b7Jun 13, 2022ListenShareModel ServingAs machine learning engineers or data scientists, we all got to the point where we built our beautiful models with wonderful test results to end up using them just in a PowerPoint presentation. The most standard way of interacting with a model is to use it in an offline setup, where we have some kind of dataset to play with. This is ok for experimentation and to build your initial model. But, the next step would be to put our precious model out in the wild so people can use it. This is what model serving is all about. It represents the mechanism of deploying the model so other people can interact with it. With model serving, you can move from experimentation to production.The most common ways of serving a model are:Model-as-Dependency;Model-as-Service.\u2699\ufe0f In a Model-as-Dependency setup, the model will be used directly by the application. For example, in a Python project, it will be installed as a package through pip or integrated into your code directly from a git repository. I think this is the easiest way, conceptually, to serve a model, but it comes with some downsides. Where the application runs, you need to have the necessary hardware for your model, which sometimes is not possible. Also, the client of the model will always have to go through the pain of managing the dependencies of the model, which in some cases is a real pain.Model-as-Dependency\u2699\ufe0f In a Model-as-Service setup the model will be accessed through a web API (e.g. RESTful API). In this way, the client can treat the model as a black box. It only has to know the model's inputs and outputs. The rest of the article will explain in more detail how the Model-as-Service works.Model-as-ServiceModel-as-ServiceAs we said the Model-as-Service will be used by its clients through a web API. Maybe for some of you, it sounds scary, but the core concepts are simple. Its architecture will have the\u2026Create an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0inFollowWritten by Paul Iusztin1.5K Followers\u00b7Writer for FAUN\u200a\u2014\u200aDeveloper Community \ud83d\udc3e\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul Iusztin and FAUN\u200a\u2014\u200aDeveloper Community \ud83d\udc3ePaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 52014grayinFAUN\u200a\u2014\u200aDeveloper Community \ud83d\udc3eMy Latest MacBook Development Setup, Favorite Apps, and Productivity HacksA selection of applications I use in my daily work\u00b75 min read\u00b7Jan 258211Aymen El AmriinFAUN\u200a\u2014\u200aDeveloper Community \ud83d\udc3eThe Hottest Open Source Projects Of 2023This article was originally posted on faun.dev.\u00b714 min read\u00b7Dec 28, 20232.3K13Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111See all from Paul IusztinSee all from FAUN\u200a\u2014\u200aDeveloper Community \ud83d\udc3eRecommended from MediumPaul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111Ankush k SingalinILLUMINATIONCrafting Quality Python Projects: A Unique Template for Success \u2014 MLOPS-IAnkush k Singal\u00b716 min read\u00b7Oct 25, 20231692ListsPredictive Modeling w/ Python20 stories\u00b7891 savesPractical Guides to Machine Learning10 stories\u00b71032 savesNatural Language Processing1179 stories\u00b7656 savesdata science and AI40 stories\u00b767 savesMohammad FaizaninMLearning.aiMLOps: Weights & Biases Experiments TrackingMachine Learning projects have hyperparameters, configurations, datasets, and code versions. It is sometimes hard to keep track of what you\u20263 min read\u00b7Aug 16, 20231141Moez AliEasily deploy machine learning models from the comfort of your NotebookStreamline your Machine Learning model deployment with Modelbit.com. Seamlessly move from Jupyter notebooks to production-ready REST APIs\u20269 min read\u00b7Sep 8, 20232621AserdargunAdvanced OOP in PythonClasses and objects: Class instantiation, self, data attributes, UML, methods, __str__, __repr__72 min read\u00b7Jan 146136DatadriftersinDev GeniusDeploy XGBoost to Local Kubernetes Cluster using FastAPI: Movie Recommendation Use CaseEfficient deployment and scaling of Machine Learning models is critical for user experience in many real-world applications.\u00b711 min read\u00b7Oct 20, 2023761See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Write a Medium tutorial post introducing the first lesson of a seven-part course on MLOps, focusing on building a production-ready feature engineering pipeline, including batch serving, using feature stores like Hopsworks, and designing pipelines for data processing and model training. The post should target mid to advanced machine learning engineers, emphasizing hands-on project building to forecast energy consumption in Denmark.",
        "post": "A Framework for Building a Production-Ready Feature Engineering Pipeline | by Paul Iusztin | Towards Data ScienceOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyThe Full Stack 7-Steps MLOps FrameworkA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.Paul Iusztin\u00b7FollowPublished inTowards Data Science\u00b713 min read\u00b7Apr 28, 202376111ListenSharePhoto by Hassan Pasha on UnsplashThis tutorial represents lesson 1 out of a 7-lesson course that will walk you step-by-step through how to design, implement, and deploy an ML system using MLOps good practices. During the course, you will build a production-ready model to forecast energy consumption levels for the next 24 hours across multiple consumer types from Denmark.By the end of this course, you will understand all the fundamentals of designing, coding and deploying an ML system using a batch-serving architecture.This course targets mid/advanced machine learning engineers who want to level up their skills by building their own end-to-end projects.Nowadays, certificates are everywhere. Building advanced end-to-end projects that you can later show off is the best way to get recognition as a professional engineer.Table of Contents:Course IntroductionCourse LessonsData SourceLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.Lesson 1: CodeConclusionReferencesIntroductionAt the end of this 7 lessons course, you will know how to:design a batch-serving architectureuse Hopsworks as a feature storedesign a feature engineering pipeline that reads data from an APIbuild a training pipeline with hyper-parameter tunninguse W&B as an ML Platform to track your experiments, models, and metadataimplement a batch prediction pipelineuse Poetry to build your own Python packagesdeploy your own private PyPi serverCreate an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0in76176111FollowWritten by Paul Iusztin1.5K Followers\u00b7Writer for Towards Data Science\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul Iusztin and Towards Data SciencePaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Sheila TeoinTowards Data ScienceHow I Won Singapore\u2019s GPT-4 Prompt Engineering CompetitionA deep dive into the strategies I learned for harnessing the power of Large Language Models (LLMs)\u00b723 min read\u00b7Dec 29, 202311.2K125Cristian LeoinTowards Data ScienceThe Math behind Adam OptimizerWhy is Adam the most popular optimizer in Deep Learning? Let\u2019s understand it by diving into its math, and recreating the algorithm.16 min read\u00b7Jan 301.3K9Paul IusztininDecoding MLThe ultimate guide on installing PyTorch with CUDA support in all possible ways\u2192 Using Pip, Conda, Poetry, Docker, or directly on the system\u00b710 min read\u00b7Jan 42231See all from Paul IusztinSee all from Towards Data ScienceRecommended from MediumPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Deepanshu tyagiMLOps 2024 Roadmap: From Zero to ProIn the rapidly evolving landscape of artificial intelligence and machine learning, the significance of operationalizing machine learning\u20264 min read\u00b7Jan 36034ListsPredictive Modeling w/ Python20 stories\u00b7891 savesPractical Guides to Machine Learning10 stories\u00b71032 savesNatural Language Processing1179 stories\u00b7656 savesThe New Chatbots: ChatGPT, Bard, and Beyond12 stories\u00b7301 savesAnkush k SingalinILLUMINATIONCrafting Quality Python Projects: A Unique Template for Success \u2014 MLOPS-IAnkush k Singal\u00b716 min read\u00b7Oct 25, 20231692Moez AliEasily deploy machine learning models from the comfort of your NotebookStreamline your Machine Learning model deployment with Modelbit.com. Seamlessly move from Jupyter notebooks to production-ready REST APIs\u20269 min read\u00b7Sep 8, 20232621DatadriftersinDev GeniusDeploy XGBoost to Local Kubernetes Cluster using FastAPI: Movie Recommendation Use CaseEfficient deployment and scaling of Machine Learning models is critical for user experience in many real-world applications.\u00b711 min read\u00b7Oct 20, 2023761Nicolas PogeantinMLearning.aiBuilding a Scalable MLOps Stack\u200a\u2014\u200a From Data to DeploymentThis article explores the development of a streamlined ML system, showcasing the journey from data to deployment, simplifying complex\u2026\u00b75 min read\u00b7Sep 8, 202354See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Write a LinkedIn post explaining a 7-step process on how to fine-tune an open-source LLM to create a real-time financial advisor.",
        "post": "\ud835\udff3-\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfd\ud835\ude00 on \ud835\uddf5\ud835\uddfc\ud835\ude04 to \ud835\uddf3\ud835\uddf6\ud835\uddfb\ud835\uddf2-\ud835\ude01\ud835\ude02\ud835\uddfb\ud835\uddf2 an open-source \ud835\udddf\ud835\udddf\ud835\udde0 to create your real-time financial advisor \u2193\n\nIn the past weeks, we covered \ud835\ude04\ud835\uddf5\ud835\ude06 you must fine-tune an LLM and \ud835\ude04\ud835\uddf5\ud835\uddee\ud835\ude01 resources & tools you need.\n\n.\n\nNow, let's see how you can hook all the pieces together into a single fine-tuning module \u2193\n\n\ud835\udfed. \ud835\udddf\ud835\uddfc\ud835\uddee\ud835\uddf1 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\udde4&\ud835\uddd4 \ud835\uddf1\ud835\uddee\ud835\ude01\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude01\n\nOur Q&A samples have the following structure keys: \"about_me,\" \"user_context,\" \"question,\" and \"answer.\"\n\nFor task-specific fine-tuning, you need only 100-1000 samples. Thus, you can directly load the whole JSON in memory.\n\nAfter you map every sample to a list of Python \ud835\ude25\ud835\ude22\ud835\ude35\ud835\ude22\ud835\ude24\ud835\ude2d\ud835\ude22\ud835\ude34\ud835\ude34\ud835\ude26\ud835\ude34 to validate the structure & type of the ingested instances.\n\n\ud835\udfee. \ud835\udde3\ud835\uddff\ud835\uddf2\ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddf0\ud835\uddf2\ud835\ude00\ud835\ude00 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\udde4&\ud835\uddd4 \ud835\uddf1\ud835\uddee\ud835\ude01\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude01 \ud835\uddf6\ud835\uddfb\ud835\ude01\ud835\uddfc \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddfa\ud835\uddfd\ud835\ude01\ud835\ude00\n\nThe first step is to use \ud835\ude36\ud835\ude2f\ud835\ude34\ud835\ude35\ud835\ude33\ud835\ude36\ud835\ude24\ud835\ude35\ud835\ude36\ud835\ude33\ud835\ude26\ud835\ude25 to clean every sample by removing redundant characters.\n\nAfter, as every sample consists of multiple fields, you must map it to a single piece of text, known as the prompt.\n\nTo do so, you define a \ud835\ude17\ud835\ude33\ud835\ude30\ud835\ude2e\ud835\ude31\ud835\ude35\ud835\ude1b\ud835\ude26\ud835\ude2e\ud835\ude31\ud835\ude2d\ud835\ude22\ud835\ude35\ud835\ude26 class to manage all your prompts. You will use it to map all the sample keys to a prompt using f-strings.\n\nThe last step is to map the list of Python \ud835\ude25\ud835\ude22\ud835\ude35\ud835\ude22\ud835\ude24\ud835\ude2d\ud835\ude22\ud835\ude34\ud835\ude34\ud835\ude26\ud835\ude34 to a HuggingFace dataset and map every sample to a prompt, as discussed above.\n\n\ud835\udfef. \ud835\udddf\ud835\uddfc\ud835\uddee\ud835\uddf1 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\ude02\ud835\ude00\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\udde4\ud835\udddf\ud835\uddfc\ud835\udde5\ud835\uddd4\n\nLoad a pretrained Falcon 7B LLM by passing a \ud835\ude23\ud835\ude2a\ud835\ude35\ud835\ude34\ud835\ude22\ud835\ude2f\ud835\ude25\ud835\ude23\ud835\ude3a\ud835\ude35\ud835\ude26\ud835\ude34 quantization configuration that loads all the weights on 4 bits.\n\nAfter using LoRA, you freeze the weights of the original Falcon LLM and attach a set of trainable adapters to it.\n\n\ud835\udff0. \ud835\uddd9\ud835\uddf6\ud835\uddfb\ud835\uddf2-\ud835\ude01\ud835\ude02\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4\n\nThe \ud835\ude35\ud835\ude33\ud835\ude2d package makes this step simple.\n\nYou pass to the \ud835\ude1a\ud835\ude0d\ud835\ude1b\ud835\ude1b\ud835\ude33\ud835\ude22\ud835\ude2a\ud835\ude2f\ud835\ude26\ud835\ude33 class the training arguments, the dataset and the model and call the \ud835\ude35\ud835\ude33\ud835\ude22\ud835\ude2a\ud835\ude2f() method.\n\nOne crucial aspect is configuring an experiment tracker, such as Comet ML, to log all vital metrics & artifacts.\n\n\ud835\udff1. \ud835\udde3\ud835\ude02\ud835\ude00\ud835\uddf5 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddef\ud835\uddf2\ud835\ude00\ud835\ude01 \ud835\uddfa\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddf9 \ud835\ude01\ud835\uddfc \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddfa\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddf9 \ud835\uddff\ud835\uddf2\ud835\uddf4\ud835\uddf6\ud835\ude00\ud835\ude01\ud835\uddff\ud835\ude06\n\nOne of the final steps is to attach a callback to the \ud835\ude1a\ud835\ude0d\ud835\ude1b\ud835\ude1b\ud835\ude33\ud835\ude22\ud835\ude2a\ud835\ude2f\ud835\ude26\ud835\ude33 class that runs when the training ends to push the model with the lowest loss to the model registry as the new production candidate.\n\n\ud835\udff2. \ud835\uddd8\ud835\ude03\ud835\uddee\ud835\uddf9\ud835\ude02\ud835\uddee\ud835\ude01\ud835\uddf2 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddfb\ud835\uddf2\ud835\ude04 \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddf1\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\uddf0\ud835\uddee\ud835\uddfb\ud835\uddf1\ud835\uddf6\ud835\uddf1\ud835\uddee\ud835\ude01\ud835\uddf2\n\nEvaluating generative AI models is tricky.\n\nYou can run the LLM on the test set and log the prompts & answers to Comet ML's monitoring system to check them manually.\n\nIf the provided answers are valid, using the model registry dashboard, you will manually release it to replace the old LLM.\n\n\ud835\udff3. \ud835\uddd7\ud835\uddf2\ud835\uddfd\ud835\uddf9\ud835\uddfc\ud835\ude06 \ud835\ude01\ud835\uddfc \ud835\uddd5\ud835\uddf2\ud835\uddee\ud835\uddfa\n\nIt is as easy as wrapping the training & inference functions (or classes) with a Python \"@\ud835\ude22\ud835\ude31\ud835\ude31.\ud835\ude33\ud835\ude36\ud835\ude2f()\" decorator."
    },
    {
        "instruction": "Compose a Medium post in which you are sharing behind-the-scenes insights and lessons from creating an MLOps course, focusing on the realities of undertaking an 'imperfect' ML project and the valuable learning opportunities it presents.",
        "post": "Imperfections Unveiled: The Intriguing Reality Behind Our MLOps Course Creation | by Paul Iusztin | Towards Data ScienceOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyTHE FULL STACK 7-STEPS MLOPS FRAMEWORKImperfections Unveiled: The Intriguing Reality Behind Our MLOps Course CreationBonus Lesson: Behind the Scenes of an \u2018Imperfect\u2019 ML Project \u2014 Lessons and InsightsPaul Iusztin\u00b7FollowPublished inTowards Data Science\u00b710 min read\u00b7Jun 19, 2023451ListenSharePhoto by Hassan Pasha on UnsplashThis article represents a last bonus lesson out of a 7-lesson course that walked you step-by-step through how to design, implement, and deploy an ML system using MLOps good practices. During the course, you built a production-ready model to forecast energy consumption levels for the next 24 hours across multiple consumer types from Denmark.During the course, you learned all the fundamentals of designing, coding and deploying an ML system using a batch-serving architecture.This course targets mid/advanced ML or software engineers who want to level up their skills by building their own ML end-to-end projects.Nowadays, certificates are everywhere. Building advanced end-to-end projects that you can later show off is the best way to get recognition as a professional engineer.Table of Contents:Course IntroductionCourse LessonsData SourceBonus Lesson: Behind the Scenes of an \u2018Imperfect\u2019 ML Project \u2014 Lessons and InsightsConclusionReferencesCourse IntroductionDuring the 7 lessons course, you learned how to:design a batch-serving architectureuse Hopsworks as a feature storedesign a feature engineering pipeline that reads data from an APIbuild a training pipeline with hyper-parameter tunninguse W&B as an ML Platform to track your experiments, models, and metadataimplement a batch prediction pipelineuse Poetry to build your own Python packagesdeploy your own private PyPi serverCreate an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0in45451FollowWritten by Paul Iusztin1.5K Followers\u00b7Writer for Towards Data Science\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul Iusztin and Towards Data SciencePaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Sheila TeoinTowards Data ScienceHow I Won Singapore\u2019s GPT-4 Prompt Engineering CompetitionA deep dive into the strategies I learned for harnessing the power of Large Language Models (LLMs)\u00b723 min read\u00b7Dec 29, 202311.2K125Cristian LeoinTowards Data ScienceThe Math behind Adam OptimizerWhy is Adam the most popular optimizer in Deep Learning? Let\u2019s understand it by diving into its math, and recreating the algorithm.16 min read\u00b7Jan 301.3K9Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111See all from Paul IusztinSee all from Towards Data ScienceRecommended from MediumPaul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111Ankush k SingalinILLUMINATIONCrafting Quality Python Projects: A Unique Template for Success \u2014 MLOPS-IAnkush k Singal\u00b716 min read\u00b7Oct 25, 20231692ListsPredictive Modeling w/ Python20 stories\u00b7891 savesPractical Guides to Machine Learning10 stories\u00b71032 savesNatural Language Processing1179 stories\u00b7656 savesThe New Chatbots: ChatGPT, Bard, and Beyond12 stories\u00b7301 savesMohammad FaizaninMLearning.aiMLOps: Weights & Biases Experiments TrackingMachine Learning projects have hyperparameters, configurations, datasets, and code versions. It is sometimes hard to keep track of what you\u20263 min read\u00b7Aug 16, 20231141Moez AliEasily deploy machine learning models from the comfort of your NotebookStreamline your Machine Learning model deployment with Modelbit.com. Seamlessly move from Jupyter notebooks to production-ready REST APIs\u20269 min read\u00b7Sep 8, 20232621Vasil DedejskiServing machine learning models with Ray ServeBuilding intelligent solutions that are driven by data and utilizing machine learning models is one of the most interesting emerging\u20265 min read\u00b7Aug 26, 202312SemaphoreMLOps: From Jupyter to ProductionJupyter notebooks are great for learning and running experiments on Machine Learning. They, however, fall short when it comes to\u20266 min read\u00b7Feb 127See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Develop an in-depth tutorial to be posted on Medium for Lesson 7 of an eight-part series on creating a RAG LangChain application, emphasizing LLMOps, model registry, vector databases, embedding LM, and the deployment of these components using serverless architecture and RESTful API.",
        "post": "RAG LangChain app using the 3-pipeline design | Decoding MLOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyThe Hands-On LLMs SeriesDesign a RAG LangChain application leveraging the 3-pipeline architectureLesson 7: LLMOps. Model registry. Vector DBs. Embedding LM.Paul Iusztin\u00b7FollowPublished inDecoding ML\u00b718 min read\u00b7Jan 30115ListenShareImage by DALL-E\u2192 the 7th out of 8 lessons of the Hands-On LLMs free courseBy finishing the Hands-On LLMs free course, you will learn how to use the 3-pipeline architecture & LLMOps good practices to design, build, and deploy a real-time financial advisor powered by LLMs & vector DBs.We will primarily focus on the engineering & MLOps aspects. Thus, by the end of this series, you will know how to build & deploy a real ML system, not some isolated code in Notebooks (we haven\u2019t used any Notebooks at all).More precisely, these are the 3 components you will learn to build:a real-time streaming pipeline (deployed on AWS) that listens to financial news, cleans & embeds the documents, and loads them to a vector DBa fine-tuning pipeline (deployed as a serverless continuous training) that fine-tunes an LLM on financial data using QLoRA, monitors the experiments using an experiment tracker and saves the best model to a model registryan inference pipeline built in LangChain (deployed as a serverless RESTful API) that loads the fine-tuned LLM from the model registry and answers financial questions using RAG (leveraging the vector DB populated with financial news in real-time)High-level architecture of the Hands-On LLMs FREE course [Image by the Author].We will also show you how to integrate various serverless tools, such as:Comet ML: as your ML Platform;Qdrant: as your vector DB;Beam: as your serverless infrastructure.Curios? Check the video below to understand better what you will learn \u2193Create an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0in115115FollowWritten by Paul Iusztin1.5K Followers\u00b7Editor for Decoding ML\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul Iusztin and Decoding MLPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Paul IusztininDecoding MLThe ultimate guide on installing PyTorch with CUDA support in all possible ways\u2192 Using Pip, Conda, Poetry, Docker, or directly on the system\u00b710 min read\u00b7Jan 42231Paul IusztininDecoding MLPrepare your RAG LangChain application for productionLesson 8: LLMOps. Serverless infrastructure. RESTful API. Gradio.\u00b719 min read\u00b7Feb 3208Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111See all from Paul IusztinSee all from Decoding MLRecommended from MediumPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Guodong (Troy) ZhaoinBootcampHow to improve RAG results in your LLM apps: from basics to advancedImprove your RAG quality and latency in your LLM app13 min read\u00b7Jan 228995ListsPredictive Modeling w/ Python20 stories\u00b7891 savesPractical Guides to Machine Learning10 stories\u00b71032 savesNatural Language Processing1179 stories\u00b7656 savesdata science and AI40 stories\u00b767 savesSuman DasFine Tune Large Language Model (LLM) on a Custom Dataset with QLoRAThe field of natural language processing has been revolutionized by large language models (LLMs), which showcase advanced capabilities and\u202615 min read\u00b7Jan 253383Ivan ReznikovinTowards AILangChain 101: Part 3a. Talking to Documents: Load, Split and simple RAG with LCELThis is Part 3 of the Langchain 101 series, where we\u2019ll discuss how to load data, split it, store data, and create simple RAG with LCEL9 min read\u00b75 days ago1051Senthil EinLevel Up CodingUnlocking LLM\u2019s Potential with RAG: A Complete Guide from Basics to Advanced TechniquesUsing OpenAI, Google Gemini Pro, and Open Source Models47 min read\u00b76 days ago6916AndreiinLlamaIndex BlogA Cheat Sheet and Some Recipes For Building Advanced RAGIt\u2019s the start of a new year and perhaps you\u2019re looking to break into the RAG scene by building your very first RAG system. Or, maybe\u20267 min read\u00b7Jan 51.5K5See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Explain the five essential concepts that should be part of every LLM fine-tuning toolkit, as detailed in Lesson 4 of an eight-part series. Discuss in a Medium article generating a Q&A dataset, understanding the three stages of training, and comparing encoder vs. decoder LLMs among other critical insights.",
        "post": "5 core LLM concepts you must know | Decoding MLOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyThe Hands-On LLMs Series5 concepts that must be in your LLM fine-tuning kitLesson 4: Generate a Q&A dataset, the 3 stages of training, encoder vs. decoder LLMs and more!Paul Iusztin\u00b7FollowPublished inDecoding ML\u00b715 min read\u00b7Jan 16116ListenShareImage by DALL-E\u2192 the 4th out of 8 lessons of the Hands-On LLMs free courseBy finishing the Hands-On LLMs free course, you will learn how to use the 3-pipeline architecture & LLMOps good practices to design, build, and deploy a real-time financial advisor powered by LLMs & vector DBs.We will primarily focus on the engineering & MLOps aspects. Thus, by the end of this series, you will know how to build & deploy a real ML system, not some isolated code in Notebooks (we haven\u2019t used any Notebooks at all).More precisely, these are the 3 components you will learn to build:a real-time streaming pipeline (deployed on AWS) that listens to financial news, cleans & embeds the documents, and loads them to a vector DBa fine-tuning pipeline (deployed as a serverless continuous training) that fine-tunes an LLM on financial data using QLoRA, monitors the experiments using an experiment tracker and saves the best model to a model registryan inference pipeline built in LangChain (deployed as a serverless RESTful API) that loads the fine-tuned LLM from the model registry and answers financial questions using RAG (leveraging the vector DB populated with financial news in real-time)High-level architecture of the Hands-On LLMs FREE course [Image by the Author].We will also show you how to integrate various serverless tools, such as:Comet ML as your ML Platform;Qdrant as your vector DB;Beam as your infrastructure.Curios? Check the video below to understand better what you will learn \u2193Create an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0in116116FollowWritten by Paul Iusztin1.5K Followers\u00b7Editor for Decoding ML\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul Iusztin and Decoding MLPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Paul IusztininDecoding MLThe ultimate guide on installing PyTorch with CUDA support in all possible ways\u2192 Using Pip, Conda, Poetry, Docker, or directly on the system\u00b710 min read\u00b7Jan 42231Paul IusztininDecoding MLPrepare your RAG LangChain application for productionLesson 8: LLMOps. Serverless infrastructure. RESTful API. Gradio.\u00b719 min read\u00b7Feb 3208Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111See all from Paul IusztinSee all from Decoding MLRecommended from MediumPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Suman DasFine Tune Large Language Model (LLM) on a Custom Dataset with QLoRAThe field of natural language processing has been revolutionized by large language models (LLMs), which showcase advanced capabilities and\u202615 min read\u00b7Jan 253383ListsPredictive Modeling w/ Python20 stories\u00b7891 savesNatural Language Processing1179 stories\u00b7656 savesPractical Guides to Machine Learning10 stories\u00b71032 savesdata science and AI40 stories\u00b767 savesRania _HossamLLAMA FROM SCRATCHMeta AI and Microsoft have joined forces to introduce Llama 2, the next generation of Meta\u2019s open-source large language model.13 min read\u00b7Oct 22, 20233222Senthil EinLevel Up CodingUnlocking LLM\u2019s Potential with RAG: A Complete Guide from Basics to Advanced TechniquesUsing OpenAI, Google Gemini Pro, and Open Source Models47 min read\u00b76 days ago6916Thu VuinTowards Data ScienceHow to Learn AI on Your Own (a self-study guide)If your hands touch a keyboard for work, Artificial Intelligence is going to change your job in the next few years.\u00b712 min read\u00b7Jan 53.2K27Micha\u0142 Marci\u0144czuk, Ph.D.inCodeNLPCost of running ML experiments on GPU\u200a\u2014\u200aAWS Cloud vs local GPUDid you know that you are giving away an RTX 4090 for free by running ML experiments for a year?\u00b74 min read\u00b7Nov 11, 2023893See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Create a detailed guide for Medium on designing advanced Scikit-Learn pipelines for machine learning projects, including how to use advanced components for custom model creation. Provide examples for building a forecasting model, emphasizing modularity and scalability.",
        "post": "How to Quickly Design Advanced Sklearn Pipelines | Towards Data ScienceOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyTutorialHow to Quickly Design Advanced Sklearn PipelinesCompose all the components from Scikit-Learn Pipelines to build custom production-ready modelsPaul Iusztin\u00b7FollowPublished inTowards Data Science\u00b710 min read\u00b7Nov 2, 20223212ListenSharePhoto by Clint Patterson on UnsplashThis tutorial will teach you how and when to use all the advanced tools from the Sklearn Pipelines ecosystem to build custom, scalable, and modular machine learning models that can easily be deployed in production.In isolation, there is much content about different components from the Sklearn Pipelines toolbox. I am writing this tutorial because it is precious to see how all those components work together into a single, more complex system.I will use a concrete example and show you how and when to use the following components:TransformerMixinBaseEstimatorFunctionTransformerColumnTransformerFeatureUnionTransformedTargetRegressorKnowing how to use them individually is easy, that is why this tutorial will emphasize on when to use them and how to use them interchangeably in a complex system.GoalWe will build a forecasting model to predict the following year's global mean wheat yield.The main focus will be on the advanced concepts of the Sklearn Pipeline components. Therefore, we won't spend much time on other data science principles.Table of ContentsDatasetSummary of Pipeline FundamentalsConfigurationData PreparationBuilding the PipelineGlobal Pipeline. Let's Put Things Together.How to Use the Global PipelineNOTE: If you are interested only in the advanced topics of Sklearn Pipelines, skip directly to Building the Pipeline.DatasetWe are using a publicly available dataset [1] provided by Pangaea, which tracks global historical yearly yields for various plants from 1981 to 2016.Create an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0in3213212FollowWritten by Paul Iusztin1.5K Followers\u00b7Writer for Towards Data Science\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul Iusztin and Towards Data SciencePaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Sheila TeoinTowards Data ScienceHow I Won Singapore\u2019s GPT-4 Prompt Engineering CompetitionA deep dive into the strategies I learned for harnessing the power of Large Language Models (LLMs)\u00b723 min read\u00b7Dec 29, 202311.2K125Cristian LeoinTowards Data ScienceThe Math behind Adam OptimizerWhy is Adam the most popular optimizer in Deep Learning? Let\u2019s understand it by diving into its math, and recreating the algorithm.16 min read\u00b7Jan 301.3K9Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111See all from Paul IusztinSee all from Towards Data ScienceRecommended from MediumMarco PeixeiroinTowards Data ScienceA Practical Guide to Feature Selection Using SklearnA hands-on tutorial on how to select the best features for your model using Python and scikit-learn\u00b710 min read\u00b7Sep 27, 20221061DatadriftersinDev GeniusDeploy XGBoost to Local Kubernetes Cluster using FastAPI: Movie Recommendation Use CaseEfficient deployment and scaling of Machine Learning models is critical for user experience in many real-world applications.\u00b711 min read\u00b7Oct 20, 2023761ListsCoding & Development11 stories\u00b7442 savesPredictive Modeling w/ Python20 stories\u00b7891 savesPractical Guides to Machine Learning10 stories\u00b71032 savesChatGPT21 stories\u00b7454 savesAbish PiusinWriting in the World of Artificial IntelligenceRevisit the Fundamentals: Using Sklearn Pipelines for Faster, Robust and Reproducible Machine\u2026Data science and machine learning workflows often involve multiple steps, from data preprocessing to model training and evaluation\u2026\u00b76 min read\u00b7Nov 12, 20235Jones ntonganaUnderstanding Interaction and Polynomial Features in PySpark: A Simple GuideIntroduction3 min read\u00b7Sep 1, 2023Niranjan AppajiA Guide to Handling High Cardinality in Categorical VariablesHigh cardinality refers to a situation in a dataset where a particular feature has a large number of distinct values. In other words, a\u20265 min read\u00b7Dec 28, 20232Moez AliEasily deploy machine learning models from the comfort of your NotebookStreamline your Machine Learning model deployment with Modelbit.com. Seamlessly move from Jupyter notebooks to production-ready REST APIs\u20269 min read\u00b7Sep 8, 20232621See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Craft a detailed tutorial Medium post for Lesson 6 of an eight-part series on LLMOps for Medium platform, focusing on creating continuous training pipelines for LLMs. Cover serverless infrastructure, experiment trackers, model registries, and prompt testing, with a particular focus on using QLoRA for fine-tuning LLMs on financial data.",
        "post": "Continuous training LLM pipelines using LLMOps | Decoding MLOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyThe Hands-On LLMs SeriesFrom LLM development to continuous training pipelines using LLMOpsLesson 6: Serverless infrastructure. Experiment tracker. Model registry. Prompts testing.Paul Iusztin\u00b7FollowPublished inDecoding ML\u00b718 min read\u00b7Jan 24108ListenShareImage by DALL-E\u2192 the 6th out of 8 lessons of the Hands-On LLMs free courseBy finishing the Hands-On LLMs free course, you will learn how to use the 3-pipeline architecture & LLMOps good practices to design, build, and deploy a real-time financial advisor powered by LLMs & vector DBs.We will primarily focus on the engineering & MLOps aspects. Thus, by the end of this series, you will know how to build & deploy a real ML system, not some isolated code in Notebooks (we haven\u2019t used any Notebooks at all).More precisely, these are the 3 components you will learn to build:a real-time streaming pipeline (deployed on AWS) that listens to financial news, cleans & embeds the documents, and loads them to a vector DBa fine-tuning pipeline (deployed as a serverless continuous training) that fine-tunes an LLM on financial data using QLoRA, monitors the experiments using an experiment tracker and saves the best model to a model registryan inference pipeline built in LangChain (deployed as a serverless RESTful API) that loads the fine-tuned LLM from the model registry and answers financial questions using RAG (leveraging the vector DB populated with financial news in real-time)High-level architecture of the Hands-On LLMs FREE course [Image by the Author].We will also show you how to integrate various serverless tools, such as:Comet ML: as your ML Platform;Qdrant: as your vector DB;Beam: as your serverless infrastructure.Curios? Check the video below to understand better what you will learn \u2193Create an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0in108108FollowWritten by Paul Iusztin1.5K Followers\u00b7Editor for Decoding ML\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul Iusztin and Decoding MLPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Paul IusztininDecoding MLThe ultimate guide on installing PyTorch with CUDA support in all possible ways\u2192 Using Pip, Conda, Poetry, Docker, or directly on the system\u00b710 min read\u00b7Jan 42231Paul IusztininDecoding MLPrepare your RAG LangChain application for productionLesson 8: LLMOps. Serverless infrastructure. RESTful API. Gradio.\u00b719 min read\u00b7Feb 3208Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111See all from Paul IusztinSee all from Decoding MLRecommended from MediumPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Suman DasFine Tune Large Language Model (LLM) on a Custom Dataset with QLoRAThe field of natural language processing has been revolutionized by large language models (LLMs), which showcase advanced capabilities and\u202615 min read\u00b7Jan 253383ListsPredictive Modeling w/ Python20 stories\u00b7891 savesPractical Guides to Machine Learning10 stories\u00b71032 savesNatural Language Processing1179 stories\u00b7656 savesdata science and AI40 stories\u00b767 savesMohammad FaizaninMLearning.aiMLOps: Weights & Biases Experiments TrackingMachine Learning projects have hyperparameters, configurations, datasets, and code versions. It is sometimes hard to keep track of what you\u20263 min read\u00b7Aug 16, 20231141Matt WhiteLLMOps is Dead! Long Live GenOps!Generative AI adoption requires a holistic approach that will scale to handle all modalities and implementation methods, not just language.7 min read\u00b7Nov 27, 2023120Ali ArabiLLMOps\u200a\u2014\u200aMLOps for LLMs on AWSCan also see this post on the Caylent blog.9 min read\u00b7Oct 12, 202316Bijit GhoshEnd-to-End LLMOps PlatformLarge language models (LLMs) like GPT-4, LlaMA, Falcon, Claude, Cohere, PaLM, have demonstrated immense capabilities for natural language\u20268 min read\u00b7Nov 6, 2023541See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Craft a LinkedIn post discussing the importance of feature stores in ML production systems, highlighting three key aspects.",
        "post": "These are 3 characteristics showing that a feature store is a crucial component within any end-to-end ML production system.\n.\n#\ud835\udfcf \ud835\udc05\ud835\udc1e\ud835\udc1a\ud835\udc2d\ud835\udc2e\ud835\udc2b\ud835\udc1e \ud835\udc26\ud835\udc1a\ud835\udc27\ud835\udc1a\ud835\udc20\ud835\udc1e\ud835\udc26\ud835\udc1e\ud835\udc27\ud835\udc2d\nFeature management is the ability to discover and share features across teams. This characteristic may save time and computation as multiple features can be shared across models. Also, it helps share knowledge across teams or projects.\n#\ud835\udfd0 \ud835\udc05\ud835\udc1e\ud835\udc1a\ud835\udc2d\ud835\udc2e\ud835\udc2b\ud835\udc1e \ud835\udc1c\ud835\udc28\ud835\udc26\ud835\udc29\ud835\udc2e\ud835\udc2d\ud835\udc1a\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27\nFeature computation means you compute your features only once and store them for future uses. In this case, it acts like a data warehouse, and it may save a lot of time and computation, especially when computing specific features consume a lot of resources (usually the case).\n#\ud835\udfd1 \ud835\udc05\ud835\udc1e\ud835\udc1a\ud835\udc2d\ud835\udc2e\ud835\udc2b\ud835\udc1e \ud835\udc1c\ud835\udc28\ud835\udc27\ud835\udc2c\ud835\udc22\ud835\udc2c\ud835\udc2d\ud835\udc1e\ud835\udc27\ud835\udc1c\ud835\udc32\nFeature consistency refers to the problem of having two separate pipelines for the model between training and production (known as the training-serving skew issue). Using a feature store, you unify the logic of feature creation, using the feature store both in training and production.\n.\nI am curious to know what tools you use for your feature store solutions. Please Let me know if the comments below \ud83d\udc47\nhashtag\n#\nfeaturestore\nhashtag\n#\ndatawarehouse\nhashtag\n#\nmlops\n-----\n\ud83d\udca1 My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n."
    },
    {
        "instruction": "Create a LinkedIn post about evaluating the necessity of additional data for improving a model's performance using learning curves.",
        "post": "Everybody says you can improve your model by training it on more data. Is it true? How can we test this theory before spending money and effort collecting that new data?\n.\nGood news. The solution is quite simple.\nUsing the \ud835\udc0b\ud835\udc1e\ud835\udc1a\ud835\udc2b\ud835\udc27\ud835\udc22\ud835\udc27\ud835\udc20 \ud835\udc02\ud835\udc2e\ud835\udc2b\ud835\udc2f\ud835\udc1e graph, you can quickly see the performance evolution of the model with various amounts of data.\nIt is important to validate this because more data is not the answer to all your problems. \ud83d\ude02\n.\nThe main idea of the \ud835\udc0b\ud835\udc1e\ud835\udc1a\ud835\udc2b\ud835\udc27\ud835\udc22\ud835\udc27\ud835\udc20 \ud835\udc02\ud835\udc2e\ud835\udc2b\ud835\udc2f\ud835\udc1e method is simple:\n1. You create N subsets from your training dataset. Where the first subset is the smallest. You keep increasing the subset size at a given rate until you reach the full dataset size: 1 < 2 < 3 < ... < N.\n2. You train your model on subsample 1.\n3. You test your, model on the test split (which is not divided; it is not good practice to touch your test split).\n4. Save the results.\n5. Repeat steps 2-4 on all your subsets.\n6. Plot the results.\n.\nUsing this mechanism, you can simulate if adding more data will boost your model's performance.\nAlso, you can quickly compare different models and evaluate which one needs more/fewer data to achieve your desired accuracy.\n.\nCheck out how easy it is to do this with Sklearn:\nhttps://lnkd.in/d358Dhhd\n.\n\ud83d\udca1 My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n."
    },
    {
        "instruction": "Promote an upcoming Q&A session with Paul Iusztin, highlighting the opportunity for the audience to engage and ask anything about MLOps.",
        "post": "Next Tuesday we'll have the 2nd edition of \ud835\uddd4\ud835\ude00\ud835\uddf8 \ud835\udde0\ud835\uddee\ud835\uddff\ud835\ude03\ud835\uddf2\ud835\uddf9\ud835\uddfc\ud835\ude02\ud835\ude00 \ud835\udde0\ud835\udddf\ud835\udde2\ud835\uddfd\ud835\ude00 \ud835\uddd4\ud835\uddfb\ud835\ude06\ud835\ude01\ud835\uddf5\ud835\uddf6\ud835\uddfb\ud835\uddf4: A live Q&A sessions where we try to answer all your questions \ud83e\udd17 This time Paul Iusztin will join us, sign up here: https://lnkd.in/eVk7e8k5"
    },
    {
        "instruction": "Explain the 3-pipeline architecture for ML systems in A LinkedIN post, highlighting its benefits for structure, modularity, collaboration, and efficient transition from development to production in MLOps practices.",
        "post": "We all know how \ud835\uddfa\ud835\uddf2\ud835\ude00\ud835\ude00\ud835\ude06 \ud835\udde0\ud835\udddf \ud835\ude00\ud835\ude06\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfa\ud835\ude00 can get. That is where the \ud835\udfef-\ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2 \ud835\uddee\ud835\uddff\ud835\uddf0\ud835\uddf5\ud835\uddf6\ud835\ude01\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2 \ud835\uddf8\ud835\uddf6\ud835\uddf0\ud835\uddf8\ud835\ude00 \ud835\uddf6\ud835\uddfb.\nThe 3-pipeline design is a way to bring structure & modularity to your ML system and improve your MLOps processes.\nThis is how \u2193\n=== \ud835\udde3\ud835\uddff\ud835\uddfc\ud835\uddef\ud835\uddf9\ud835\uddf2\ud835\uddfa ===\nDespite advances in MLOps tooling, transitioning from prototype to production remains challenging.\nIn 2022, only 54% of the models get into production. Auch.\nSo what happens?\nSometimes the model is not mature enough, sometimes there are some security risks, but most of the time...\n...the architecture of the ML system is built with research in mind, or the ML system becomes a massive monolith that is extremely hard to refactor from offline to online.\nSo, good processes and a well-defined architecture are as crucial as good tools and models.\n=== \ud835\udde6\ud835\uddfc\ud835\uddf9\ud835\ude02\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb ===\n\ud835\ude1b\ud835\ude29\ud835\ude26 3-\ud835\ude31\ud835\ude2a\ud835\ude31\ud835\ude26\ud835\ude2d\ud835\ude2a\ud835\ude2f\ud835\ude26 \ud835\ude22\ud835\ude33\ud835\ude24\ud835\ude29\ud835\ude2a\ud835\ude35\ud835\ude26\ud835\ude24\ud835\ude35\ud835\ude36\ud835\ude33\ud835\ude26.\nFirst, let's understand what the 3-pipeline design is.\nIt is a mental map that helps you simplify the development process and split your monolithic ML pipeline into 3 components:\n1. the feature pipeline\n2. the training pipeline\n3. the inference pipeline\n...also known as the Feature/Training/Inference (FTI) architecture.\n.\n#\ud835\udfed. The feature pipeline transforms your data into features & labels, which are stored and versioned in a feature store.\n#\ud835\udfee. The training pipeline ingests a specific version of the features & labels from the feature store and outputs the trained models, which are stored and versioned inside a model registry.\n#\ud835\udfef. The inference pipeline takes a given version of the features and trained models and outputs the predictions to a client.\n.\nThis is why the 3-pipeline design is so beautiful:\n- it is intuitive\n- it brings structure, as on a higher level, all ML systems can be reduced to these 3 components\n- it defines a transparent interface between the 3 components, making it easier for multiple teams to collaborate\n- the ML system has been built with modularity in mind since the beginning\n- the 3 components can easily be divided between multiple teams (if necessary)\n- every component can use the best stack of technologies available for the job\n- every component can be deployed, scaled, and monitored independently\n- the feature pipeline can easily be either batch, streaming or both\nBut the most important benefit is that...\n...by following this pattern, you know 100% that your ML model will move out of your Notebooks into production.\n.\nWhat do you think about the 3-pipeline architecture? Have you used it?\nIf you want to know more about the 3-pipeline design, I recommend this awesome article from\nHopsworks\n\u2193\n\u21b3\ud83d\udd17 From MLOps to ML Systems with Feature/Training/Inference Pipelines:\nhttps://lnkd.in/dRnhHDdg\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\n\ud83d\udca1 Follow me for daily lessons about MLE and MLOps."
    },
    {
        "instruction": "Craft a Medium tutorial post detailing the final lesson of an eight-part series on creating a production-ready RAG application using LangChain, focusing on LLMOps, prompt monitoring, and serverless deployment with RESTful APIs and Gradio for a real-time financial advisor system.",
        "post": "Production-ready RAG LangChain application | Decoding MLOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyThe Hands-On LLMs SeriesPrepare your RAG LangChain application for productionLesson 8: LLMOps. Prompt monitoring. Serverless infrastructure. RESTful API. Gradio.Paul Iusztin\u00b7FollowPublished inDecoding ML\u00b719 min read\u00b7Feb 3208ListenShareImage by DALL-E\u2192 the 8th out of 8 lessons of the Hands-On LLMs free courseBy finishing the Hands-On LLMs free course, you will learn how to use the 3-pipeline architecture & LLMOps good practices to design, build, and deploy a real-time financial advisor powered by LLMs & vector DBs.We will primarily focus on the engineering & MLOps aspects. Thus, by the end of this series, you will know how to build & deploy a real ML system, not some isolated code in Notebooks (we haven\u2019t used any Notebooks at all).More precisely, these are the 3 components you will learn to build:a real-time streaming pipeline (deployed on AWS) that listens to financial news, cleans & embeds the documents, and loads them to a vector DBa fine-tuning pipeline (deployed as a serverless continuous training) that fine-tunes an LLM on financial data using QLoRA, monitors the experiments using an experiment tracker and saves the best model to a model registryan inference pipeline built in LangChain (deployed as a serverless RESTful API) that loads the fine-tuned LLM from the model registry and answers financial questions using RAG (leveraging the vector DB populated with financial news in real-time)High-level architecture of the Hands-On LLMs FREE course [Image by the Author].We will also show you how to integrate various serverless tools, such as:Comet ML: as your ML Platform;Qdrant: as your vector DB;Beam: as your serverless infrastructure.Curios? Check the video below to understand better what you will learn \u2193Create an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0in208208FollowWritten by Paul Iusztin1.5K Followers\u00b7Editor for Decoding ML\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul Iusztin and Decoding MLPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Paul IusztininDecoding MLThe ultimate guide on installing PyTorch with CUDA support in all possible ways\u2192 Using Pip, Conda, Poetry, Docker, or directly on the system\u00b710 min read\u00b7Jan 42231Paul IusztininDecoding MLThis is how you can build & deploy a streaming pipeline to populate a vector DB for real-time RAGLesson 3: Real-time text embeddings. Docker. GitHub Actions CI/CD pipeline. AWS\u00b720 min read\u00b7Jan 111861Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111See all from Paul IusztinSee all from Decoding MLRecommended from MediumIvan ReznikovinTowards AILangChain 101: Part 3a. Talking to Documents: Load, Split and simple RAG with LCELThis is Part 3 of the Langchain 101 series, where we\u2019ll discuss how to load data, split it, store data, and create simple RAG with LCEL9 min read\u00b75 days ago1051Senthil EinLevel Up CodingUnlocking LLM\u2019s Potential with RAG: A Complete Guide from Basics to Advanced TechniquesUsing OpenAI, Google Gemini Pro, and Open Source Models47 min read\u00b76 days ago6916ListsPredictive Modeling w/ Python20 stories\u00b7891 savesPractical Guides to Machine Learning10 stories\u00b71032 savesNatural Language Processing1179 stories\u00b7656 savesdata science and AI40 stories\u00b767 savesPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Suman DasFine Tune Large Language Model (LLM) on a Custom Dataset with QLoRAThe field of natural language processing has been revolutionized by large language models (LLMs), which showcase advanced capabilities and\u202615 min read\u00b7Jan 253383Guodong (Troy) ZhaoinBootcampHow to improve RAG results in your LLM apps: from basics to advancedImprove your RAG quality and latency in your LLM app13 min read\u00b7Jan 228995Param MehtainUSF-Data ScienceA LangChain chatbot using PDFsTable of j14 min read\u00b7Feb 1306See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Explain the benefits of using the walrus operator in Python for writing cleaner code.",
        "post": "Here is some \ud835\udde3\ud835\ude06\ud835\ude01\ud835\uddf5\ud835\uddfc\ud835\uddfb \ud835\ude00\ud835\ude06\ud835\uddfb\ud835\ude01\ud835\uddee\ud835\ude05 \ud835\ude00\ud835\ude02\ud835\uddf4\ud835\uddee\ud835\uddff that will help you \ud835\ude04\ud835\uddff\ud835\uddf6\ud835\ude01\ud835\uddf2 \ud835\uddf0\ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\uddfb\ud835\uddf2\ud835\uddff \ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2 \u2193 \n\nI am talking about the \ud835\ude38\ud835\ude22\ud835\ude2d\ud835\ude33\ud835\ude36\ud835\ude34 \ud835\ude30\ud835\ude31\ud835\ude26\ud835\ude33\ud835\ude22\ud835\ude35\ud835\ude30\ud835\ude33 denoted by the `:=` symbol. \n\nIt was introduced in Python 3.8, but I rarely see it used. \n\nThus, as a \"clean code\" freak, I wanted to dedicate a post to it. \n\n\ud835\uddea\ud835\uddf5\ud835\uddee\ud835\ude01 \ud835\uddf1\ud835\uddfc\ud835\uddf2\ud835\ude00 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\ude04\ud835\uddee\ud835\uddf9\ud835\uddff\ud835\ude02\ud835\ude00 \ud835\uddfc\ud835\uddfd\ud835\uddf2\ud835\uddff\ud835\uddee\ud835\ude01\ud835\uddfc\ud835\uddff \ud835\uddf1\ud835\uddfc? \n\nIt's an assignment expression that allows you to assign and return a value in the same expression. \n\n\ud835\uddea\ud835\uddf5\ud835\ude06 \ud835\ude00\ud835\uddf5\ud835\uddfc\ud835\ude02\ud835\uddf9\ud835\uddf1 \ud835\ude06\ud835\uddfc\ud835\ude02 \ud835\ude02\ud835\ude00\ud835\uddf2 \ud835\uddf6\ud835\ude01? \n\n\ud835\ude0a\ud835\ude30\ud835\ude2f\ud835\ude24\ud835\ude2a\ud835\ude34\ud835\ude26\ud835\ude2f\ud835\ude26\ud835\ude34\ud835\ude34: It reduces the number of lines needed for variable assignment and checking, making code more concise. \n\n\ud835\ude19\ud835\ude26\ud835\ude22\ud835\ude25\ud835\ude22\ud835\ude23\ud835\ude2a\ud835\ude2d\ud835\ude2a\ud835\ude35\ud835\ude3a: It can enhance readability by keeping related logic close, although this depends on the context and the reader's familiarity with exotic Python syntax. \n\n\ud835\ude43\ud835\ude5a\ud835\ude67\ud835\ude5a \ud835\ude56\ud835\ude67\ud835\ude5a \ud835\ude68\ud835\ude64\ud835\ude62\ud835\ude5a \ud835\ude5a\ud835\ude6d\ud835\ude56\ud835\ude62\ud835\ude65\ud835\ude61\ud835\ude5a\ud835\ude68 \n\n\u2193\u2193\u2193 \n\n1. Using the walrus operator, you can directly assign the result of the \ud835\ude2d\ud835\ude26\ud835\ude2f() function inside an if statement. \n\n2. Avoid calling the same function twice in a while loop. The benefit is less code and makes everything more readable. \n\n3. Another use case arises in list comprehensions where a value computed in a filtering condition is also needed in the expression body. Before the \ud835\ude38\ud835\ude22\ud835\ude2d\ud835\ude33\ud835\ude36\ud835\ude34 \ud835\ude30\ud835\ude31\ud835\ude26\ud835\ude33\ud835\ude22\ud835\ude35\ud835\ude30\ud835\ude33, if you had to apply a function to an item from a list and filter it based on some criteria, you had to refactor it to a standard for loop. \n\n. \n\nWhen writing clean code, the detail matters. \n\nThe details make the difference between a codebase that can be read like a book or one with 10 WTFs / seconds. \n\n. \n\nWhat do you think? Does the walrus operator make the Python code more readable and concise? \n\nhashtag\n#machinelearning \nhashtag\n#mlops \nhashtag\n#datascience \n"
    },
    {
        "instruction": "Describe integrating a real-time streaming data source into an ML system on LinkedIn. Detail the infrastructure and processes required, emphasizing the approach's scalability and efficiency in enhancing system capabilities.",
        "post": "\ud835\udde8\ud835\uddfb\ud835\uddf6\ud835\uddf3\ud835\ude06 \ud835\uddef\ud835\uddee\ud835\ude01\ud835\uddf0\ud835\uddf5 \ud835\uddee\ud835\uddfb\ud835\uddf1 \ud835\ude00\ud835\ude01\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddfa\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\udde0\ud835\udddf \ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2\ud835\ude00\nWhat happens if you want to introduce a real-time/streaming data source into your system?\nYou cry. Just kidding. It is a lot easier than it sounds.\nLet's get some context.\nUntil now, you used only a static data source to train your model & compute your features.\nBut you find out that your business wants to use real-time news feeds as features for your model.\n\ud835\uddea\ud835\uddf5\ud835\uddee\ud835\ude01 \ud835\uddf1\ud835\uddfc \ud835\ude06\ud835\uddfc\ud835\ude02 \ud835\uddf1\ud835\uddfc?\nYou have to implement 2 \ud835\ude2e\ud835\ude22\ud835\ude2a\ud835\ude2f \ud835\ude31\ud835\ude2a\ud835\ude31\ud835\ude26\ud835\ude2d\ud835\ude2a\ud835\ude2f\ud835\ude26\ud835\ude34 \ud835\ude27\ud835\ude30\ud835\ude33 \ud835\ude3a\ud835\ude30\ud835\ude36\ud835\ude33 \ud835\ude2f\ud835\ude26\ud835\ude38 \ud835\ude34\ud835\ude35\ud835\ude33\ud835\ude26\ud835\ude22\ud835\ude2e\ud835\ude2a\ud835\ude2f\ud835\ude28 \ud835\ude2a\ud835\ude2f\ud835\ude31\ud835\ude36\ud835\ude35 \ud835\ude34\ud835\ude30\ud835\ude36\ud835\ude33\ud835\ude24\ud835\ude26:\n#\ud835\udfed. One that will quickly transform the raw data into features and make them accessible into the feature store to be used by the production services.\n#\ud835\udfee. One that will store the raw data in the static raw data source (e.g., a warehouse) so it will be used later for experimentation and research.\nBefore ingesting into your system, the real-time data source might need an extra processing step to standardize and adapt the data format to your interface.\nA standard strategy for:\n#\ud835\udfed. Kafka as your streaming platform\n#\ud835\udfee. Flink/Kafka Streams as your streaming processing units\nFor step #2. most of the time, you will have access to out-of-the-box data connectors that quickly load the real-time data into your static data storage (e.g., from Kafka to an S3 bucket or Big Query data warehouse).\nTo conclude...\nTo add a streaming data source to your current infrastructure, you need the following:\n- Kafka\n- Flink/Kafka Streams\n- to move your streaming data source into your static one\n- to quickly compute features and load them into the feature store\nThus, it isn't hard\u2014just a lot of infrastructure to set up.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndata\n-----\n\ud83d\udca1 Follow me if you want to level up in designing ML systems using MLOps good practices."
    },
    {
        "instruction": "Write a tutorialto be posted on Medium on cleaning and understanding computer vision datasets using tools for detecting duplicates, outliers, and visualizing data distributions. Emphasize practical applications, such as preparing datasets for machine learning projects, and include examples with a video dataset.",
        "post": "Master Data Integrity to Clean Your Computer Vision Datasets | by Paul Iusztin | Towards Data ScienceOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyTutorialMaster Data Integrity to Clean Your Computer Vision DatasetsHandle Data Leakage. Reduce Labeling Costs. Decrease Computation Time and Expenses.Paul Iusztin\u00b7FollowPublished inTowards Data Science\u00b710 min read\u00b7Dec 19, 20222381ListenSharePhoto by JESHOOTS.COM on UnsplashData integrity is one of the biggest concerns for companies and engineers in the latest period.The amount of data we have to process and understand only gets more significant, and manually looking at millions of samples is not sustainable. Thus, we need tools that can help us navigate our datasets.This tutorial will present how to clean, visualize and understand Computer Vision datasets, such as videos or images.We will be working on a video of the most precious thing in my house, my cat. Our end goal is to extract essential frames from the video that, lately, can be sent to labeling and to train your model. Extracting crucial information from a video is not straightforward because the video properties change constantly. For example, in the beginning, the video is highly static, and starting from the middle there is a lot of action. Thus we need an intelligent way to understand the properties of the video to eliminate duplicate images, find the outliers, and cluster similar photos.GIF from the video of my sassy cat [GIF by the Author].Leveraging FastDup, a tool for understanding and cleaning CV datasets, we will show you how to solve the above-mentioned problems.The End GoalWe will present a tutorial on decoding a video of my sassy cat and extracting all the frames from it as images. We will use FastDup to visualize different statistics over the pictures dataset.The main goal is to remove similar/duplicate images from the dataset. In the next section, we will detail why cleaning your dataset from duplicates is crucial. Ultimately, we will look at the outliers before and after removing all the similar photos.Why Is It Important to Remove Duplicate Images?When the data gets bigger is extremely difficult to parse and understand all your samples manually\u2026Create an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0in2382381FollowWritten by Paul Iusztin1.5K Followers\u00b7Writer for Towards Data Science\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul Iusztin and Towards Data SciencePaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Sheila TeoinTowards Data ScienceHow I Won Singapore\u2019s GPT-4 Prompt Engineering CompetitionA deep dive into the strategies I learned for harnessing the power of Large Language Models (LLMs)\u00b723 min read\u00b7Dec 29, 202311.2K125Cristian LeoinTowards Data ScienceThe Math behind Adam OptimizerWhy is Adam the most popular optimizer in Deep Learning? Let\u2019s understand it by diving into its math, and recreating the algorithm.16 min read\u00b7Jan 301.3K9Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111See all from Paul IusztinSee all from Towards Data ScienceRecommended from MediumPaul IusztininDecoding MLThe ultimate guide on installing PyTorch with CUDA support in all possible ways\u2192 Using Pip, Conda, Poetry, Docker, or directly on the system\u00b710 min read\u00b7Jan 42231Redmen IshabComputer Vision AI With Yolo-v8Object detection using Yolo v85 min read\u00b7Jan 2420ListsTech & Tools16 stories\u00b7143 savesStories to Help You Grow as a Software Developer19 stories\u00b7803 savesGeneral Coding Knowledge20 stories\u00b7905 savesIcon Design36 stories\u00b7222 savesMoez AliEasily deploy machine learning models from the comfort of your NotebookStreamline your Machine Learning model deployment with Modelbit.com. Seamlessly move from Jupyter notebooks to production-ready REST APIs\u20269 min read\u00b7Sep 8, 20232621Youssef BoutalebDeveloping a Custom YOLOv8 Model for Number Detection on Meters Using FastAPI and Gradio\ud83d\udc49 Check out my Huggingface app to test the model online.11 min read\u00b7Dec 11, 2023105Takoua SaadaniinUBIAI NLPExploring the Significance of YOLO in 2024: A Revolution in Object DetectionIn the rapidly evolving field of object detection, the acronym \u201cYOLO,\u201d which stands for \u201cYou Only Look Once,\u201d has become synonymous with\u20266 min read\u00b7Jan 24132Henry NavarroYOLOv8.1 is Here! Featuring Oriented Bounding Boxes Applications\ud83d\ude80 \ud83d\udce6\ud83d\udd0dExploring the New Frontier in Object Detection with YOLOv8\u2019s Latest Update\u00b76 min read\u00b7Jan 241812See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Promote the adoption of Terraform for infrastructure management in ML projects on LinkedIn. Share personal savings and efficiencies gained from using Terraform, positioning it as a crucial tool for cost-effective and scalable ML deployments.",
        "post": "I was uselessly spending 1000$ dollars every month on cloud machines until I started using this tool \ud83d\udc47\nTerraform!\n.\n\ud835\udc05\ud835\udc22\ud835\udc2b\ud835\udc2c\ud835\udc2d, \ud835\udc25\ud835\udc1e\ud835\udc2d'\ud835\udc2c \ud835\udc2e\ud835\udc27\ud835\udc1d\ud835\udc1e\ud835\udc2b\ud835\udc2c\ud835\udc2d\ud835\udc1a\ud835\udc27\ud835\udc1d \ud835\udc30\ud835\udc21\ud835\udc32 \ud835\udc30\ud835\udc1e \ud835\udc27\ud835\udc1e\ud835\udc1e\ud835\udc1d \ud835\udc13\ud835\udc1e\ud835\udc2b\ud835\udc2b\ud835\udc1a\ud835\udc1f\ud835\udc28\ud835\udc2b\ud835\udc26.\nWhen you want to deploy a software application, there are two main steps:\n1. Provisioning infrastructure\n2. Deploying applications\nA regular workflow would be that before deploying your applications or building your CI/CD pipelines, you manually go and spin up your, let's say, AWS machines.\nInitially, this workflow should be just fine, but there are two scenarios when it could get problematic.\n#1. Your infrastructure gets too big and complicated. Thus, it is cumbersome and might yield bugs in manually replicating it.\n#2. In the world of AI, there are many cases when you want to spin up a GPU machine to train your models, and afterward, you don't need it anymore. Thus, if you forget to close it, you will end up uselessly paying a lot of $$$.\nWith Terraform, you can solve both of these issues.\n.\nSo...\n\ud835\udc16\ud835\udc21\ud835\udc1a\ud835\udc2d \ud835\udc22\ud835\udc2c \ud835\udc13\ud835\udc1e\ud835\udc2b\ud835\udc2b\ud835\udc1a\ud835\udc1f\ud835\udc28\ud835\udc2b\ud835\udc26?\nIt sits on the provisioning infrastructure layer as a: \"infrastructure as code\" tool that:\n- is declarative (you focus on the WHAT, not on the HOW)\n- automates and manages your infrastructure\n- is open source\nYeah... yeah... that sounds fancy. But \ud835\udc30\ud835\udc21\ud835\udc1a\ud835\udc2d \ud835\udc1c\ud835\udc1a\ud835\udc27 \ud835\udc08 \ud835\udc1d\ud835\udc28 \ud835\udc30\ud835\udc22\ud835\udc2d\ud835\udc21 \ud835\udc22\ud835\udc2d?\nLet's take AWS as an example, where you have to:\n- create a VPC\n- create AWS users and permissions\n- spin up EC2 machines\n- install programs (e.g., Docker)\n- create a K8s cluster\nUsing Terraform...\nYou can do all that just by providing a configuration file that reflects the state of your infrastructure.\nBasically, it helps you create all the infrastructure you need programmatically. Isn't that awesome?\n.\nIf you want to quickly understand Terraform enough to start using it in your own projects,\n\u21b3 check out my 7-minute read article: \ud835\ude1a\ud835\ude35\ud835\ude30\ud835\ude31 \ud835\ude14\ud835\ude22\ud835\ude2f\ud835\ude36\ud835\ude22\ud835\ude2d\ud835\ude2d\ud835\ude3a \ud835\ude0a\ud835\ude33\ud835\ude26\ud835\ude22\ud835\ude35\ud835\ude2a\ud835\ude2f\ud835\ude28 \ud835\ude20\ud835\ude30\ud835\ude36\ud835\ude33 \ud835\ude08\ud835\ude1e\ud835\ude1a \ud835\ude10\ud835\ude2f\ud835\ude27\ud835\ude33\ud835\ude22\ud835\ude34\ud835\ude35\ud835\ude33\ud835\ude36\ud835\ude24\ud835\ude35\ud835\ude36\ud835\ude33\ud835\ude26. \ud835\ude1c\ud835\ude34\ud835\ude26 \ud835\ude1b\ud835\ude26\ud835\ude33\ud835\ude33\ud835\ude22\ud835\ude27\ud835\ude30\ud835\ude33\ud835\ude2e!\nLink from the comments \u2193\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\nsoftwareengineering\n-----\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps."
    },
    {
        "instruction": "Announce a new course on building hands-on LLM systems with a focus on good LLMOps principles, highlighting the content and inviting the audience to enroll for free.",
        "post": "If you want to \ud835\udc25\ud835\udc1e\ud835\udc1a\ud835\udc2b\ud835\udc27 for FREE to \ud835\udc1b\ud835\udc2e\ud835\udc22\ud835\udc25\ud835\udc1d \ud835\udc21\ud835\udc1a\ud835\udc27\ud835\udc1d\ud835\udc2c-\ud835\udc28\ud835\udc27 \ud835\udc0b\ud835\udc0b\ud835\udc0c \ud835\udc2c\ud835\udc32\ud835\udc2c\ud835\udc2d\ud835\udc1e\ud835\udc26\ud835\udc2c using good LLMOps principles, we want to announce that we just \ud835\udc1f\ud835\udc22\ud835\udc27\ud835\udc22\ud835\udc2c\ud835\udc21\ud835\udc1e\ud835\udc1d the code & video lessons for the \"\ud835\udc07\ud835\udc1a\ud835\udc27\ud835\udc1d\ud835\udc2c-\ud835\udc28\ud835\udc27 \ud835\udc0b\ud835\udc0b\ud835\udc0c\ud835\udc2c\" \ud835\udc1c\ud835\udc28\ud835\udc2e\ud835\udc2b\ud835\udc2c\ud835\udc1e.\nBy finishing the Hands-On LLMs free course, you will learn how to use the 3-pipeline architecture & LLMOps good practices to design, build, and deploy a real-time financial advisor powered by LLMs & vector DBs.\nWe will primarily focus on the engineering & MLOps aspects.\nThus, by the end of this series, you will know how to build & deploy a real ML system, not some isolated code in Notebooks.\n\ud835\udc0c\ud835\udc28\ud835\udc2b\ud835\udc1e \ud835\udc29\ud835\udc2b\ud835\udc1e\ud835\udc1c\ud835\udc22\ud835\udc2c\ud835\udc1e\ud835\udc25\ud835\udc32, \ud835\udc2d\ud835\udc21\ud835\udc1e\ud835\udc2c\ud835\udc1e \ud835\udc1a\ud835\udc2b\ud835\udc1e \ud835\udc2d\ud835\udc21\ud835\udc1e 3 \ud835\udc1c\ud835\udc28\ud835\udc26\ud835\udc29\ud835\udc28\ud835\udc27\ud835\udc1e\ud835\udc27\ud835\udc2d\ud835\udc2c \ud835\udc32\ud835\udc28\ud835\udc2e \ud835\udc30\ud835\udc22\ud835\udc25\ud835\udc25 \ud835\udc25\ud835\udc1e\ud835\udc1a\ud835\udc2b\ud835\udc27 \ud835\udc2d\ud835\udc28 \ud835\udc1b\ud835\udc2e\ud835\udc22\ud835\udc25\ud835\udc1d:\n1.  a \ud835\udc2b\ud835\udc1e\ud835\udc1a\ud835\udc25-\ud835\udc2d\ud835\udc22\ud835\udc26\ud835\udc1e \ud835\udc2c\ud835\udc2d\ud835\udc2b\ud835\udc1e\ud835\udc1a\ud835\udc26\ud835\udc22\ud835\udc27\ud835\udc20 \ud835\udc29\ud835\udc22\ud835\udc29\ud835\udc1e\ud835\udc25\ud835\udc22\ud835\udc27\ud835\udc1e (deployed on AWS) that listens to financial news, cleans & embeds the documents, and loads them to a vector DB\n2.  a \ud835\udc1f\ud835\udc22\ud835\udc27\ud835\udc1e-\ud835\udc2d\ud835\udc2e\ud835\udc27\ud835\udc22\ud835\udc27\ud835\udc20 \ud835\udc29\ud835\udc22\ud835\udc29\ud835\udc1e\ud835\udc25\ud835\udc22\ud835\udc27\ud835\udc1e (deployed as a serverless continuous training) that fine-tunes an LLM on financial data using QLoRA, monitors the experiments using an experiment tracker and saves the best model to a model registry\n3.  an \ud835\udc22\ud835\udc27\ud835\udc1f\ud835\udc1e\ud835\udc2b\ud835\udc1e\ud835\udc27\ud835\udc1c\ud835\udc1e \ud835\udc29\ud835\udc22\ud835\udc29\ud835\udc1e\ud835\udc25\ud835\udc22\ud835\udc27\ud835\udc1e built in LangChain (deployed as a serverless RESTful API) that loads the fine-tuned LLM from the model registry and answers financial questions using RAG (leveraging the vector DB populated with financial news in real-time)\nWe will also show you how to integrate various serverless tools, such as:\n\u2022 Comet ML as your ML Platform;\n\u2022 Qdrant as your vector DB;\n\u2022 Beam as your infrastructure.\n\ud835\udc16\ud835\udc21\ud835\udc28 \ud835\udc22\ud835\udc2c \ud835\udc2d\ud835\udc21\ud835\udc22\ud835\udc2c \ud835\udc1f\ud835\udc28\ud835\udc2b?\nThe series targets MLE, DE, DS, or SWE who want to learn to engineer LLM systems using LLMOps good principles.\n\ud835\udc07\ud835\udc28\ud835\udc30 \ud835\udc30\ud835\udc22\ud835\udc25\ud835\udc25 \ud835\udc32\ud835\udc28\ud835\udc2e \ud835\udc25\ud835\udc1e\ud835\udc1a\ud835\udc2b\ud835\udc27?\nThe series contains 4 hands-on video lessons and the open-source code you can access on GitHub.\n\ud835\udc02\ud835\udc2e\ud835\udc2b\ud835\udc22\ud835\udc28\ud835\udc2e\ud835\udc2c?\n\u21b3 Check it out and support us with a \u2b50:  \ud83d\udd17\nhttps://lnkd.in/dZgqtf8f\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps."
    },
    {
        "instruction": "Describe the process of designing and deploying a Mistral LLM to AWS SageMaker.",
        "post": "This is \ud835\uddf5\ud835\uddfc\ud835\ude04 you can \ud835\uddf1\ud835\uddf2\ud835\ude00\ud835\uddf6\ud835\uddf4\ud835\uddfb and \ud835\uddf1\ud835\uddf2\ud835\uddfd\ud835\uddf9\ud835\uddfc\ud835\ude06 a \ud835\udde0\ud835\uddf6\ud835\ude00\ud835\ude01\ud835\uddff\ud835\uddee\ud835\uddf9 \ud835\udddf\ud835\udddf\ud835\udde0 to \ud835\uddd4\ud835\uddea\ud835\udde6 \ud835\udde6\ud835\uddee\ud835\uddf4\ud835\uddf2\ud835\uddfa\ud835\uddee\ud835\uddf8\ud835\uddf2\ud835\uddff \u2193\n\n\ud835\uddd4\ud835\uddf9\ud835\ude04\ud835\uddee\ud835\ude06\ud835\ude00, the \ud835\uddf3\ud835\uddf6\ud835\uddff\ud835\ude00\ud835\ude01 \ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfd is to \ud835\ude02\ud835\uddfb\ud835\uddf1\ud835\uddf2\ud835\uddff\ud835\ude00\ud835\ude01\ud835\uddee\ud835\uddfb\ud835\uddf1 the \ud835\uddff\ud835\uddf2\ud835\uddfe\ud835\ude02\ud835\uddf6\ud835\uddff\ud835\uddf2\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01\ud835\ude00 of your \ud835\ude00\ud835\ude06\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfa. \n\n\ud83e\udd14 Ask yourself:\n\n- What are my \ud835\ude25\ud835\ude22\ud835\ude35\ud835\ude22 \ud835\ude31\ud835\ude33\ud835\ude30\ud835\ude24\ud835\ude26\ud835\ude34\ud835\ude34\ud835\ude2a\ud835\ude2f\ud835\ude28 \ud835\ude2f\ud835\ude26\ud835\ude26\ud835\ude25\ud835\ude34? Consider the diversity of the data formats and complexity.\n\n- How does the \ud835\ude38\ud835\ude30\ud835\ude33\ud835\ude2c\ud835\ude2d\ud835\ude30\ud835\ude22\ud835\ude25 look? Is it real-time or batch processing? What's the concurrency level?\n\n- What about \ud835\ude31\ud835\ude33\ud835\ude30\ud835\ude24\ud835\ude26\ud835\ude34\ud835\ude34\ud835\ude2a\ud835\ude2f\ud835\ude28 \ud835\ude24\ud835\ude30\ud835\ude2f\ud835\ude34\ud835\ude35\ud835\ude33\ud835\ude22\ud835\ude2a\ud835\ude2f\ud835\ude35\ud835\ude34? Latency requirements, accuracy versus speed, and, importantly, cost considerations.\n\n- Can it \ud835\ude34\ud835\ude24\ud835\ude22\ud835\ude2d\ud835\ude26 and \ud835\ude22\ud835\ude25\ud835\ude22\ud835\ude31\ud835\ude35? Your strategy must include scalability and flexibility for future expansion.\n\n.\n\nAlex Vesa wrote an article in Decoding ML where he:\n- made an overview of AWS Sagemaker\n- showed how to design the system requirements and deploy a Mistral LLM using AWS Sagemaker\n\n\ud835\ude0f\ud835\ude26\ud835\ude33\ud835\ude26 \ud835\ude2a\ud835\ude34 \ud835\ude38\ud835\ude29\ud835\ude22\ud835\ude35 \ud835\ude3a\ud835\ude30\ud835\ude36 \ud835\ude38\ud835\ude2a\ud835\ude2d\ud835\ude2d \ud835\ude2d\ud835\ude26\ud835\ude22\ud835\ude33\ud835\ude2f \u2193\n\n\ud835\udde6\ud835\uddee\ud835\uddf4\ud835\uddf2\ud835\uddfa\ud835\uddee\ud835\uddf8\ud835\uddf2\ud835\uddff \ud835\udddc\ud835\uddfb\ud835\uddf3\ud835\uddf2\ud835\uddff\ud835\uddf2\ud835\uddfb\ud835\uddf0\ud835\uddf2 \ud835\uddd8\ud835\uddfb\ud835\uddf1\ud835\uddfd\ud835\uddfc\ud835\uddf6\ud835\uddfb\ud835\ude01 \ud835\uddd6\ud835\uddfc\ud835\uddfa\ud835\uddfd\ud835\uddfc\ud835\uddfb\ud835\uddf2\ud835\uddfb\ud835\ude01\ud835\ude00\n\n\ud835\udde6\ud835\uddee\ud835\uddf4\ud835\uddf2\ud835\uddfa\ud835\uddee\ud835\uddf8\ud835\uddf2\ud835\uddff \ud835\uddd8\ud835\uddfb\ud835\uddf1\ud835\uddfd\ud835\uddfc\ud835\uddf6\ud835\uddfb\ud835\ude01: The scalable and secure API for real-time predictions.\n\n\ud835\udde6\ud835\uddee\ud835\uddf4\ud835\uddf2\ud835\uddfa\ud835\uddee\ud835\uddf8\ud835\uddf2\ud835\uddff \ud835\udde0\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddf9: The artifact from training, ready for predictions.\n\n\ud835\udde6\ud835\uddee\ud835\uddf4\ud835\uddf2\ud835\uddfa\ud835\uddee\ud835\uddf8\ud835\uddf2\ud835\uddff \ud835\uddd6\ud835\uddfc\ud835\uddfb\ud835\uddf3\ud835\uddf6\ud835\uddf4\ud835\ude02\ud835\uddff\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb: Specifies the hardware and software setup.\n\n\ud835\udde6\ud835\uddee\ud835\uddf4\ud835\uddf2\ud835\uddfa\ud835\uddee\ud835\uddf8\ud835\uddf2\ud835\uddff \ud835\udddc\ud835\uddfb\ud835\uddf3\ud835\uddf2\ud835\uddff\ud835\uddf2\ud835\uddfb\ud835\uddf0\ud835\uddf2 \ud835\uddd6\ud835\uddfc\ud835\uddfa\ud835\uddfd\ud835\uddfc\ud835\uddfb\ud835\uddf2\ud835\uddfb\ud835\ude01: Manages model deployment to endpoints efficiently.\n\n\u2193\u2193\u2193\n\nSageMaker's multi-model endpoints offer a dynamic and efficient way to manage and serve multiple models:\n\n- dynamically manages model lifecycles on multi-model endpoints, loading models on demand\n\n- routes additional requests to other instances if one is overloaded\n\n- manages memory by unloading unused models to free up space for new ones\n\n- models can be added or removed by managing them in the S3 bucket without updating the endpoint\n\nDeploying LLMs via AWS SageMaker is a complex task that demands a deep understanding of system requirements and resource management.\n\n.\n\nIn the \ud835\uddee\ud835\uddff\ud835\ude01\ud835\uddf6\ud835\uddf0\ud835\uddf9\ud835\uddf2, you will \ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddfb how to \ud835\uddf1\ud835\uddf2\ud835\uddfd\ud835\uddf9\ud835\uddfc\ud835\ude06 a \ud835\udde0\ud835\uddf6\ud835\ude00\ud835\ude01\ud835\uddff\ud835\uddee\ud835\uddf9 \ud835\udff3\ud835\uddd5 on \ud835\uddd4\ud835\uddea\ud835\udde6 \ud835\udde6\ud835\uddee\ud835\uddf4\ud835\uddf2\ud835\uddfa\ud835\uddee\ud835\uddf8\ud835\uddf2\ud835\uddff using a g5.12xlarge instance.\n\nTo do so, you will \ud835\uddf4\ud835\uddfc \ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfd-\ud835\uddef\ud835\ude06-\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfd \ud835\ude01\ud835\uddf5\ud835\uddff\ud835\uddfc\ud835\ude02\ud835\uddf4\ud835\uddf5 the \ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2 to \ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddfb \ud835\uddf5\ud835\uddfc\ud835\ude04 to:\n\n1. Configure Hardware requirements per replica\n2. Prepare a Deployment Service Class\n3. How to deploy the LLM\n4. Cleaning Sagemaker resources\n\n\ud835\uddd6\ud835\uddf5\ud835\uddf2\ud835\uddf0\ud835\uddf8 \ud835\uddf6\ud835\ude01 \ud835\uddfc\ud835\ude02\ud835\ude01 \ud835\uddfc\ud835\uddfb \ud835\uddd7\ud835\uddf2\ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\udde0\ud835\udddf\n\n\u2193\u2193\u2193\n\n\ud83d\udd17 \ud835\ude20\ud835\ude30\ud835\ude36'\ud835\ude33\ud835\ude26 \ud835\ude2f\ud835\ude30\ud835\ude35 \ud835\ude25\ud835\ude2a\ud835\ude28\ud835\ude28\ud835\ude2a\ud835\ude2f\ud835\ude28 \ud835\ude25\ud835\ude26\ud835\ude26\ud835\ude31\ud835\ude26\ud835\ude33 \ud835\ude2a\ud835\ude2f\ud835\ude35\ud835\ude30 \ud835\ude24\ud835\ude30\ud835\ude2f\ud835\ude24\ud835\ude26\ud835\ude31\ud835\ude35\ud835\ude34? \ud835\ude0f\ud835\ude26\ud835\ude33\ud835\ude26'\ud835\ude34 \ud835\ude38\ud835\ude29\ud835\ude22\ud835\ude35 \ud835\ude29\ud835\ude22\ud835\ude31\ud835\ude31\ud835\ude26\ud835\ude2f\ud835\ude34... : https://lnkd.in/ee8FeMHa\n\nhashtag\n#machinelearning \nhashtag\n#mlops \nhashtag\n#datascience\n"
    },
    {
        "instruction": "Discuss the importance of a model registry in decoupling experimental and production pipelines of ML systems on LinkedIn. Explain how it facilitates smooth transitions from development to deployment, enhancing system reliability.",
        "post": "\ud835\udde6\ud835\ude02\ud835\uddfd\ud835\uddf2\ud835\uddff\ud835\uddf0\ud835\uddf5\ud835\uddee\ud835\uddff\ud835\uddf4\ud835\uddf2 \ud835\ude06\ud835\uddfc\ud835\ude02\ud835\uddff \ud835\udde0\ud835\udddf \ud835\ude00\ud835\ude06\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfa: \ud835\ude02\ud835\ude00\ud835\uddf2 \ud835\uddee \ud835\uddfa\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddf9 \ud835\uddff\ud835\uddf2\ud835\uddf4\ud835\uddf6\ud835\ude00\ud835\ude01\ud835\uddff\ud835\ude06\nA model registry is the holy grail of any production-ready ML system.\nThe model registry is the critical component that decouples your offline pipeline (experimental/research phase) from your production pipeline.\n\ud835\uddd6\ud835\uddfc\ud835\uddfa\ud835\uddfd\ud835\ude02\ud835\ude01\ud835\uddf2 \ud835\udde2\ud835\uddf3\ud835\uddf3\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2 \ud835\uddd9\ud835\uddf2\ud835\uddee\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2\ud835\ude00\nUsually, when training your model, you use a static data source.\nUsing a feature engineering pipeline, you compute the necessary features used to train the model.\nThese features will be stored inside a features store.\nAfter processing your data, your training pipeline creates the training & testing splits and starts training the model.\nThe output of your training pipeline is the trained weights, also known as the model artifact.\n\ud835\udddb\ud835\uddf2\ud835\uddff\ud835\uddf2 \ud835\uddf6\ud835\ude00 \ud835\ude04\ud835\uddf5\ud835\uddf2\ud835\uddff\ud835\uddf2 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddfa\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddf9 \ud835\uddff\ud835\uddf2\ud835\uddf4\ud835\uddf6\ud835\ude00\ud835\ude01\ud835\uddff\ud835\ude06 \ud835\uddf8\ud835\uddf6\ud835\uddf0\ud835\uddf8\ud835\ude00 \ud835\uddf6\ud835\uddfb \ud83d\udc47\nThis artifact will be pushed into the model registry under a new version that can easily be tracked.\nSince this point, the new model artifact version can be pulled by any serving strategy:\n#1. batch\n#2. request-response\n#3. streaming\nYour inference pipeline doesn\u2019t care how the model artifact was generated. It just has to know what model to use and how to transform the data into features.\nNote that this strategy is independent of the type of model & hardware you use:\n- classic model (Sklearn, XGboost),\n- distributed system (Spark),\n- deep learning model (PyTorch)\nTo summarize...\nUsing a model registry is a simple and effective method to:\n-> detach your experimentation from your production environment,\nregardless of what framework or hardware you use.\nTo learn more, check out my practical & detailed example of how to use a model registry in my article: \ud835\ude08 \ud835\ude0e\ud835\ude36\ud835\ude2a\ud835\ude25\ud835\ude26 \ud835\ude35\ud835\ude30 \ud835\ude09\ud835\ude36\ud835\ude2a\ud835\ude2d\ud835\ude25\ud835\ude2a\ud835\ude2f\ud835\ude28 \ud835\ude0c\ud835\ude27\ud835\ude27\ud835\ude26\ud835\ude24\ud835\ude35\ud835\ude2a\ud835\ude37\ud835\ude26 \ud835\ude1b\ud835\ude33\ud835\ude22\ud835\ude2a\ud835\ude2f\ud835\ude2a\ud835\ude2f\ud835\ude28 \ud835\ude17\ud835\ude2a\ud835\ude31\ud835\ude26\ud835\ude2d\ud835\ude2a\ud835\ude2f\ud835\ude26\ud835\ude34 \ud835\ude27\ud835\ude30\ud835\ude33 \ud835\ude14\ud835\ude22\ud835\ude39\ud835\ude2a\ud835\ude2e\ud835\ude36\ud835\ude2e \ud835\ude19\ud835\ude26\ud835\ude34\ud835\ude36\ud835\ude2d\ud835\ude35\ud835\ude34\nLink in the comments \ud83d\udc47\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndata\n-----\n\ud83d\udca1 Follow me if you want to level up in designing ML systems using MLOps good practices."
    },
    {
        "instruction": "Reflect on participating in the Let's Talk AI podcast in a LinkedIn post. Share insights from the discussion on ML engineering and MLOps, highlighting topics like AI system design, job roles, and the use of Airflow in automation.",
        "post": "Are you into MLOps and ML Engineering?\nI was honored to pour my MLE & MLOps wisdom into the podcast \ud835\ude13\ud835\ude26\ud835\ude35'\ud835\ude34 \ud835\ude1b\ud835\ude22\ud835\ude2d\ud835\ude2c \ud835\ude08\ud835\ude10 hosted.\nI had a great time talking with\nThomas Bustos\n, where he had some fantastic questions about:\n- building and engineering AI systems\n- finding your niche in AI\n- different ML job positions\n- Airflow for automating ML\n- deploying multiple versions and communicating effectively\n- explaining technical complexity to customers\n... and more\nIf this sounds like something you are interested in, check it out \ud83d\udc47\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndata\n-----\n\ud83d\udca1 Follow me if you want to level up in designing ML systems using MLOps good practices."
    },
    {
        "instruction": "Create a promotional post to introduce your network to a new product or service. Describe the key features, benefits, and how it can address a specific need or problem your audience may have. Include a call-to-action encouraging engagement or further exploration.",
        "post": "Come to see how you can be a part of a place of innovation, with projects like: \u27a1\ufe0f lek3.co, charging electric cars anywhere. Quick and easy. \u27a1\ufe0f 2park.io, smart parking at your fingertips and \u27a1\ufe0f SafeFleet, innovating the telematics industry. We're waiting for you! :) \u27a1\ufe0f \u27a1\ufe0f \u27a1\ufe0f https://lnkd.in/g-sEMiW #iot #design #future #hr #team #smartcities #projects #electriccars #telematics #parks #futurism #charging #software #projecting #safefleet #recruiting #lek3 #lek3co"
    },
    {
        "instruction": "Write me a LinkedIn Post of about 250 - 300 words about  learning MLE and mMLOps. In here you are trying to promovate your MLOps course showcasing its pros.",
        "post": "Want to learn \ud835\udde0\ud835\udddf\ud835\uddd8 & \ud835\udde0\ud835\udddf\ud835\udde2\ud835\uddfd\ud835\ude00 in a \ud835\ude00\ud835\ude01\ud835\uddff\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2\ud835\uddf1 \ud835\ude04\ud835\uddee\ud835\ude06, for \ud835\uddf3\ud835\uddff\ud835\uddf2\ud835\uddf2, and with \ud835\uddf5\ud835\uddee\ud835\uddfb\ud835\uddf1\ud835\ude00-\ud835\uddfc\ud835\uddfb \ud835\uddf2\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00? \n \nThen you should check out my \ud835\udde7\ud835\uddf5\ud835\uddf2 \ud835\uddd9\ud835\ude02\ud835\uddf9\ud835\uddf9 \ud835\udde6\ud835\ude01\ud835\uddee\ud835\uddf0\ud835\uddf8 \ud835\udff3-\ud835\udde6\ud835\ude01\ud835\uddf2\ud835\uddfd\ud835\ude00 \ud835\udde0\ud835\udddf\ud835\udde2\ud835\uddfd\ud835\ude00 \ud835\uddd9\ud835\uddff\ud835\uddee\ud835\uddfa\ud835\uddf2\ud835\ude04\ud835\uddfc\ud835\uddff\ud835\uddf8 FREE course. \n \n. \n \nIn \ud835\udfee.\ud835\udff1 \ud835\uddf5\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\ude00 \ud835\uddfc\ud835\uddf3 \ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4 & \ud835\ude03\ud835\uddf6\ud835\uddf1\ud835\uddf2\ud835\uddfc \ud835\uddfa\ud835\uddee\ud835\ude01\ud835\uddf2\ud835\uddff\ud835\uddf6\ud835\uddee\ud835\uddf9\ud835\ude00, you will \ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddfb \ud835\uddf5\ud835\uddfc\ud835\ude04 \ud835\ude01\ud835\uddfc: \n \n- design a batch-serving architecture \n- use Hopsworks as a feature store \n- design a feature engineering pipeline that reads data from an API \n- build a training pipeline with hyper-parameter tunning \n- use W&B as an ML Platform to track your experiments, models, and metadata \n- implement a batch prediction pipeline \n- use Poetry to build your own Python packages \n- deploy your own private PyPi server \n- orchestrate everything with Airflow \n- use the predictions to code a web app using FastAPI and Streamlit \n- use Docker to containerize your code \n- use Great Expectations to ensure data validation and integrity \n- monitor the performance of the predictions over time \n- deploy everything to GCP \n- build a CI/CD pipeline using GitHub Actions \n- trade-offs & future improvements discussion \n \n...where all the pieces are integrated into a single end-to-end ML system that forecasts hourly energy levels across Denmark. \n \n\ud835\uddec\ud835\uddfc\ud835\ude02 \ud835\uddf0\ud835\uddee\ud835\uddfb \ud835\uddee\ud835\uddf0\ud835\uddf0\ud835\uddf2\ud835\ude00\ud835\ude00 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddf0\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\ude00\ud835\uddf2 \ud835\uddfc\ud835\uddfb: \n\u279d \ud835\ude14\ud835\ude26\ud835\ude25\ud835\ude2a\ud835\ude36\ud835\ude2e'\ud835\ude34 \ud835\ude1b\ud835\ude0b\ud835\ude1a \ud835\ude31\ud835\ude36\ud835\ude23\ud835\ude2d\ud835\ude2a\ud835\ude24\ud835\ude22\ud835\ude35\ud835\ude2a\ud835\ude30\ud835\ude2f: text tutorials + videos \n\u279d \ud835\ude0e\ud835\ude2a\ud835\ude35\ud835\ude0f\ud835\ude36\ud835\ude23: open-source code + docs \n \nThe course is on Medium's TDS publication to make it easily accessible to people worldwide. Thus \u2193 \n \n... anyone can learn the fundamentals of MLE & MLOps. \n \nSo, no more excuses. Just go and build your own project \ud83d\udd25 \n \nCheck it out \u2193 \n \n\ud83d\udd17 \ud835\ude1b\ud835\ude29\ud835\ude26 \ud835\ude0d\ud835\ude36\ud835\ude2d\ud835\ude2d \ud835\ude1a\ud835\ude35\ud835\ude22\ud835\ude24\ud835\ude2c 7-\ud835\ude1a\ud835\ude35\ud835\ude26\ud835\ude31\ud835\ude34 \ud835\ude14\ud835\ude13\ud835\ude16\ud835\ude31\ud835\ude34 \ud835\ude0d\ud835\ude33\ud835\ude22\ud835\ude2e\ud835\ude26\ud835\ude38\ud835\ude30\ud835\ude33\ud835\ude2c: \nhttps://lnkd.in/d_GVpZ9X \n \nhashtag\n#machinelearning \nhashtag\n#mlops \nhashtag\n#datascience "
    },
    {
        "instruction": "Promote the Hands-on LLMs course, emphasizing its comprehensive approach to building production-ready applications with LLMs and detailing the key components students will learn to develop and deploy.",
        "post": "Want to \ud835\ude00\ud835\ude01\ud835\uddee\ud835\uddff\ud835\ude01 \ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4 to \ud835\uddef\ud835\ude02\ud835\uddf6\ud835\uddf9\ud835\uddf1 \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddf1\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb-\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddf1\ud835\ude06 applications using \ud835\udddf\ud835\udddf\ud835\udde0\ud835\ude00? Then, I want to let you know that Pau and I \ud835\ude00\ud835\ude01\ud835\uddee\ud835\uddff\ud835\ude01\ud835\uddf2\ud835\uddf1 \ud835\uddff\ud835\uddf2\ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\ude00\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\ude03\ud835\uddf6\ud835\uddf1\ud835\uddf2\ud835\uddfc \ud835\uddf9\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2\ud835\ude00 for the FREE \ud835\udddb\ud835\uddee\ud835\uddfb\ud835\uddf1\ud835\ude00-\ud835\udde2\ud835\uddfb \ud835\udddf\ud835\udddf\ud835\udde0\ud835\ude00 \ud835\uddf0\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\ude00\ud835\uddf2.\nThe Hands-On LLMs course is not just another demo of how to make a few predictions in a notebook.\nYou'll walk away with a \ud835\uddf3\ud835\ude02\ud835\uddf9\ud835\uddf9\ud835\ude06 \ud835\uddfc\ud835\uddfd\ud835\uddf2\ud835\uddff\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb\ud835\uddee\ud835\uddf9 \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddf1\ud835\ude02\ud835\uddf0\ud835\ude01, leveraging Large Language Models (LLMs) to build a chatbot for financial investment advice.\n=== \ud835\uddea\ud835\uddf5\ud835\uddee\ud835\ude01 \ud835\uddec\ud835\uddfc\ud835\ude02'\ud835\uddf9\ud835\uddf9 \ud835\uddd5\ud835\ude02\ud835\uddf6\ud835\uddf9\ud835\uddf1 ===\nWithin the course, you will leverage the \ud835\udfef-\ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2 \ud835\uddee\ud835\uddff\ud835\uddf0\ud835\uddf5\ud835\uddf6\ud835\ude01\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2, as follows:\n\ud835\udfed. \ud835\uddd9\ud835\uddf2\ud835\uddee\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2 \ud835\udde3\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2: You'll create a system to ingest real-time financial news\u2014crucial for up-to-date advice.\n\ud835\udfee. \ud835\udde7\ud835\uddff\ud835\uddee\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\udde3\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2: You'll fine-tune an LLM to specialize the model in making financial decisions.\n\ud835\udfef. \ud835\udddc\ud835\uddfb\ud835\uddf3\ud835\uddf2\ud835\uddff\ud835\uddf2\ud835\uddfb\ud835\uddf0\ud835\uddf2 \ud835\udde3\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2: You'll combine all the components and deploy the model as a RESTful API, making your application accessible worldwide.\nThese pipelines will be independently developed, deployed, and scaled, ensuring modular and clean code.\nCheck it out \u2193\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps."
    },
    {
        "instruction": "Write a LinkedIn post presenting a list of essential ML monitoring metrics, aiming to educate followers on effective monitoring practices.",
        "post": "I compiled a list of the top 10 metrics your ML monitoring system should track.\n.\n#\ud835\udfcf \ud835\udc0e\ud835\udc29\ud835\udc1e\ud835\udc2b\ud835\udc1a\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27\ud835\udc1a\ud835\udc25 \ud835\udc26\ud835\udc1e\ud835\udc2d\ud835\udc2b\ud835\udc22\ud835\udc1c\ud835\udc2c\nUsed to monitor the health of your overall software system.\n\u279d latency\n\u279d throughput\n\u279d CPU/GPU utilization\n\u279d memory utilization\n\u279d the number of requests your model receives in the last X minutes/hours/days\n\u279d the number of successful requests\n#\ud835\udfd0 \ud835\udc0c\ud835\udc0b-\ud835\udc2c\ud835\udc29\ud835\udc1e\ud835\udc1c\ud835\udc22\ud835\udc1f\ud835\udc22\ud835\udc1c \ud835\udc26\ud835\udc1e\ud835\udc2d\ud835\udc2b\ud835\udc22\ud835\udc1c\ud835\udc2c\nUsed to monitor the performance of your ML system.\n\u279d accuracy (extremely useful when you have feedback or natural labels from the user: click rate, upvote, downvote, purchases, bookmarks, views, etc.)\n\u279d predictions (as predictions are low dimensional various statistics are easy to compute + the distribution of the prediction represents a proxy for the input distribution)\n\u279d features (feature validation + two-sample tests for drift detection)\n\u279d raw inputs (these are harder to monitor due to their scattered nature within the infrastructure)\n-----\n\ud83d\udca1 My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n."
    },
    {
        "instruction": "Create a LinkedIn post celebrating the anniversary of a book on deep learning, offering copies to the audience.",
        "post": "We'll send five of these to lucky people anywhere in the world \ud83c\udf0e\nWe want to celebrate with you - our book just turned 1 - they grow up so fast.\n\ud83d\udc49 Simply comment below by adding (@) friends who should get the book. We randomly select your friends and contact them for their address to ship the books.\n\ud83d\udcd6 About the book:\n\"Why does my neural network not learn?\" by @Frank Hafner and I, will kickstart your friends' deep learning journey!\nWith best practices and extensive experience from applying deep learning in research and development, they will get their own neural networks to train.\n\ud83d\ude80 Check it out in more detail, and get your own copy here (make sure to search on your amazon marketplace, we are basically available everywhere) >>\nhttps://lnkd.in/exfkjY9t\n\u2615 And as always, feel free to follow here on LinkedIn.\nhashtag\n#\ndatascience\nhashtag\n#\npython\nhashtag\n#\ndeeplearning\nhashtag\n#\nGiveaway\nhashtag\n#\nbooks\nhashtag\n#\nkdp\nhashtag\n#\nmachinelearning"
    },
    {
        "instruction": "Please compose for Medium platform a comprehensive guide for Lesson 1 of an eight-part series on deploying a real-time financial advisor system using LLMs, focusing on the design of the LLM architecture using the 3-pipeline pattern. Highlight the integration of streaming pipelines, RAG, LLMOps, and the use of AWS, vector databases, and serverless tools for building and deploying the system.",
        "post": "The framework to deploy LLM systems to do RAG | Decoding MLOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyThe Hands-On LLMs SeriesThe LLMs kit: Build a production-ready real-time financial advisor system using streaming pipelines, RAG, and LLMOpsLesson 1: LLM architecture system design using the 3-pipeline patternPaul Iusztin\u00b7FollowPublished inDecoding ML\u00b712 min read\u00b7Jan 5201ListenShareImage by DALL-E\u2192 the 1st out of 8 lessons of the Hands-On LLMs free courseBy finishing the Hands-On LLMs free course, you will learn how to use the 3-pipeline architecture & LLMOps good practices to design, build, and deploy a real-time financial advisor powered by LLMs & vector DBs.We will primarily focus on the engineering & MLOps aspects. Thus, by the end of this series, you will know how to build & deploy a real ML system, not some isolated code in Notebooks (we haven\u2019t used any Notebooks at all).More precisely, these are the 3 components you will learn to build:a real-time streaming pipeline (deployed on AWS) that listens to financial news, cleans & embeds the documents, and loads them to a vector DBa fine-tuning pipeline (deployed as a serverless continuous training) that fine-tunes an LLM on financial data using QLoRA, monitors the experiments using an experiment tracker and saves the best model to a model registryan inference pipeline built in LangChain (deployed as a serverless RESTful API) that loads the fine-tuned LLM from the model registry and answers financial questions using RAG (leveraging the vector DB populated with financial news in real-time)High-level architecture of the Hands-On LLMs FREE course [Image by the Author].We will also show you how to integrate various serverless tools, such as:Comet ML as your ML Platform;Qdrant as your vector DB;Beam as your infrastructure.Curios? Check the video below to understand better what you will learn \u2193Create an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0in201201FollowWritten by Paul Iusztin1.5K Followers\u00b7Editor for Decoding ML\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul Iusztin and Decoding MLPaul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111Paul IusztininDecoding MLThe ultimate guide on installing PyTorch with CUDA support in all possible ways\u2192 Using Pip, Conda, Poetry, Docker, or directly on the system\u00b710 min read\u00b7Jan 42231Paul IusztininDecoding MLThis is how you can build & deploy a streaming pipeline to populate a vector DB for real-time RAGLesson 3: Real-time text embeddings. Docker. GitHub Actions CI/CD pipeline. AWS\u00b720 min read\u00b7Jan 111861Paul IusztininDecoding MLPrepare your RAG LangChain application for productionLesson 8: LLMOps. Serverless infrastructure. RESTful API. Gradio.\u00b719 min read\u00b7Feb 3208See all from Paul IusztinSee all from Decoding MLRecommended from MediumPaul IusztininDecoding MLWhy you must choose streaming over batch pipelines when doing RAG in LLM applicationsLesson 2: RAG, streaming pipelines, vector DBs, text processing\u00b712 min read\u00b7Jan 9187AndreiinLlamaIndex BlogA Cheat Sheet and Some Recipes For Building Advanced RAGIt\u2019s the start of a new year and perhaps you\u2019re looking to break into the RAG scene by building your very first RAG system. Or, maybe\u20267 min read\u00b7Jan 51.5K5ListsPredictive Modeling w/ Python20 stories\u00b7891 savesPractical Guides to Machine Learning10 stories\u00b71032 savesNatural Language Processing1179 stories\u00b7656 savesdata science and AI40 stories\u00b767 savesGuodong (Troy) ZhaoinBootcampHow to improve RAG results in your LLM apps: from basics to advancedImprove your RAG quality and latency in your LLM app13 min read\u00b7Jan 228995Thu VuinTowards Data ScienceHow to Learn AI on Your Own (a self-study guide)If your hands touch a keyboard for work, Artificial Intelligence is going to change your job in the next few years.\u00b712 min read\u00b7Jan 53.2K27Suman DasFine Tune Large Language Model (LLM) on a Custom Dataset with QLoRAThe field of natural language processing has been revolutionized by large language models (LLMs), which showcase advanced capabilities and\u202615 min read\u00b7Jan 253383DatadriftersinDev GeniusDeploy XGBoost to Local Kubernetes Cluster using FastAPI: Movie Recommendation Use CaseEfficient deployment and scaling of Machine Learning models is critical for user experience in many real-world applications.\u00b711 min read\u00b7Oct 20, 2023761See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Share a comparison between ML development environments and continuous training setups on LinkedIn, focusing on their distinct purposes and designs. Highlight the strategic importance of each in the lifecycle of ML system deployment.",
        "post": "What is the \ud835\uddf1\ud835\uddf6\ud835\uddf3\ud835\uddf3\ud835\uddf2\ud835\uddff\ud835\uddf2\ud835\uddfb\ud835\uddf0\ud835\uddf2 between your \ud835\udde0\ud835\udddf \ud835\uddf1\ud835\uddf2\ud835\ude03\ud835\uddf2\ud835\uddf9\ud835\uddfc\ud835\uddfd\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01 and \ud835\uddf0\ud835\uddfc\ud835\uddfb\ud835\ude01\ud835\uddf6\ud835\uddfb\ud835\ude02\ud835\uddfc\ud835\ude02\ud835\ude00 \ud835\ude01\ud835\uddff\ud835\uddee\ud835\uddf6\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddf2\ud835\uddfb\ud835\ude03\ud835\uddf6\ud835\uddff\ud835\uddfc\ud835\uddfb\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01\ud835\ude00?\nThey might do the same thing, but their design is entirely different \u2193\n\ud835\udde0\ud835\udddf \ud835\uddd7\ud835\uddf2\ud835\ude03\ud835\uddf2\ud835\uddf9\ud835\uddfc\ud835\uddfd\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01 \ud835\uddd8\ud835\uddfb\ud835\ude03\ud835\uddf6\ud835\uddff\ud835\uddfc\ud835\uddfb\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01\nAt this point, your main goal is to ingest the raw and preprocessed data through versioned artifacts (or a feature store), analyze it & generate as many experiments as possible to find the best:\n- model\n- hyperparameters\n- augmentations\nBased on your business requirements, you must maximize some specific metrics, find the best latency-accuracy trade-offs, etc.\nYou will use an experiment tracker to compare all these experiments.\nAfter you settle on the best one, the output of your ML development environment will be:\n- a new version of the code\n- a new version of the configuration artifact\nHere is where the research happens. Thus, you need flexibility.\nThat is why we decouple it from the rest of the ML systems through artifacts (data, config, & code artifacts).\n\ud835\uddd6\ud835\uddfc\ud835\uddfb\ud835\ude01\ud835\uddf6\ud835\uddfb\ud835\ude02\ud835\uddfc\ud835\ude02\ud835\ude00 \ud835\udde7\ud835\uddff\ud835\uddee\ud835\uddf6\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddd8\ud835\uddfb\ud835\ude03\ud835\uddf6\ud835\uddff\ud835\uddfc\ud835\uddfb\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01\nHere is where you want to take the data, code, and config artifacts and:\n- train the model on all the required data\n- output a staging versioned model artifact\n- test the staging model artifact\n- if the test passes, label it as the new production model artifact\n- deploy it to the inference services\nA common strategy is to build a CI/CD pipeline that (e.g., using GitHub Actions):\n- builds a docker image from the code artifact (e.g., triggered manually or when a new artifact version is created)\n- start the training pipeline inside the docker container that pulls the feature and config artifacts and outputs the staging model artifact\n- manually look over the training report -> If everything went fine, manually trigger the testing pipeline\n- manually look over the testing report -> if everything worked fine (e.g., the model is better than the previous one), manually trigger the CD pipeline that deploys the new model to your inference services\nNote how the model registry quickly helps you to decouple all the components.\nAlso, because training and testing metrics are not always black & white, it is tough to 100% automate the CI/CD pipeline.\nThus, you need a human in the loop when deploying ML models.\nTo conclude...\nThe ML development environment is where you do your research to find better models:\n- \ud835\ude2a\ud835\ude2f\ud835\ude31\ud835\ude36\ud835\ude35: data artifact\n- \ud835\ude30\ud835\ude36\ud835\ude35\ud835\ude31\ud835\ude36\ud835\ude35: code & config artifacts\nThe continuous training environment is used to train & test the production model at scale:\n- \ud835\ude2a\ud835\ude2f\ud835\ude31\ud835\ude36\ud835\ude35: data, code, config artifacts\n- \ud835\ude30\ud835\ude36\ud835\ude35\ud835\ude31\ud835\ude36\ud835\ude35: model artifact\nThis is not a fixed solution, as ML systems are still an open question. I would love to see your opinion in the comments. \u2193\n.\nBut if you want to see this strategy in action,\nCheck out my \ud835\udde7\ud835\uddf5\ud835\uddf2 \ud835\uddd9\ud835\ude02\ud835\uddf9\ud835\uddf9 \ud835\udde6\ud835\ude01\ud835\uddee\ud835\uddf0\ud835\uddf8 \ud835\udff3-\ud835\udde6\ud835\ude01\ud835\uddf2\ud835\uddfd\ud835\ude00 \ud835\udde0\ud835\udddf\ud835\udde2\ud835\uddfd\ud835\ude00 \ud835\uddd9\ud835\uddff\ud835\uddee\ud835\uddfa\ud835\uddf2\ud835\ude04\ud835\uddfc\ud835\uddff\ud835\uddf8 FREE Course.\nLink in the comments \u2193\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning"
    },
    {
        "instruction": "Highlight the top 6 ML Platform features crucial for developing and maintaining ML systems, emphasizing their role in ensuring reproducibility, collaboration, and operational efficiency.",
        "post": "\ud835\udde7\ud835\uddfc\ud835\uddfd \ud835\udff2 \ud835\udde0\ud835\udddf \ud835\udde3\ud835\uddf9\ud835\uddee\ud835\ude01\ud835\uddf3\ud835\uddfc\ud835\uddff\ud835\uddfa \ud835\uddf3\ud835\uddf2\ud835\uddee\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2\ud835\ude00 you must know and use in your ML system.\nHere they are \u2193\n#\ud835\udfed. \ud835\uddd8\ud835\ude05\ud835\uddfd\ud835\uddf2\ud835\uddff\ud835\uddf6\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01 \ud835\udde7\ud835\uddff\ud835\uddee\ud835\uddf0\ud835\uddf8\ud835\uddf6\ud835\uddfb\ud835\uddf4\nIn your ML development phase, you generate lots of experiments.\nTracking and comparing the metrics between them is crucial in finding the optimal model.\n#\ud835\udfee. \ud835\udde0\ud835\uddf2\ud835\ude01\ud835\uddee\ud835\uddf1\ud835\uddee\ud835\ude01\ud835\uddee \ud835\udde6\ud835\ude01\ud835\uddfc\ud835\uddff\ud835\uddf2\nIts primary purpose is reproducibility.\nTo know how a model was generated, you need to know:\n- the version of the code\n- the version of the packages\n- hyperparameters/config\n- total compute\n- version of the dataset\n... and more\n#\ud835\udfef. \ud835\udde9\ud835\uddf6\ud835\ude00\ud835\ude02\ud835\uddee\ud835\uddf9\ud835\uddf6\ud835\ude00\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb\ud835\ude00\nMost of the time, along with the metrics, you must log a set of visualizations for your experiment.\nSuch as:\n- images\n- videos\n- prompts\n- t-SNE graphs\n- 3D point clouds\n... and more\n#\ud835\udff0. \ud835\udde5\ud835\uddf2\ud835\uddfd\ud835\uddfc\ud835\uddff\ud835\ude01\ud835\ude00\nYou don't work in a vacuum.\nYou have to present your work to other colleges or clients.\nA report lets you take the metadata and visualizations from your experiment...\n...and create, deliver and share a targeted presentation for your clients or peers.\n#\ud835\udff1. \ud835\uddd4\ud835\uddff\ud835\ude01\ud835\uddf6\ud835\uddf3\ud835\uddee\ud835\uddf0\ud835\ude01\ud835\ude00\nThe most powerful feature out of them all.\nAn artifact is a versioned object that is an input or output for your task.\nEverything can be an artifact, but the most common cases are:\n- data\n- model\n- code\nWrapping your assets around an artifact ensures reproducibility.\nFor example, you wrap your features into an artifact (e.g., features:3.1.2), which you can consume into your ML development step.\nThe ML development step will generate config (e.g., config:1.2.4) and code (e.g., code:1.0.2) artifacts used in the continuous training pipeline.\nDoing so lets you quickly respond to questions such as \"What I used to generate the model?\" and \"What Version?\"\n#\ud835\udff2. \ud835\udde0\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddf9 \ud835\udde5\ud835\uddf2\ud835\uddf4\ud835\uddf6\ud835\ude00\ud835\ude01\ud835\uddff\ud835\ude06\nThe model registry is the ultimate way to make your model accessible to your production ecosystem.\nFor example, in your continuous training pipeline, after the model is trained, you load the weights as an artifact into the model registry (e.g., model:1.2.4).\nYou label this model as \"staging\" under a new version and prepare it for testing. If the tests pass, mark it as \"production\" under a new version and prepare it for deployment (e.g., model:2.1.5).\n.\nAll of these features are used in a mature ML system. What is your favorite one?\nYou can see all these features in action in my \ud835\udde7\ud835\uddf5\ud835\uddf2 \ud835\uddd9\ud835\ude02\ud835\uddf9\ud835\uddf9 \ud835\udde6\ud835\ude01\ud835\uddee\ud835\uddf0\ud835\uddf8 \ud835\udff3-\ud835\udde6\ud835\ude01\ud835\uddf2\ud835\uddfd\ud835\ude00 \ud835\udde0\ud835\udddf\ud835\udde2\ud835\uddfd\ud835\ude00 \ud835\uddd9\ud835\uddff\ud835\uddee\ud835\uddfa\ud835\uddf2\ud835\ude04\ud835\uddfc\ud835\uddff\ud835\uddf8 FREE course. Link in the comments \u2193\nhashtag\n#\nmlops\nhashtag\n#\nmachinelearning\nhashtag\n#\ndata\n-----\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps."
    },
    {
        "instruction": "Write a Medium article for Lesson 5 of an eight-part series detailing the process of writing generic code to fine-tune any LLM using QLoRA. Include practical steps for setting up a Q&A dataset loader, prompt templates, and training configurations, emphasizing the flexibility and efficiency of the approach.",
        "post": "Write generic code to fine-tune any LLM using QLoRA | Decoding MLOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyThe Hands-On LLMs SeriesThe secret of writing generic code to fine-tune any LLM using QLoRALesson 5: Q&A dataset loader. Prompt templates. Training configs.Paul Iusztin\u00b7FollowPublished inDecoding ML\u00b717 min read\u00b7Jan 18102ListenShareImage by DALL-E\u2192 the 5th out of 8 lessons of the Hands-On LLMs free courseBy finishing the Hands-On LLMs free course, you will learn how to use the 3-pipeline architecture & LLMOps good practices to design, build, and deploy a real-time financial advisor powered by LLMs & vector DBs.We will primarily focus on the engineering & MLOps aspects. Thus, by the end of this series, you will know how to build & deploy a real ML system, not some isolated code in Notebooks (we haven\u2019t used any Notebooks at all).More precisely, these are the 3 components you will learn to build:a real-time streaming pipeline (deployed on AWS) that listens to financial news, cleans & embeds the documents, and loads them to a vector DBa fine-tuning pipeline (deployed as a serverless continuous training) that fine-tunes an LLM on financial data using QLoRA, monitors the experiments using an experiment tracker and saves the best model to a model registryan inference pipeline built in LangChain (deployed as a serverless RESTful API) that loads the fine-tuned LLM from the model registry and answers financial questions using RAG (leveraging the vector DB populated with financial news in real-time)High-level architecture of the Hands-On LLMs FREE course [Image by the Author].We will also show you how to integrate various serverless tools, such as:Comet ML: as your ML Platform;Qdrant: as your vector DB;Beam: as your serverless infrastructure.Curios? Check the video below to understand better what you will learn \u2193Create an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0in102102FollowWritten by Paul Iusztin1.5K Followers\u00b7Editor for Decoding ML\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul Iusztin and Decoding MLPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Paul IusztininDecoding MLThe ultimate guide on installing PyTorch with CUDA support in all possible ways\u2192 Using Pip, Conda, Poetry, Docker, or directly on the system\u00b710 min read\u00b7Jan 42231Paul IusztininDecoding MLPrepare your RAG LangChain application for productionLesson 8: LLMOps. Serverless infrastructure. RESTful API. Gradio.\u00b719 min read\u00b7Feb 3208Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111See all from Paul IusztinSee all from Decoding MLRecommended from MediumPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Suman DasFine Tune Large Language Model (LLM) on a Custom Dataset with QLoRAThe field of natural language processing has been revolutionized by large language models (LLMs), which showcase advanced capabilities and\u202615 min read\u00b7Jan 253383ListsPredictive Modeling w/ Python20 stories\u00b7891 savesPractical Guides to Machine Learning10 stories\u00b71032 savesNatural Language Processing1179 stories\u00b7656 savesdata science and AI40 stories\u00b767 savesMohammad FaizaninMLearning.aiMLOps: Weights & Biases Experiments TrackingMachine Learning projects have hyperparameters, configurations, datasets, and code versions. It is sometimes hard to keep track of what you\u20263 min read\u00b7Aug 16, 20231141Ivan ReznikovinTowards AILangChain 101: Part 3a. Talking to Documents: Load, Split and simple RAG with LCELThis is Part 3 of the Langchain 101 series, where we\u2019ll discuss how to load data, split it, store data, and create simple RAG with LCEL9 min read\u00b75 days ago1051Rania _HossamLLAMA FROM SCRATCHMeta AI and Microsoft have joined forces to introduce Llama 2, the next generation of Meta\u2019s open-source large language model.13 min read\u00b7Oct 22, 20233222Dillip AmurQLoRA: Fine-Tuning Large Language Models (LLM\u2019s)In this blog I will aim to explain the concept and important terminology related to QLoRA. The focus of this blog is to provide better\u202612 min read\u00b7Nov 28, 202361See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Discuss the versatility of transforming data with a feature store in an ML production system on LinkedIn. Highlight three innovative approaches to data transformation that enhance model performance and system efficiency.",
        "post": "These are 3 ways you didn't know about how you can transform your data when using a feature store.\nA feature store helps you quickly solve the training serving skew issue by offering you a consistent way to transform your data into features between the training and inference pipelines.\nThe issue boils down to WHEN you do the transformation.\nWhen using a feature store, there are 3 main ways you can transform your data:\n\ud835\udfcf. \ud835\udc01\ud835\udc1e\ud835\udc1f\ud835\udc28\ud835\udc2b\ud835\udc1e \ud835\udc2c\ud835\udc2d\ud835\udc28\ud835\udc2b\ud835\udc22\ud835\udc27\ud835\udc20 \ud835\udc2d\ud835\udc21\ud835\udc1e \ud835\udc1d\ud835\udc1a\ud835\udc2d\ud835\udc1a \ud835\udc22\ud835\udc27 \ud835\udc2d\ud835\udc21\ud835\udc1e \ud835\udc1f\ud835\udc1e\ud835\udc1a\ud835\udc2d\ud835\udc2e\ud835\udc2b\ud835\udc1e \ud835\udc2c\ud835\udc2d\ud835\udc28\ud835\udc2b\ud835\udc1e\nIn the feature engineering pipeline, you do everything: clean, validate, aggregate, reduce, and transform your data.\nEven if this is the most intuitive way of doing things, it is the worse.\n\ud83d\udfe2 ultra-low latency\n\ud83d\udd34 hard to do EDA on transformed data\n\ud83d\udd34 store duplicated/redundant data\n\ud835\udfd0. \ud835\udc12\ud835\udc2d\ud835\udc28\ud835\udc2b\ud835\udc1e \ud835\udc2d\ud835\udc21\ud835\udc1e \ud835\udc2d\ud835\udc2b\ud835\udc1a\ud835\udc27\ud835\udc2c\ud835\udc1f\ud835\udc28\ud835\udc2b\ud835\udc26\ud835\udc1a\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27 \ud835\udc22\ud835\udc27 \ud835\udc32\ud835\udc28\ud835\udc2e\ud835\udc2b \ud835\udc29\ud835\udc22\ud835\udc29\ud835\udc1e\ud835\udc25\ud835\udc22\ud835\udc27\ud835\udc1e \ud835\udc28\ud835\udc2b \ud835\udc26\ud835\udc28\ud835\udc1d\ud835\udc1e\ud835\udc25 \ud835\udc29\ud835\udc2b\ud835\udc1e-\ud835\udc29\ud835\udc2b\ud835\udc28\ud835\udc1c\ud835\udc1e\ud835\udc2c\ud835\udc2c\ud835\udc22\ud835\udc27\ud835\udc20 \ud835\udc25\ud835\udc1a\ud835\udc32\ud835\udc1e\ud835\udc2b\ud835\udc2c\nIn the feature engineering pipeline, you perform only the cleaning, validation, aggregations, and reduction steps.\nLater, by incorporating all your transformations into your pipeline object or pre-processing layers, you automatically save them along your model.\nThus, you can input your cleaned data into your pipeline, and it will know how to handle it.\n\ud83d\udfe2 store only cleaned data\n\ud83d\udfe2 easily explore your data\n\ud83d\udd34 the transformations are done on the client\n\ud835\udfd1. \ud835\udc18\ud835\udc28\ud835\udc2e \ud835\udc1a\ud835\udc2d\ud835\udc2d\ud835\udc1a\ud835\udc1c\ud835\udc21 \ud835\udc2d\ud835\udc28 \ud835\udc1e\ud835\udc2f\ud835\udc1e\ud835\udc2b\ud835\udc32 \ud835\udc1c\ud835\udc25\ud835\udc1e\ud835\udc1a\ud835\udc27\ud835\udc1e\ud835\udc1d \ud835\udc1d\ud835\udc1a\ud835\udc2d\ud835\udc1a \ud835\udc2c\ud835\udc28\ud835\udc2e\ud835\udc2b\ud835\udc1c\ud835\udc1e \ud835\udc1a \ud835\udc14\ud835\udc03\ud835\udc05 \ud835\udc2d\ud835\udc2b\ud835\udc1a\ud835\udc27\ud835\udc2c\ud835\udc1f\ud835\udc28\ud835\udc2b\ud835\udc26\ud835\udc1a\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27\nThis is similar to solution 2., but instead of attaching the transformation directly to your model, you attached them as a UDF to the feature store.\nfeature = cleaned data source + UDF\nSo when you request a feature, the feature store will automatically trigger the UDF on a server and return it.\n\ud83d\udfe2 store only cleaned data\n\ud83d\udfe2 easily explore your data\n\ud83d\udfe2 the transformations are done on the server\n\ud83d\udfe2 scalable (using Spark)\n\ud83d\udd34 hard to implement\nAs a recap,\nThere are 3 ways you can perform your transformations to solve the train serving skew when using a feature store.\nWhat method do you think is the best?\n.\n\u21b3 To see method #\ud835\udfee. in action\nCheck out my \ud835\ude08 \ud835\ude0e\ud835\ude36\ud835\ude2a\ud835\ude25\ud835\ude26 \ud835\ude35\ud835\ude30 \ud835\ude09\ud835\ude36\ud835\ude2a\ud835\ude2d\ud835\ude25\ud835\ude2a\ud835\ude2f\ud835\ude28 \ud835\ude0c\ud835\ude27\ud835\ude27\ud835\ude26\ud835\ude24\ud835\ude35\ud835\ude2a\ud835\ude37\ud835\ude26 \ud835\ude1b\ud835\ude33\ud835\ude22\ud835\ude2a\ud835\ude2f\ud835\ude2a\ud835\ude2f\ud835\ude28 \ud835\ude17\ud835\ude2a\ud835\ude31\ud835\ude26\ud835\ude2d\ud835\ude2a\ud835\ude2f\ud835\ude26\ud835\ude34 \ud835\ude27\ud835\ude30\ud835\ude33 \ud835\ude14\ud835\ude22\ud835\ude39\ud835\ude2a\ud835\ude2e\ud835\ude36\ud835\ude2e \ud835\ude19\ud835\ude26\ud835\ude34\ud835\ude36\ud835\ude2d\ud835\ude35\ud835\ude34 article\nLink in the comments \u2193\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps."
    },
    {
        "instruction": "Introduce the capability of embedding spreadsheets in Streamlit applications using Mito in a LinkedIn post. Highlight its benefits for data exploration and analysis within Streamlit dashboards, simplifying data manipulation.",
        "post": "Here is how you can \ud835\uddf2\ud835\uddfa\ud835\uddef\ud835\uddf2\ud835\uddf1 \ud835\uddee \ud835\ude00\ud835\uddfd\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddf1\ud835\ude00\ud835\uddf5\ud835\uddf2\ud835\uddf2\ud835\ude01 \ud835\uddf1\ud835\uddf6\ud835\uddff\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\uddf9\ud835\ude06 \ud835\uddf6\ud835\uddfb\ud835\ude01\ud835\uddfc \ud835\ude06\ud835\uddfc\ud835\ude02\ud835\uddff \ud835\udde6\ud835\ude01\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddfa\ud835\uddf9\ud835\uddf6\ud835\ude01 \ud835\uddee\ud835\uddfd\ud835\uddfd\ud835\uddf9\ud835\uddf6\ud835\uddf0\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb with just \ud835\uddee \ud835\uddf3\ud835\uddf2\ud835\ude04 \ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2\ud835\ude00 \ud835\uddfc\ud835\uddf3 \ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2.\nFor sure, you heard about\nTry Mito\n.\n\ud835\ude10\ud835\ude27 \ud835\ude2f\ud835\ude30\ud835\ude35, \ud835\ude29\ud835\ude26\ud835\ude33\ud835\ude26 \ud835\ude2a\ud835\ude34 \ud835\ude22 \ud835\ude32\ud835\ude36\ud835\ude2a\ud835\ude24\ud835\ude2c \ud835\ude33\ud835\ude26\ud835\ude24\ud835\ude22\ud835\ude31:\n\"Mito is a way to incorporate an Excel-like component into your beloved notebooks to explore data without writing any Python code (it writes it for you based on your interactions).\"\n.\nRecently, they introduced the same experience, but this time for Streamlit.\nNow you can add a spreadsheet in your dashboard with just a few lines of code:\n\"\"\"\nfrom mitosheet.streamlit.v1 import spreadsheet\n# ... rest of your streamlit app\nspreadsheet()\n\"\"\"\n.\n\ud835\udddb\ud835\uddf2\ud835\uddff\ud835\uddf2 \ud835\uddf6\ud835\ude00 \ud835\ude04\ud835\uddf5\ud835\uddee\ud835\ude01 \ud835\uddf6\ud835\ude01 \ud835\uddf0\ud835\uddee\ud835\uddfb \ud835\uddf1\ud835\uddfc:\n- Import, clean, and transform datasets into a format required by the rest of the Streamlit app.\n- Do flexible data exploration and analysis.\n- Create Python scripts using Mito's code-gen capabilities.\n.\nAccess the docs in the comments \u2193\nhashtag\n#\nmachinelearning\nhashtag\n#\ndatascience\nhashtag\n#\ndata\n-----\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps."
    },
    {
        "instruction": "Provide travel tips for using Airbnb in the Netherlands, based on data analysis of Airbnb listings. In the medium post include comparisons of average prices, demand, and review scores across major cities and neighborhoods to help travelers make informed decisions.",
        "post": "This Is What You Should Know When Travelling to the Netherlands With Airbnb | by Paul Iusztin | MediumOpen in appSign upSign inWriteSign upSign inMastodonThis Is What You Should Know When Travelling to the Netherlands With AirbnbPaul Iusztin\u00b7Follow5 min read\u00b7Jun 28, 2022531ListenSharePhoto by Roman Kraft on UnsplashIntroductionIn the last period, I was considering moving to The Netherlands. I always heard that the countries in the North of Europe are the happiest in the world. Also, I read about their great culture, people, and tech opportunities (yes, I work in tech). In the end, I said to myself that it would be a great experience to try to live in such a country.My first choice was The Netherlands because it is not that far into the North and I already visited Amsterdam, which I liked a lot. I think it had a great vibe. But before taking the big step of moving there, I would like to visit the country for a longer period to see how I would fit into their culture.Now which is the easiest way to travel around these days? You guessed it, with Airbnb or other similar services like Booking.Therefore, I used the data provided by Airbnb to see what would be the best places to rent. The data can be accessed here. It contains a lot of interesting features. Starting from the price & reviews until its availability, on multiple time ranges, of a listing.Airbnb logo. Source.They provide quarterly data for the last 12 months. At the time I wrote this article their last update was on the 5th of June 2022. Therefore the data spans from 05.06.2021 until 05.06.2022.The analysis will contain the biggest cities in The Netherlands: Amsterdam, Rotterdam, and The Hague.Content1\ufe0f\u20e3 Which city has the highest price tag? What about their neighborhoods?2\ufe0f\u20e3 What city and neighborhoods are in most demand?3\ufe0f\u20e3 Are reviews for more expensive houses better?4\ufe0f\u20e3 What are the factors that affect a property\u2019s price?1\ufe0f\u20e3 Which city has the highest price tag? What about their neighborhoods?As we can see in the figure below the most expensive city is Amsterdam. Rotterdam and The Hague are in the same price range.Fig 1: Mean prices per night for the cities.The interesting part comes when we drill down the prices/night in different neighborhoods. In the figure below we can see the cheapest and most expensive neighborhoods. We can see that the prices in Amsterdam don\u2019t vary so much, while in The Hague there are some extravagant neighborhoods you can stay in. Also, if you want to spend as less as possible probably The Hague or Rotterdam is the way to go.Fig 2: Mean prices per night for the neighborhoods.2\ufe0f\u20e3 What city and neighborhoods are in most demand?Fig 3: Demand for the cities.In the figure below we can see that people that are going to Amsterdam tend to stick to the mean price which is ~$160. In Rotterdam and The Hague people either choose more expensive properties, around $160 as in Amsterdam, or cheaper ones under $50.Also, the luxurious properties cost more than $500 per night. As you can see that kind of neighborhood are not in the least in-demand group, suggesting that people are willing to pay that kind of money for a luxurious holiday. The neighborhoods that are least in demand are actually having prices close to the mean price of the particular city.Fig 4: Demand for the neighborhoods relative to their price range. The bars are ordered from the ones with the least to the ones with the highest demand.3\ufe0f\u20e3 Are reviews for more expensive houses better?Well, the answer is yes and no. We can see that the price ranges between $1000 \u2014 $1500 are having the best overall score rating. For this range is a yes. But the really expensive properties, with a range of $1500-$1750, are having the worst performance. Probably people that are booking those kinds of properties are having high expectations or are harder to please.What is good to see is that the lower price ranges of $0\u2013$500 are still having good reviews. Therefore, it is safe to look into that range.Fig 5: USD price range/night relative to their mean review score rating.4\ufe0f\u20e3 What are the factors that affect a property\u2019s price?We can see some obvious factors like the number of accommodates, bedrooms and property type. Also, the neighborhood/location has a clear impact on the price.What is more interesting is that the demand for property dictates its price. This is reflected in the availability of 30, 60, 90, and 365 factors (those reflect the number of days a property was free in a specific period).Also, we can see that the number of reviews and when the last one was written have a high impact. One last, interesting observation would be that the period since the host is on Airbnb influence the price. Probably, because he has more experience.Fig 6: Most important factors that influence the price of a property.ConclusionIn this article, we looked at how to book your favorite Airbnb property in The Netherlands by using their public dataset.We looked at the mean price in the biggest cities. Also, we drilled down on the most & least expensive neighborhoods.We compared which cities and neighborhoods are in the most demand.We compared the review score rating over different price ranges to see if a pricier property is actually better.We made a list of the most important factors that influence the price of a listing.What other factors would you consider when looking for an Airbnb property?If you want to see the code of this analysis you can check out the GitHub repository, which is available here.Thank you for reading my article!Do not hesitate to reach me via the following :Follow me on MediumConnect and reach me on LinkedInWrite me at my mail address: p.b.iusztin@gmail.comSign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipAccess the best member-only stories.Support independent authors.Listen to audio narrations.Read offline.Join the Partner Program and earn for your writing.Try for $5/monthThe NetherlandsTravelAmsterdamAirbnbBooking53531FollowWritten by Paul Iusztin1.5K Followers\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul IusztinPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111Paul IusztininDecoding MLThe ultimate guide on installing PyTorch with CUDA support in all possible ways\u2192 Using Pip, Conda, Poetry, Docker, or directly on the system\u00b710 min read\u00b7Jan 42231Paul IusztininDecoding MLPrepare your RAG LangChain application for productionLesson 8: LLMOps. Serverless infrastructure. RESTful API. Gradio.\u00b719 min read\u00b7Feb 3208See all from Paul IusztinRecommended from MediumPaul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111Moez AliEasily deploy machine learning models from the comfort of your NotebookStreamline your Machine Learning model deployment with Modelbit.com. Seamlessly move from Jupyter notebooks to production-ready REST APIs\u20269 min read\u00b7Sep 8, 20232621ListsCompany Offsite Reading List8 stories\u00b788 savesStories to Help You Live Better19 stories\u00b71169 savesSome of My Favorite Personal Essays18 stories\u00b7663 savesVisual Storytellers Playlist58 stories\u00b7202 savesMarcello DichierainData And BeyondAnomaly Detection Data Pipeline and Dashboard with Mage AI and StreamlitEasy steps to build a data pipeline and dashboard for anomaly detection with Mage AI and Streamlit\u00b78 min read\u00b7Aug 30, 2023235Niranjan AppajiA Guide to Handling High Cardinality in Categorical VariablesHigh cardinality refers to a situation in a dataset where a particular feature has a large number of distinct values. In other words, a\u20265 min read\u00b7Dec 28, 20232Jacob BennettinLevel Up CodingThe 5 paid subscriptions I actually use in 2024 as a software engineerTools I use that are cheaper than Netflix\u00b75 min read\u00b7Jan 48.2K96Nitesh ChaudhryFAANG+ Data Engineer Learning roadmap for 2024With AI & Data Science taking the center stage of all new tech investments, data engineering is set to grow rapidly with time. Good AI\u20262 min read\u00b7Jan 51.1K5See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Write a brief Medium article explaining your motivation for creating content on Medium, focusing on your mission to assist machine learning engineers in advancing their skills through practical posts on designing and operationalizing ML/MLOps systems.",
        "post": "Why Am I Writing?. My mission for writing Medium articles. | by Paul Iusztin | MediumOpen in appSign upSign inWriteSign upSign inMastodonAbout Me\u2712\ufe0f Why Am I Writing?My mission for writing Medium articles.Paul Iusztin\u00b7Follow1 min read\u00b7Nov 3, 20222832ListenSharePhoto by \u00c1lvaro Serrano on Unsplash\ud83d\ude80 My mission for writing Medium articles is to help machine learning engineers level up through hands-on practical posts, articles, and tutorials on designing and productionizing ML/MLOps systems.My dream is to master the craft of building, training, deploying, and monitoring large machine learning systems. Thus, reading, creating, and giving something to the community is essential for my learning process.Let\u2019s connect if you want to level up in productionizing ML systems:I post almost daily AI content on \ud83d\udc47\ud83c\udffc\ud83d\udcd8 LinkedIn\ud83d\udd8b\ufe0f Newsletter\ud83d\udcbb GitHub\u2709\ufe0f Email\ud83d\udc26 TwitterCreating content takes me a lot of time. If you enjoyed my work, you could support me by:joining Medium through my referral link, you can support me without any extra cost while enjoying limitless access to Medium\u2019s rich collection of stories:Join Medium with my referral link - Paul IusztinJoin to get exclusive content about best practices for taking ML systems into production \ud83d\ude80 Unlock full access to\u2026pauliusztin.medium.com2. buying me a coffee:Paul Iusztin is writing about Machine Learning, Computer Vision, and MLOps.\ud83e\udd16 My mission is to build practical intuition about professional ML and MLOps applications. \ud83d\ude80 Your support will\u2026www.buymeacoffee.comThank you \u270c\ud83c\udffc !Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipAccess the best member-only stories.Support independent authors.Listen to audio narrations.Read offline.Join the Partner Program and earn for your writing.Try for $5/monthAbout Me2832832FollowWritten by Paul Iusztin1.5K Followers\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul IusztinPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111Paul IusztininDecoding MLThe ultimate guide on installing PyTorch with CUDA support in all possible ways\u2192 Using Pip, Conda, Poetry, Docker, or directly on the system\u00b710 min read\u00b7Jan 42231Paul IusztininDecoding MLPrepare your RAG LangChain application for productionLesson 8: LLMOps. Serverless infrastructure. RESTful API. Gradio.\u00b719 min read\u00b7Feb 3208See all from Paul IusztinRecommended from MediumPaul Is PositiveinILLUMINATIONHow To Detect An AI Written ArticleWith the emergence of AI comes the rise of the fake writer\u00b74 min read\u00b7Jan 245.3K129C\u00e1ssio BolbaChange Data Capture (CDC) in Snowflakea introductory guide on snowflake streams and its advantages\u00b76 min read\u00b73 days ago1.4K21ListsStaff Picks577 stories\u00b7738 savesStories to Help You Level-Up at Work19 stories\u00b7469 savesSelf-Improvement 10120 stories\u00b71326 savesProductivity 10120 stories\u00b71211 savesThu VuinTowards Data ScienceHow to Learn AI on Your Own (a self-study guide)If your hands touch a keyboard for work, Artificial Intelligence is going to change your job in the next few years.\u00b712 min read\u00b7Jan 53.2K27Nikhil AdithyaninLevel Up CodingStock Market Sentiment Prediction with OpenAI and PythonAn interesting exploration of the power of LLMs in stock analysis11 min read\u00b76 days ago4621Cassie KozyrkovWhat are: Embeddings? Vector Databases? Vector Search? k-NN? ANN?A simple explainer to debuzz these AI buzzwords\u00b76 min read\u00b7Feb 21.6K15Dorian TeffoinDataDrivenInvestorFreelance Data Engineering Roadmap+Project ideasWe are in 2024, and you want to become a successful Data Engineer and Freelancer. You are at the right place!4 min read\u00b7Jan 264063See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Describe in a Medium post the rationale for preferring streaming over batch pipelines in RAG applications for LLMs, as explored in Lesson 2 of an eight-part series. Cover the advantages of RAG, streaming pipelines, vector databases, and text processing for achieving efficient LLM applications.",
        "post": "Streaming over batch pipelines for RAG applications | Decoding MLOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyThe Hands-On LLMs SeriesWhy you must choose streaming over batch pipelines when doing RAG in LLM applicationsLesson 2: RAG, streaming pipelines, vector DBs, text processingPaul Iusztin\u00b7FollowPublished inDecoding ML\u00b712 min read\u00b7Jan 9187ListenShareImage by DALL-E\u2192 the 2nd out of 8 lessons of the Hands-On LLMs free courseBy finishing the Hands-On LLMs free course, you will learn how to use the 3-pipeline architecture & LLMOps good practices to design, build, and deploy a real-time financial advisor powered by LLMs & vector DBs.We will primarily focus on the engineering & MLOps aspects. Thus, by the end of this series, you will know how to build & deploy a real ML system, not some isolated code in Notebooks (we haven\u2019t used any Notebooks at all).More precisely, these are the 3 components you will learn to build:a real-time streaming pipeline (deployed on AWS) that listens to financial news, cleans & embeds the documents, and loads them to a vector DBa fine-tuning pipeline (deployed as a serverless continuous training) that fine-tunes an LLM on financial data using QLoRA, monitors the experiments using an experiment tracker and saves the best model to a model registryan inference pipeline built in LangChain (deployed as a serverless RESTful API) that loads the fine-tuned LLM from the model registry and answers financial questions using RAG (leveraging the vector DB populated with financial news in real-time)High-level architecture of the Hands-On LLMs FREE course [Image by the Author].We will also show you how to integrate various serverless tools, such as:Comet ML as your ML Platform;Qdrant as your vector DB;Beam as your infrastructure.Curios? Check the video below to understand better what you will learn \u2193Create an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0in187187FollowWritten by Paul Iusztin1.5K Followers\u00b7Editor for Decoding ML\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul Iusztin and Decoding MLPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Paul IusztininDecoding MLThe ultimate guide on installing PyTorch with CUDA support in all possible ways\u2192 Using Pip, Conda, Poetry, Docker, or directly on the system\u00b710 min read\u00b7Jan 42231Paul IusztininDecoding MLPrepare your RAG LangChain application for productionLesson 8: LLMOps. Serverless infrastructure. RESTful API. Gradio.\u00b719 min read\u00b7Feb 3208Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111See all from Paul IusztinSee all from Decoding MLRecommended from MediumPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Suman DasFine Tune Large Language Model (LLM) on a Custom Dataset with QLoRAThe field of natural language processing has been revolutionized by large language models (LLMs), which showcase advanced capabilities and\u202615 min read\u00b7Jan 253383ListsPredictive Modeling w/ Python20 stories\u00b7891 savesPractical Guides to Machine Learning10 stories\u00b71032 savesNatural Language Processing1179 stories\u00b7656 savesdata science and AI40 stories\u00b767 savesDatadriftersinDev GeniusDeploy XGBoost to Local Kubernetes Cluster using FastAPI: Movie Recommendation Use CaseEfficient deployment and scaling of Machine Learning models is critical for user experience in many real-world applications.\u00b711 min read\u00b7Oct 20, 2023761Wenqi GlantzinTowards Data ScienceDeploying LLM Apps to AWS, the Open-Source Self-Service WayA step-by-step guide on deploying LlamaIndex RAGs to AWS ECS fargate\u00b712 min read\u00b7Jan 86393AndreiinLlamaIndex BlogA Cheat Sheet and Some Recipes For Building Advanced RAGIt\u2019s the start of a new year and perhaps you\u2019re looking to break into the RAG scene by building your very first RAG system. Or, maybe\u20267 min read\u00b7Jan 51.5K5Guodong (Troy) ZhaoinBootcampHow to improve RAG results in your LLM apps: from basics to advancedImprove your RAG quality and latency in your LLM app13 min read\u00b7Jan 228995See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Develop a tutorial to be posted on Medium, on automating AWS infrastructure setup using Terraform for MLEs, covering the basics of Terraform, creating a production-ready AWS infrastructure, and writing clean, modular Terraform files.",
        "post": "Stop Manually Creating Your AWS Infrastructure. Use Terraform! | by Paul Iusztin | Towards AIOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyStop Manually Creating Your AWS Infrastructure. Use Terraform!Terraform 101: How to Use Terraform as an MLE to Automate a Production-Ready AWS InfrastructurePaul Iusztin\u00b7FollowPublished inTowards AI\u00b77 min read\u00b7Aug 18, 2023159ListenSharePhoto by sebastiaan stam on UnsplashThe one tool that ML/MLOps engineers underestimate is Terraform.Terraform is an Infrastructure as Code (IaC) tool that lets you define your infrastructure in several declarative files and create, update, or destroy it with just a few CLI commands.Thus, you can easily replicate the same ML infrastructure in multiple environments (production, testing, staging, etc.) without making even one manual click!Also, you will stop worrying at night if you close all your EC2 machines, which will eat up your wallet. You will always use ONLY the resources defined in the Terraform files.Terraform supports douzans of vendors (e.g., AWS, GCP, Azure, Paperspace, etc.).In this tutorial, I will use AWS as an example to teach you the basics of Terraform.This tutorial aims to teach you the main building blocks (aka fundamentals) of Terraform. Enough to get you started and start building \ud83d\udd25Table of ContentsTerraform IntroductionLet\u2019s Build a Production-Ready AWS InfrastructureMaster writing clean & modular Terraform files1. Terraform IntroductionUsing Terraform, you can define a blueprint of your entire infrastructure using Terraform and create or destroy it using just a few commands.No more spending countless hours creating your EC2 instances manually one by one.Here is a simple example of how to create an AWS EC2 instance using Terraform:#1. Install the provider (e.g., AWS, GCP, Azure, etc.) & set up the credentials:terraform {  required_providers {    aws = {      source = \"hashicorp/aws\"      version = \"4.67.0\"    }  }}provider \"aws\" {    region = \"eu-central-1\"    access_key = \"<your_access_key>\"    secret_key = \"<your_secret_key>\"Create an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0in159159FollowWritten by Paul Iusztin1.5K Followers\u00b7Writer for Towards AI\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul Iusztin and Towards AIPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Ignacio de GregorioinTowards AIIs Mamba the End of ChatGPT As We Know It?The Great New Question\u00b78 min read\u00b7Jan 116.2K62IVAN ILINinTowards AIAdvanced RAG Techniques: an Illustrated OverviewA comprehensive study of the advanced retrieval augmented generation techniques and algorithms, systemising various approaches. The article\u202619 min read\u00b7Dec 17, 20234.3K27Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111See all from Paul IusztinSee all from Towards AIRecommended from MediumPaul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111Ankush k SingalinILLUMINATIONCrafting Quality Python Projects: A Unique Template for Success \u2014 MLOPS-IAnkush k Singal\u00b716 min read\u00b7Oct 25, 20231692ListsPredictive Modeling w/ Python20 stories\u00b7891 savesPractical Guides to Machine Learning10 stories\u00b71032 savesNatural Language Processing1179 stories\u00b7656 savesThe New Chatbots: ChatGPT, Bard, and Beyond12 stories\u00b7301 savesKrystian PietrzakWindPRO\u200a\u2014\u200aMLOps project with MLflow, Docker, AWS and Streamlit [Part 4].Using ML to improve wind forecast at a local windsurfing spot.\u00b79 min read\u00b7Dec 3, 202325Greg BiegelBuilding a serverless chatbot with Amazon BedrockThis post shows how easy it is to build a completely serverless chatbot web application with AWS services. Crucially, no data in your\u20264 min read\u00b7Jan 11982Vasil DedejskiServing machine learning models with Ray ServeBuilding intelligent solutions that are driven by data and utilizing machine learning models is one of the most interesting emerging\u20265 min read\u00b7Aug 26, 202312Samuel A OlayinkaUsing Terraform and Fargate to create Amazon\u2019s ECSHello everyone, this article is about creating an Amazon ECS using Terraform and Fargate, Terraform will be used as infrastructure as code\u202610 min read\u00b7Oct 12, 202392See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Express gratitude in a post to thank a mentor or colleague for their support and guidance in your professional journey. Share a specific example or memory of how their mentorship has positively impacted your career.",
        "post": "I'm incredibly grateful to have [mentor's or colleague's name] as a mentor. Their guidance and support have been invaluable in my professional growth. I still remember the time when [specific example or memory]. Thank you for everything! #gratitude #mentorship #careerdevelopment"
    },
    {
        "instruction": "Write a medium article outlining the steps for building and deploying a streaming pipeline to populate a vector database for real-time RAG applications, as discussed in Lesson 3 of an eight-part series. Highlight real-time text embeddings, Docker usage, GitHub Actions CI/CD pipeline, and AWS deployment.",
        "post": "Streaming engine & vector DB for real-time RAG | Decoding MLOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyThe Hands-On LLMs SeriesThis is how you can build & deploy a streaming pipeline to populate a vector DB for real-time RAGLesson 3: Real-time text embeddings. Docker. GitHub Actions CI/CD pipeline. AWSPaul Iusztin\u00b7FollowPublished inDecoding ML\u00b720 min read\u00b7Jan 111861ListenShareImage by DALL-E\u2192 the 3rd out of 8 lessons of the Hands-On LLMs free courseBy finishing the Hands-On LLMs free course, you will learn how to use the 3-pipeline architecture & LLMOps good practices to design, build, and deploy a real-time financial advisor powered by LLMs & vector DBs.We will primarily focus on the engineering & MLOps aspects. Thus, by the end of this series, you will know how to build & deploy a real ML system, not some isolated code in Notebooks (we haven\u2019t used any Notebooks at all).More precisely, these are the 3 components you will learn to build:a real-time streaming pipeline (deployed on AWS) that listens to financial news, cleans & embeds the documents, and loads them to a vector DBa fine-tuning pipeline (deployed as a serverless continuous training) that fine-tunes an LLM on financial data using QLoRA, monitors the experiments using an experiment tracker and saves the best model to a model registryan inference pipeline built in LangChain (deployed as a serverless RESTful API) that loads the fine-tuned LLM from the model registry and answers financial questions using RAG (leveraging the vector DB populated with financial news in real-time)High-level architecture of the Hands-On LLMs FREE course [Image by the Author].We will also show you how to integrate various serverless tools, such as:Comet ML as your ML Platform;Qdrant as your vector DB;Beam as your infrastructure.Curios? Check the video below to understand better what you will learn \u2193Create an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0in1861861FollowWritten by Paul Iusztin1.5K Followers\u00b7Editor for Decoding ML\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul Iusztin and Decoding MLPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Paul IusztininDecoding MLThe ultimate guide on installing PyTorch with CUDA support in all possible ways\u2192 Using Pip, Conda, Poetry, Docker, or directly on the system\u00b710 min read\u00b7Jan 42231Paul IusztininDecoding MLPrepare your RAG LangChain application for productionLesson 8: LLMOps. Serverless infrastructure. RESTful API. Gradio.\u00b719 min read\u00b7Feb 3208Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111See all from Paul IusztinSee all from Decoding MLRecommended from MediumPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Suman DasFine Tune Large Language Model (LLM) on a Custom Dataset with QLoRAThe field of natural language processing has been revolutionized by large language models (LLMs), which showcase advanced capabilities and\u202615 min read\u00b7Jan 253383ListsPredictive Modeling w/ Python20 stories\u00b7891 savesPractical Guides to Machine Learning10 stories\u00b71032 savesNatural Language Processing1179 stories\u00b7656 savesThe New Chatbots: ChatGPT, Bard, and Beyond12 stories\u00b7301 savesWenqi GlantzinTowards Data ScienceDeploying LLM Apps to AWS, the Open-Source Self-Service WayA step-by-step guide on deploying LlamaIndex RAGs to AWS ECS fargate\u00b712 min read\u00b7Jan 86393AndreiinLlamaIndex BlogA Cheat Sheet and Some Recipes For Building Advanced RAGIt\u2019s the start of a new year and perhaps you\u2019re looking to break into the RAG scene by building your very first RAG system. Or, maybe\u20267 min read\u00b7Jan 51.5K5Guodong (Troy) ZhaoinBootcampHow to improve RAG results in your LLM apps: from basics to advancedImprove your RAG quality and latency in your LLM app13 min read\u00b7Jan 228995Senthil EinLevel Up CodingUnlocking LLM\u2019s Potential with RAG: A Complete Guide from Basics to Advanced TechniquesUsing OpenAI, Google Gemini Pro, and Open Source Models47 min read\u00b76 days ago6916See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Explain in a medium article how to build a deep learning workstation by leveraging cryptocurrency mining to finance GPU purchases. Share personal experiences, including setup details, and offer advice on managing GPUs for longevity and efficiency.",
        "post": "How to Build a Deep Learning Workstation for Free | by Paul Iusztin | MediumOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyHow to Build a Deep Learning Workstation for FreePaul Iusztin\u00b7Follow4 min read\u00b7Feb 26, 202252ListenSharePhoto by Christian Wiediger on UnsplashEverybody who is getting into Deep Learning is facing the same issue. It needs a GPU to train their models on them or they will grow old until they will see any output out of their beloved project (of course there are cloud options, but for hobbyists, it is not the same as developing on your own PC). But, unfortunately, GPUs are very expensive. Nowadays, all PC components are expensive, but the GPUs are the ones that empty your pockets. This is mostly a by-product of mining cryptocurrencies. So here is the question. Should you hate the darkness or you should join it?The easiest path would be to complain about how expensive those components are and move on with your life. But I decided to join the darkness and build my own mining computer (rig). My goal was to build a workstation with 4 decent GPUs. Sadly, nothing is free in life, but this approach is almost free if you have some patience. You have to own the cash for the components, but the nice part is that you can put them to work. In 10\u201312 months, depending on what GPUs you buy, you can get your money back. Also, depending on what setup you finally want to own/afford, you can invest more/less cash into this idea. Also, it would be really helpful if you have a decent idea of how to build a PC.Now, you would say that mining is degrading your GPU's being worthless after that period of 10\u201312 months. Yeah, that could be true, but not always true. If you are careful on how you overclock your GPUs you can keep them undervolted and at low temperatures, therefore mining being even less harmful than gaming or machine learning.This is my mining setup. I ended up buying 6 GPUs after all, to speed up my process. I have some mid-range GPUs, but as I said the idea can be scaled. A better GPU will generate more money. I am using HiveOS on my mining rig, a Linux distribution that is very efficient for mining:The GPUs status.On the fourth column, you can see the temperatures. As long those numbers are under 70-celsius degrees there is no possible harm. Mine are around ~50. Far from the degrading point. Also, the fans are running at a speed of around ~40%, which is again far\u2026Create an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0in5252FollowWritten by Paul Iusztin1.5K Followers\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul IusztinPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111Paul IusztininDecoding MLThe ultimate guide on installing PyTorch with CUDA support in all possible ways\u2192 Using Pip, Conda, Poetry, Docker, or directly on the system\u00b710 min read\u00b7Jan 42231Paul IusztininDecoding MLPrepare your RAG LangChain application for productionLesson 8: LLMOps. Serverless infrastructure. RESTful API. Gradio.\u00b719 min read\u00b7Feb 3208See all from Paul IusztinRecommended from MediumPaul IusztininDecoding MLThe ultimate guide on installing PyTorch with CUDA support in all possible ways\u2192 Using Pip, Conda, Poetry, Docker, or directly on the system\u00b710 min read\u00b7Jan 42231Antonis MakropoulosinTowards Data ScienceHow to Build a Multi-GPU System for Deep Learning in 2023This story provides a guide on how to build a multi-GPU system for deep learning and hopefully save you some research time and\u202610 min read\u00b7Sep 17, 202388515ListsPredictive Modeling w/ Python20 stories\u00b7891 savesPractical Guides to Machine Learning10 stories\u00b71032 savesNatural Language Processing1179 stories\u00b7656 savesdata science and AI40 stories\u00b767 savesLakshma Reddy InduriStep-by-Step Guide to Enabling Nvidia GPU for ML/DL Models on a Windows MachineWhy You Care:\u00b77 min read\u00b7Aug 21, 20232Austin StarksinArtificial Intelligence in Plain EnglishReinforcement Learning is Dead. Long Live the Transformer!Large Language Models are more powerful than you imagine8 min read\u00b7Jan 131K26devin schumacherinSERP AICloud GPU Service Providers: The Most Popular Rental Options for AI/ML & Cloud ComputeAccess to powerful graphics processing units (GPUs) is essential for a wide range of applications, from advanced machine learning and\u202614 min read\u00b7Nov 10, 2023800Jacob BennettinLevel Up CodingThe 5 paid subscriptions I actually use in 2024 as a software engineerTools I use that are cheaper than Netflix\u00b75 min read\u00b7Jan 48.2K96See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Discuss the strategic advantage of deploying ML models using a batch architecture initially in a LinkedIn post. Emphasize the approach for quick value delivery and feedback collection, positioning it as a foundation for more complex deployment strategies.",
        "post": "Why serving an ML model using a batch architecture is so powerful?\nWhen you first start deploying your ML model, you want an initial end-to-end flow as fast as possible.\nDoing so lets you quickly provide value, get feedback, and even collect data.\n.\nBut here is the catch...\nSuccessfully serving an ML model is tricky as you need many iterations to optimize your model to work in real-time:\n- low latency\n- high throughput\nInitially, serving your model in batch mode is like a hack.\nBy storing the model's predictions in dedicated storage, you automatically move your model from offline mode to a real-time online model.\nThus, you no longer have to care for your model's latency and throughput. The consumer will directly load the predictions from the given storage.\n\ud835\udc13\ud835\udc21\ud835\udc1e\ud835\udc2c\ud835\udc1e \ud835\udc1a\ud835\udc2b\ud835\udc1e \ud835\udc2d\ud835\udc21\ud835\udc1e \ud835\udc26\ud835\udc1a\ud835\udc22\ud835\udc27 \ud835\udc2c\ud835\udc2d\ud835\udc1e\ud835\udc29\ud835\udc2c \ud835\udc28\ud835\udc1f \ud835\udc1a \ud835\udc1b\ud835\udc1a\ud835\udc2d\ud835\udc1c\ud835\udc21 \ud835\udc1a\ud835\udc2b\ud835\udc1c\ud835\udc21\ud835\udc22\ud835\udc2d\ud835\udc1e\ud835\udc1c\ud835\udc2d\ud835\udc2e\ud835\udc2b\ud835\udc1e:\n- extracts raw data from a real data source\n- clean, validate, and aggregate the raw data within a feature pipeline\n- load the cleaned data into a feature store\n- experiment to find the best model + transformations using the data from the feature store\n- upload the best model from the training pipeline into the model registry\n- inside a batch prediction pipeline, use the best model from the model registry to compute the predictions\n- store the predictions in some storage\n- the consumer will download the predictions from the storage\n- repeat the whole process hourly, daily, weekly, etc. (it depends on your context)\n.\n\ud835\ude1b\ud835\ude29\ud835\ude26 \ud835\ude2e\ud835\ude22\ud835\ude2a\ud835\ude2f \ud835\ude25\ud835\ude30\ud835\ude38\ud835\ude2f\ud835\ude34\ud835\ude2a\ud835\ude25\ud835\ude26 of deploying your model in batch mode is that the predictions will have a level of lag.\nFor example, in a recommender system, if you make your predictions daily, it won't capture a user's behavior in real-time, and it will update the predictions only at the end of the day.\nMoving to other architectures, such as request-response or streaming, will be natural after your system matures in batch mode.\n.\nSo remember, when you initially deploy your model, using a batch mode architecture will be your best shot for a good user experience.\nLet me know in the comments what your usual strategy to serve models is \u2193\nhashtag\n#\ndata\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\n-----\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps."
    },
    {
        "instruction": "Write a quick guide to understanding diffusion models, specifically Stable Diffusion, including training, generating new images, and controlling outputs using given contexts, aimed at busy professionals seeking a concise overview. Structure it for a Medium post",
        "post": "Busy? This Is Your Quick Guide to Opening the Diffusion Models Black Box | by Paul Iusztin | Towards AIOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyBusy? This Is Your Quick Guide to Opening the Diffusion Models Black BoxDecode Stable Diffusion: Train, Generate New Images, & Control Using a Given ContextPaul Iusztin\u00b7FollowPublished inTowards AI\u00b77 min read\u00b7Aug 17, 202310ListenSharePrompt: \u201cAn oil pastel drawing of a funny cat sleeping in a weird position\u201d [Image by the Author \u2014 Generated using DALL-E]If you opened my article, you probably used a text-to-image model from services such as DALL-E, Midjourney, or Stability AI.Well, all of them are based on diffusion models.Even if you want to treat them as a magic black box, having an intuition on how they work under the hood will help you generate better art.This article aims to give you an intuition on how diffusion models generate new images, are trained on a new dataset, and are controlled based on a given context (e.g., text, another image, categories, etc.).We won\u2019t go too much into the technical details but focus conceptually on the main components that make diffusion models work.Table of ContentsGet an Intuition of How Diffusion Models WorkHow Diffusion Models Generate New ImagesA Quick Guide on How to Train Diffusion ModelsControlling Stable Diffusion Models with a Given Context#1. Get an Intuition of How Diffusion Models WorkWe will use a dataset with cats as an example.Thus, let\u2019s say that we want to train a stable diffusion model to generate new cats.Then, to:Generate the dataset \u2014 add Gaussian noiseWe take every image from the dataset and gradually add Gaussian noise to them.Now, we have multiple images containing various noise levels for every initial cat image.Train the model \u2014 remove the noiseThe actual job of the model is to take a noisy image and remove the noise from it.Thus, when training the diffusion model:- it will take a noisy image as input- it will try to remove the noise- the loss is computed between the \u201ccleaned\u201d image & the original unnoisy imageCreate an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0in1010FollowWritten by Paul Iusztin1.5K Followers\u00b7Writer for Towards AI\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul Iusztin and Towards AIPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Ignacio de GregorioinTowards AIIs Mamba the End of ChatGPT As We Know It?The Great New Question\u00b78 min read\u00b7Jan 116.2K62IVAN ILINinTowards AIAdvanced RAG Techniques: an Illustrated OverviewA comprehensive study of the advanced retrieval augmented generation techniques and algorithms, systemising various approaches. The article\u202619 min read\u00b7Dec 17, 20234.3K27Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111See all from Paul IusztinSee all from Towards AIRecommended from MediumMohammad FaizaninMLearning.aiMLOps: Weights & Biases Experiments TrackingMachine Learning projects have hyperparameters, configurations, datasets, and code versions. It is sometimes hard to keep track of what you\u20263 min read\u00b7Aug 16, 20231141Paul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201ListsPredictive Modeling w/ Python20 stories\u00b7891 savesNatural Language Processing1179 stories\u00b7656 savesAI Regulation6 stories\u00b7311 savesPractical Guides to Machine Learning10 stories\u00b71032 savesOleks GorpynichinTowards AIHow does \u201cStable Diffusion\u201d really work? An Intuitive ExplanationThis short article explains how Stable Diffusion works for beginners intuitively. It is a look under the hood of tools like Midjourney.6 min read\u00b7Sep 8, 202373Ankush k SingalinILLUMINATIONCrafting Quality Python Projects: A Unique Template for Success \u2014 MLOPS-IAnkush k Singal\u00b716 min read\u00b7Oct 25, 20231692Rania _HossamLLAMA FROM SCRATCHMeta AI and Microsoft have joined forces to introduce Llama 2, the next generation of Meta\u2019s open-source large language model.13 min read\u00b7Oct 22, 20233222Suman DasFine Tune Large Language Model (LLM) on a Custom Dataset with QLoRAThe field of natural language processing has been revolutionized by large language models (LLMs), which showcase advanced capabilities and\u202615 min read\u00b7Jan 253383See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "Explain the benefits and ease of using serverless technology, particularly Beam, for deploying and managing ML models on GPUs.",
        "post": "\ud835\uddd7\ud835\uddf2\ud835\uddfd\ud835\uddf9\ud835\uddfc\ud835\ude06\ud835\uddf6\ud835\uddfb\ud835\uddf4 & \ud835\uddfa\ud835\uddee\ud835\uddfb\ud835\uddee\ud835\uddf4\ud835\uddf6\ud835\uddfb\ud835\uddf4 ML models is \ud835\uddf5\ud835\uddee\ud835\uddff\ud835\uddf1, especially when running your models on GPUs.\nBut \ud835\ude00\ud835\uddf2\ud835\uddff\ud835\ude03\ud835\uddf2\ud835\uddff\ud835\uddf9\ud835\uddf2\ud835\ude00\ud835\ude00 makes things \ud835\uddf2\ud835\uddee\ud835\ude00\ud835\ude06.\nUsing\nBeam\nas your serverless provider, deploying & managing ML models can be as easy as \u2193\n\ud835\uddd7\ud835\uddf2\ud835\uddf3\ud835\uddf6\ud835\uddfb\ud835\uddf2 \ud835\ude06\ud835\uddfc\ud835\ude02\ud835\uddff \ud835\uddf6\ud835\uddfb\ud835\uddf3\ud835\uddff\ud835\uddee\ud835\ude00\ud835\ude01\ud835\uddff\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2 & \ud835\uddf1\ud835\uddf2\ud835\uddfd\ud835\uddf2\ud835\uddfb\ud835\uddf1\ud835\uddf2\ud835\uddfb\ud835\uddf0\ud835\uddf6\ud835\uddf2\ud835\ude00\nIn a few lines of code, you define the application that contains:\n- the requirements of your infrastructure, such as the CPU, RAM, and GPU\n- the dependencies of your application\n- the volumes from where you can load your data and store your artifacts\n\ud835\uddd7\ud835\uddf2\ud835\uddfd\ud835\uddf9\ud835\uddfc\ud835\ude06 \ud835\ude06\ud835\uddfc\ud835\ude02\ud835\uddff \ud835\uddf7\ud835\uddfc\ud835\uddef\ud835\ude00\nUsing the Beam application, you can quickly decore your Python functions to:\n- run them once on the given serverless application\n- put your task/job in a queue to be processed or even schedule it using a CRON-based syntax\n- even deploy it as a RESTful API endpoint\n.\nAs you can see in the image below, you can have one central function for training or inference, and with minimal effort, you can switch from all these deployment methods.\nAlso, you don't have to bother at all with managing the infrastructure on which your jobs run. You specify what you need, and Beam takes care of the rest.\nBy doing so, you can directly start to focus on your application and stop carrying about the infrastructure.\nThis is the power of serverless!\n\u21b3 Check out\nBeam\nto learn more: \ud83d\udd17\nhttps://lnkd.in/d4-pkCxc\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps."
    },
    {
        "instruction": "Write a brief LinkedIn post introducing resources for starting with MLOps, highlighting a website and a GitHub repository as starting points.",
        "post": "Have you ever wondered where to start learning MLOps?\n.\nYou can find lots of valuable information by checking out the MLOps organization site:\nhttps://ml-ops.org/\nAlso, you can find hundreds of essential references by following this GitHub repository:\nhttps://lnkd.in/dxknAmTK\nNow you have no excuse to start learning MLOps!\nhashtag\n#\nmlops\nhashtag\n#\ncontent\nhashtag\n#\nlearning\n-----\n\ud83d\udca1 My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n."
    },
    {
        "instruction": "Compose teh content for a Medium post in which you guide buy-and-hold investors on automating their investment orders using a Python package. Focus on how to use the package to optimize purchase times and amounts for assets, aiming to buy below the mean price each month.",
        "post": "How to Automate Your Orders as a Buy and Hold Investor | by Paul Iusztin | MLearning.ai | MediumOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyHow to Automate Your Orders as a Buy and Hold InvestorPaul Iusztin\u00b7FollowPublished inMLearning.ai\u00b76 min read\u00b7Jan 27, 202270ListenShareBuy Monthly Under the Mean Price with Yacht.As buy-and-hold investors, we are not that interested in short-market fluctuations. We usually just buy some specific assets periodically. Often though we get to buy assets when they are at the top of the cycle because we just want to do our monthly finances and don\u2019t want to waste time with any technical analysis. This is fine, but we usually lose some % every month, which over long periods would compound nicely increasing our overall returns.This is where Yacht kicks in. Yacht is a Python package specialized in Order Execution. It is not a trading bot. It is built for the buy and hold investors with the thought to boost their performance. Let\u2019s give an example of how Yacht will be used. We want to invest 1000$ monthly into the SPY ETF. Yacht will listen to the trends of the market in that specific month and tell us when is the best moment to buy. It will also tell us what amount would be best to buy within a specific day. So it will sell nothing, it will just buy a given asset over a month with the idea to buy it below the mean price within that specific month. Long story short, the goal of Yacht is to give you a discount on your favorite assets.Note: The amount of money and the period of one month are just examples. Those can be adjusted to your own needs.This will be a tutorial on how to begin using Yacht. You can see the code on GitHub.Short IntroductionYacht is a Python package. It is a Deep Reinforcement Learning framework for the Financial Market. It is modular and flexible so it is easily adjustable to our own needs. It has environments for Trading and Order Execution. It supports data downloading, processing & caching. Also, it has cool features like Wandb integration and renderers to see how your agent is performing. It supports lots of metrics. An app layer can easily be added to build your ideas on top of it. But it is only in the early stages of its development. Therefore, our main focus was on Order Execution.I am the main maintainer of the codebase. If you are interested in participating in it don\u2019t hesitate to write me at p.e.iusztin@gmail or my LinkedIn account.Create an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0in7070FollowWritten by Paul Iusztin1.5K Followers\u00b7Writer for MLearning.ai\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul Iusztin and MLearning.aiPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Fabio MatricardiinMLearning.aiHow I Built a Chatbot that Crushed ChatGPT with Zero Cost AI ToolsChallenge Accepted! How I created a chatbot that surpassed the performance of the famous ChatGPT model using free and open source AI tools\u2026\u00b79 min read\u00b76 days ago1.4K5Maximilian VogelinMLearning.aiThe ChatGPT list of lists: A collection of 3000+ prompts, GPTs, use-cases, tools, APIs, extensions\u2026Updated Jan-11, 2024. Added GPT Store, new resources.11 min read\u00b7Feb 7, 202311.5K141Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111See all from Paul IusztinSee all from MLearning.aiRecommended from MediumPaul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111Moez AliEasily deploy machine learning models from the comfort of your NotebookStreamline your Machine Learning model deployment with Modelbit.com. Seamlessly move from Jupyter notebooks to production-ready REST APIs\u20269 min read\u00b7Sep 8, 20232621ListsPredictive Modeling w/ Python20 stories\u00b7891 savesPractical Guides to Machine Learning10 stories\u00b71032 savesNatural Language Processing1179 stories\u00b7656 savesdata science and AI40 stories\u00b767 savesMohammad FaizaninMLearning.aiMLOps: Weights & Biases Experiments TrackingMachine Learning projects have hyperparameters, configurations, datasets, and code versions. It is sometimes hard to keep track of what you\u20263 min read\u00b7Aug 16, 20231141Nikhil AdithyaninLevel Up CodingStock Market Sentiment Prediction with OpenAI and PythonAn interesting exploration of the power of LLMs in stock analysis11 min read\u00b76 days ago4621Vasil DedejskiServing machine learning models with Ray ServeBuilding intelligent solutions that are driven by data and utilizing machine learning models is one of the most interesting emerging\u20265 min read\u00b7Aug 26, 202312Austin StarksinArtificial Intelligence in Plain EnglishReinforcement Learning is Dead. Long Live the Transformer!Large Language Models are more powerful than you imagine8 min read\u00b7Jan 131K26See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    },
    {
        "instruction": "On LinkedIn, outline methods for evaluating LLM outputs, focusing on structured answers, using LLM as a classifier, and generating multiple answers for selection. Stress the importance of automating this process for efficiency.",
        "post": "Here are 3 techniques you must know to evaluate your LLMs quickly.\nManually testing the output of your LLMs is a tedious and painful process \u2192 you need to automate it.\nIn generative AI, most of the time, you cannot leverage standard metrics.\nThus, the real question is, how do you evaluate the outputs of an LLM?\nDepending on your problem, here is what you can do \u2193\n#\ud835\udfed. \ud835\udde6\ud835\ude01\ud835\uddff\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2\ud835\uddf1 \ud835\uddee\ud835\uddfb\ud835\ude00\ud835\ude04\ud835\uddf2\ud835\uddff\ud835\ude00 - \ud835\ude06\ud835\uddfc\ud835\ude02 \ud835\uddf8\ud835\uddfb\ud835\uddfc\ud835\ude04 \ud835\uddf2\ud835\ude05\ud835\uddee\ud835\uddf0\ud835\ude01\ud835\uddf9\ud835\ude06 \ud835\ude04\ud835\uddf5\ud835\uddee\ud835\ude01 \ud835\ude06\ud835\uddfc\ud835\ude02 \ud835\ude04\ud835\uddee\ud835\uddfb\ud835\ude01 \ud835\ude01\ud835\uddfc \ud835\uddf4\ud835\uddf2\ud835\ude01\nEven if you use an LLM to generate text, you can ask it to generate a response in a structured format (e.g., JSON) that can be parsed.\nYou know exactly what you want (e.g., a list of products extracted from the user's question).\nThus, you can easily compare the generated and ideal answers using classic approaches.\nFor example, when extracting the list of products from the user's input, you can do the following:\n- check if the LLM outputs a valid JSON structure\n- use a classic method to compare the generated and real answers\n#\ud835\udfee. \ud835\udde1\ud835\uddfc \"\ud835\uddff\ud835\uddf6\ud835\uddf4\ud835\uddf5\ud835\ude01\" \ud835\uddee\ud835\uddfb\ud835\ude00\ud835\ude04\ud835\uddf2\ud835\uddff (\ud835\uddf2.\ud835\uddf4., \ud835\uddf4\ud835\uddf2\ud835\uddfb\ud835\uddf2\ud835\uddff\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddf1\ud835\uddf2\ud835\ude00\ud835\uddf0\ud835\uddff\ud835\uddf6\ud835\uddfd\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb\ud835\ude00, \ud835\ude00\ud835\ude02\ud835\uddfa\ud835\uddfa\ud835\uddee\ud835\uddff\ud835\uddf6\ud835\uddf2\ud835\ude00, \ud835\uddf2\ud835\ude01\ud835\uddf0.)\nWhen generating sentences, the LLM can use different styles, words, etc. Thus, traditional metrics (e.g., BLUE score) are too rigid to be useful.\nYou can leverage another LLM to test the output of our initial LLM. The trick is in what questions to ask.\nWhen testing LLMs, you won't have a big testing split size as you are used to. A set of 10-100 tricky examples usually do the job (it won't be costly).\nHere, we have another 2 sub scenarios:\n\u21b3 \ud835\udfee.\ud835\udfed \ud835\uddea\ud835\uddf5\ud835\uddf2\ud835\uddfb \ud835\ude06\ud835\uddfc\ud835\ude02 \ud835\uddf1\ud835\uddfc\ud835\uddfb'\ud835\ude01 \ud835\uddf5\ud835\uddee\ud835\ude03\ud835\uddf2 \ud835\uddee\ud835\uddfb \ud835\uddf6\ud835\uddf1\ud835\uddf2\ud835\uddee\ud835\uddf9 \ud835\uddee\ud835\uddfb\ud835\ude00\ud835\ude04\ud835\uddf2\ud835\uddff \ud835\ude01\ud835\uddfc \ud835\uddf0\ud835\uddfc\ud835\uddfa\ud835\uddfd\ud835\uddee\ud835\uddff\ud835\uddf2 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddee\ud835\uddfb\ud835\ude00\ud835\ude04\ud835\uddf2\ud835\uddff \ud835\ude01\ud835\uddfc (\ud835\ude06\ud835\uddfc\ud835\ude02 \ud835\uddf1\ud835\uddfc\ud835\uddfb'\ud835\ude01 \ud835\uddf5\ud835\uddee\ud835\ude03\ud835\uddf2 \ud835\uddf4\ud835\uddff\ud835\uddfc\ud835\ude02\ud835\uddfb\ud835\uddf1 \ud835\ude01\ud835\uddff\ud835\ude02\ud835\ude01\ud835\uddf5)\nYou don't have access to an expert to write an ideal answer for a given question to compare it to.\nBased on the initial prompt and generated answer, you can compile a set of questions and pass them to an LLM. Usually, these are Y/N questions that you can easily quantify and check the validity of the generated answer.\nThis is known as \"Rubric Evaluation\"\nFor example:\n\"\"\"\n- Is there any disagreement between the response and the context? (Y or N)\n- Count how many questions the user asked. (output a number)\n...\n\"\"\"\nThis strategy is intuitive, as you can ask the LLM any question you are interested in as long it can output a quantifiable answer (Y/N or a number).\n\u21b3 \ud835\udfee.\ud835\udfee. \ud835\uddea\ud835\uddf5\ud835\uddf2\ud835\uddfb \ud835\ude06\ud835\uddfc\ud835\ude02 \ud835\uddf1\ud835\uddfc \ud835\uddf5\ud835\uddee\ud835\ude03\ud835\uddf2 \ud835\uddee\ud835\uddfb \ud835\uddf6\ud835\uddf1\ud835\uddf2\ud835\uddee\ud835\uddf9 \ud835\uddee\ud835\uddfb\ud835\ude00\ud835\ude04\ud835\uddf2\ud835\uddff \ud835\ude01\ud835\uddfc \ud835\uddf0\ud835\uddfc\ud835\uddfa\ud835\uddfd\ud835\uddee\ud835\uddff\ud835\uddf2 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddff\ud835\uddf2\ud835\ude00\ud835\uddfd\ud835\uddfc\ud835\uddfb\ud835\ude00\ud835\uddf2 \ud835\ude01\ud835\uddfc (\ud835\ude06\ud835\uddfc\ud835\ude02 \ud835\uddf5\ud835\uddee\ud835\ude03\ud835\uddf2 \ud835\uddf4\ud835\uddff\ud835\uddfc\ud835\ude02\ud835\uddfb\ud835\uddf1 \ud835\ude01\ud835\uddff\ud835\ude02\ud835\ude01\ud835\uddf5)\nWhen you have access to an answer manually created by a group of experts, things are easier.\nYou will use an LLM to compare the generated and ideal answers based on semantics, not structure.\nFor example:\n\"\"\"\n(A) The submitted answer is a subset of the expert answer and entirely consistent.\n...\n(E) The answers differ, but these differences don't matter.\n\"\"\"\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning"
    },
    {
        "instruction": "Introduce the Composite design pattern on LinkedIn as essential for ML engineers, particularly for structuring efficient ML pipelines. Explain its benefits in simplifying complex projects by combining processing steps.",
        "post": "This is my \ud835\uddf3\ud835\uddee\ud835\ude03\ud835\uddfc\ud835\uddff\ud835\uddf6\ud835\ude01\ud835\uddf2 \ud835\uddf1\ud835\uddf2\ud835\ude00\ud835\uddf6\ud835\uddf4\ud835\uddfb \ud835\uddfd\ud835\uddee\ud835\ude01\ud835\ude01\ud835\uddf2\ud835\uddff\ud835\uddfb that you must know as an ML engineer.\nMost ML engineers completely ignore software design patterns, but let me explain why you should know this one for your machine learning projects \u2193\nI am talking about Composite.\nThe Composite pattern is a structural design pattern that helps you compose objects in a tree-like structure.\nLet me explain by starting with the problem.\n\ud835\udde3\ud835\uddff\ud835\uddfc\ud835\uddef\ud835\uddf9\ud835\uddf2\ud835\uddfa\nLet's say that you want to build an ML pipeline that performs object detection + tracking.\nYou can easily divide it into smaller pipelines, such as:\n1. preprocessing\n2. training | inference\n3. postprocessing\nAlso, these 3 pipelines, in their turn, are split into smaller components.\nLet's say that to speed up the ML pipeline. You want to run everything in parallel if possible.\nThus, depending on the use case, it would be best to have a module to compose components sequentially or in parallel.\n\u274c If you don't think this through, your code can quickly transform into spaghetti.\n\ud835\udde6\ud835\uddfc\ud835\uddf9\ud835\ude02\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb\n\u2705 Now, the Composite design pattern kicks in.\n-> \ud835\ude1b\ud835\ude29\ud835\ude2a\ud835\ude34 \ud835\ude2a\ud835\ude34 \ud835\ude29\ud835\ude30\ud835\ude38 \ud835\ude3a\ud835\ude30\ud835\ude36 \ud835\ude24\ud835\ude22\ud835\ude2f \ud835\ude2a\ud835\ude2e\ud835\ude31\ud835\ude2d\ud835\ude26\ud835\ude2e\ud835\ude26\ud835\ude2f\ud835\ude35 \ud835\ude35\ud835\ude29\ud835\ude26 \ud835\ude14\ud835\ude13 \ud835\ude31\ud835\ude2a\ud835\ude31\ud835\ude26\ud835\ude2d\ud835\ude2a\ud835\ude2f\ud835\ude26 \ud835\ude22\ud835\ude23\ud835\ude30\ud835\ude37\ud835\ude26 \ud835\ude36\ud835\ude34\ud835\ude2a\ud835\ude2f\ud835\ude28 \ud835\ude35\ud835\ude29\ud835\ude26 \ud835\ude0a\ud835\ude30\ud835\ude2e\ud835\ude31\ud835\ude30\ud835\ude34\ud835\ude2a\ud835\ude35\ud835\ude26 \ud835\ude31\ud835\ude22\ud835\ude35\ud835\ude35\ud835\ude26\ud835\ude33\ud835\ude2f:\n1. Define a standard interface for all the transformations. Let's call it \"Transformation.\"\n2. We create an abstract class called \"AtomicTransformation\" that inherits the \"Transformation\" interface for an atomic transformation.\n3. We implement an abstract class called \"CompositeTransformation\" for running multiple transformations. This class inherits the \"Transformation\" interface but also inputs a list of \"Transformation\" objects as input.\n4. Depending on how you want to call a sequence of transformations, you can inherit the \"CompositeTransformation\" interface and implement classes for:\n- \"SequenceTransformations\"\n- \"ParallelTransformations,\"\n- \"DistributedTransformations,\" etc.\n5. Now, when you want to implement a granular transformation (e.g., normalize the image). You implement the \"AtomicTransformation\" interface.\n6. When you want to glue multiple transformations together, you leverage the \"CompositeTransformation\" classes.\n7. When you call a \"CompositeTransformation\" under the hood, it calls the list of \"Transformation\" objects until it hits an \"AtomicTransformation\" object which will do the actual transformation.\nNote that because both the \"AtomicTransformation\" and \"CompositeTransformation\" inherit the \"Transformation\" interface, you can use them interchangeably, like LEGOs.\nThat is powerful.\nThat is why we all love Sklearn and their \"Pipeline\" interface \ud83d\udd25\n.\nIf you want to know how to apply other software design patterns in MLE, I left in the comments an interesting article \u2193\nhashtag\n#\nmachinelearning\nhashtag\n#\ndesignpattern\nhashtag\n#\nmlops\n-----\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps."
    },
    {
        "instruction": "Craft a LinkedIn post sharing how to efficiently use GitHub Actions and Docker to create CI/CD pipelines for ML/MLOps projects. Highlight the ease of setup and the benefits of automation and scalability offered by these tools.",
        "post": "Writing your own ML models is history.\nThe true value is in your data, how you prepare it, and your computer power.\nTo demonstrate my statement. Here is how you can write a Python script to train your LLM at scale in under 5 minutes \u2193\n#\ud835\udfed. Load your data in JSON format and convert it into a Hugging Dataset\n#\ud835\udfee. Use Huggingface to load the LLM and pass it to the SFTTrainer, along with the tokenizer and training & evaluation datasets.\n#\ud835\udfef. Wrap your training script with a serverless solution, such as Beam, which quickly lets you access a cluster of GPUs to train large models.\n\ud83d\udea8 As you can see, the secret ingredients are not the LLM but:\n- the amount of data\n- the quality of data\n- how you process the data\n- $$$ for compute power\n- the ability to scale the system\n.\n\ud83d\udca1 My advice\n\u21b3 If you don't plan to become an ML researcher, shift your focus from the latest models to your data and infrastructure.\n.\n\ud835\udde1\ud835\uddfc\ud835\ude01\ud835\uddf2: Integrating serverless services, such as Beam, makes the deployment of your training pipeline fast & seamless, leaving you to focus only on the last piece of the puzzle: your data.\n\u21b3\ud83d\udd17 Check out Beam's docs to find out more:\nhttps://lnkd.in/dtu2MWSp\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning\n-----\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps."
    },
    {
        "instruction": "Create a guide for Medium on installing PyTorch with CUDA support using various methods including Pip, Conda, Poetry, Docker, or directly on the system. Emphasize troubleshooting tips and practical advice for a smooth setup process.",
        "post": "The go-to guide on installing PyTorch & CUDA | Decoding MLOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyThe ultimate guide on installing PyTorch with CUDA support in all possible ways\u2192 Using Pip, Conda, Poetry, Docker, or directly on the systemPaul Iusztin\u00b7FollowPublished inDecoding ML\u00b710 min read\u00b7Jan 42231ListenShareImage by DALL-EWe all know that one of the most annoying things in Deep Learning is installing PyTorch with CUDA support.Nowadays, installing PyTorch & CUDA using pip or conda is relatively easy. Unfortunately, many production projects require you to use Poetry or Docker. That is where things get more complicated.That is why I am writing this article as a practical living document showing how to install these 2 beasts in all possible ways.This tutorial is a living document that I plan to use to install PyTorch & CUDA myself. Thus, I will update this doc whenever I test something I like. Also, in the comments section, feel free to add any other methods you use to install torch & CUDA or troubleshoot potential issues. Let\u2019s create the go-to document that makes installing PyTorch & CUDA a piece of cake!Important observation: I am mainly using Ubuntu. Thus, I will use concrete examples based on it. But this article can easily be extrapolated to other operating systems.Another important observation: I have used Python 3.10, torch 2.0.1 and CUDA 11.8 in most examples. Feel free to change it with your required versions. You can find them on PyTorch's main page.Table of ContentsUseful ConceptsSystemPipConda (or Mamba)PoetryDockerTest out the installationTroubleshooting#1. Useful ConceptsCUDACUDA (Compute Unified Device Architecture) is a parallel computing platform and application programming interface (API) created by NVIDIA. It allows developers to use a CUDA-enabled graphics processing unit (GPU) for general-purpose processing.The CUDA Toolkit includes libraries, debugging and optimization tools, a compiler, and a runtime library to deploy your applications on GPU-accelerated platforms.Create an account to read\u00a0the\u00a0full\u00a0story.The\u00a0author made this story available to\u00a0Medium\u00a0members\u00a0only.If you\u2019re new to Medium, create a new account to read this story on us.Continue in appOr, continue in mobile webSign up with GoogleSign up with FacebookSign up with emailAlready have an account? Sign\u00a0in2232231FollowWritten by Paul Iusztin1.5K Followers\u00b7Editor for Decoding ML\ud83e\udd16 Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Socials & ML/MLOps Hub: https://www.pauliusztin.me/FollowMore from Paul Iusztin and Decoding MLPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streaming\u2026Lesson 1: LLM architecture system design using the 3-pipeline pattern\u00b712 min read\u00b7Jan 5201Paul IusztininDecoding MLPrepare your RAG LangChain application for productionLesson 8: LLMOps. Serverless infrastructure. RESTful API. Gradio.\u00b719 min read\u00b7Feb 3208Paul IusztininDecoding MLThis is how you can build & deploy a streaming pipeline to populate a vector DB for real-time RAGLesson 3: Real-time text embeddings. Docker. GitHub Actions CI/CD pipeline. AWS\u00b720 min read\u00b7Jan 111861Paul IusztininTowards Data ScienceA Framework for Building a Production-Ready Feature Engineering PipelineLesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.\u00b713 min read\u00b7Apr 28, 202376111See all from Paul IusztinSee all from Decoding MLRecommended from MediumAnnaTensorFlow \ud83e\udd1dConda\ud83e\udd1dNVIDIA GPU on Ubuntu 22.04.3 LTS\ud83c\udfb5\u201dMake friends, make friends, never, never break friends\u2026\u201d\ud83c\udfb511 min read\u00b7Dec 26, 2023131Redmen IshabComputer Vision AI With Yolo-v8Object detection using Yolo v85 min read\u00b7Jan 2420ListsOur favorite writing prompts and inspiration15 stories\u00b758 savesCoding & Development11 stories\u00b7442 savesMedium's Huge List of Publications Accepting Submissions262 stories\u00b71838 savesStaff Picks577 stories\u00b7738 savesLearnbay_OfficialPytorch vs. Tensorflow: Major Difference Among Deep LearningComprehend Significant Differences Between Pytorch and Tensorflow for Deep Learning12 min read\u00b7Nov 21, 202363Chaim RandinTowards Data SciencePyTorch Model Performance Analysis and OptimizationHow to Use PyTorch Profiler and TensorBoard to Accelerate Training and Reduce Cost14 min read\u00b7Jun 12, 20232044Austin StarksinArtificial Intelligence in Plain EnglishReinforcement Learning is Dead. Long Live the Transformer!Large Language Models are more powerful than you imagine8 min read\u00b7Jan 131K26Reza KalantarinPython in Plain EnglishHow to Freeze Model Weights in PyTorch for Transfer Learning: Step-by-Step TutorialTransfer learning is a machine learning technique where a pre-trained model is adapted for a new, but similar problem. One of the key steps\u20263 min read\u00b7Sep 4, 202323See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."
    }
]