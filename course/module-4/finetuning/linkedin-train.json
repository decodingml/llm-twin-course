[
  {
    "instruction": "I need a LinkedIn Post of about 250-300 words about learning how to become an engineer for ML Systems in which you promote decodingml. Write it structured using lists.",
    "content": "Want to ğ—¹ğ—²ğ—®ğ—¿ğ—» to ğ—²ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ ğ—½ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜ğ—¶ğ—¼ğ—»-ğ—¿ğ—²ğ—®ğ—±ğ˜† ğ— ğ—Ÿ ğ˜€ğ˜†ğ˜€ğ˜ğ—²ğ—ºğ˜€ but don't know where to start?\n\nA lot of content on ML is in a Jupyter Notebook and revolves around building excellent models using some static datasets.\n\nThat is great.\n\nğ˜‰ğ˜¶ğ˜µ ğ˜µğ˜©ğ˜¢ğ˜µ ğ˜¸ğ˜°ğ˜¯'ğ˜µ ğ˜©ğ˜¦ğ˜­ğ˜± ğ˜ºğ˜°ğ˜¶ ğ˜£ğ˜¶ğ˜ªğ˜­ğ˜¥ ğ˜³ğ˜¦ğ˜¢ğ˜­-ğ˜¸ğ˜°ğ˜³ğ˜­ğ˜¥ ğ˜”ğ˜“ ğ˜´ğ˜ºğ˜´ğ˜µğ˜¦ğ˜®ğ˜´ ğ˜µğ˜©ğ˜¢ğ˜µ ğ˜¸ğ˜ªğ˜­ğ˜­ ğ˜±ğ˜³ğ˜°ğ˜·ğ˜ªğ˜¥ğ˜¦ ğ˜³ğ˜¦ğ˜¢ğ˜­ ğ˜·ğ˜¢ğ˜­ğ˜¶ğ˜¦ ğ˜¢ğ˜µ ğ˜ºğ˜°ğ˜¶ğ˜³ ğ˜«ğ˜°ğ˜£.\n\nAs Alex Vesa said in his previous post: \"ML engineering is 10% machine learning and 90% engineering.\"\n\nğ—§ğ—µğ—² ğ˜€ğ—¼ğ—¹ğ˜‚ğ˜ğ—¶ğ—¼ğ—»?\n\nğ˜‹ğ˜¦ğ˜¤ğ˜°ğ˜¥ğ˜ªğ˜¯ğ˜¨ ğ˜”ğ˜“\n\n.\n\nAs Alex Vesa and Alexandru Razvant started to write alongside me at Decoding ML, we grew the topics we will talk about in Decoding ML.\n\nThe ğ—½ğ—¿ğ—¶ğ—ºğ—®ğ—¿ğ˜† ğ—»ğ—¶ğ—°ğ—µğ—² is the same: \"Battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems.\"\n\nBut with their 10+ years of experience in AI, we started creating content on:\n\n- LLM, RAG, prompt engineering, LLM evaluation\n- Computer vision, 3D object detection & segmentation, tracking\n\nBut with a ğ—³ğ—¼ğ—°ğ˜‚ğ˜€ on ğ—µğ—¼ğ˜„ to ğ—²ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ the ğ˜€ğ˜†ğ˜€ğ˜ğ—²ğ—º ğ—®ğ—¿ğ—¼ğ˜‚ğ—»ğ—± the ğ—ºğ—¼ğ—±ğ—²ğ—¹ğ˜€ by using ğ— ğ—Ÿğ—¢ğ—½ğ˜€ and ğ—²ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ğ—¶ğ—»ğ—´ ğ—´ğ—¼ğ—¼ğ—± ğ—½ğ—¿ğ—®ğ—°ğ˜ğ—¶ğ—°ğ—²ğ˜€:\n\n- Python\n- ML system design\n- SageMaker\n- TensortRT, ONNX\n- Docker, CI/CD, version control\n- experiment trackers, model registries\n- feature stories, data lineage, vector DBs\n- orchestration tools\n- deployment & autoscaling strategies\n- model training & serving \n- monitoring & observability\n\n...and the best part is that we will do our best to ğ—½ğ—¿ğ—¼ğ˜ƒğ—¶ğ—±ğ—² the ğ—°ğ—¼ğ—±ğ—² for all our articles.\n\nSo...\n\nğ˜ğ˜¢ğ˜¯ğ˜µ ğ˜µğ˜° ğ˜­ğ˜¦ğ˜¢ğ˜³ğ˜¯ ğ˜µğ˜° ğ˜£ğ˜¶ğ˜ªğ˜­ğ˜¥ ğ˜±ğ˜³ğ˜°ğ˜¥ğ˜¶ğ˜¤ğ˜µğ˜ªğ˜°ğ˜¯-ğ˜¨ğ˜³ğ˜¢ğ˜¥ğ˜¦ ğ˜”ğ˜“ & ğ˜”ğ˜“ğ˜–ğ˜±ğ˜´ ğ˜´ğ˜ºğ˜´ğ˜µğ˜¦ğ˜®ğ˜´?\n\nğ—¦ğ˜‚ğ—¯ğ˜ƒğ—°ğ—¿ğ—¶ğ—¯ğ—² ğ˜ğ—¼ ğ˜ğ—µğ—² ğ——ğ—²ğ—°ğ—¼ğ—±ğ—¶ğ—»ğ—´ ğ— ğ—Ÿ ğ—¡ğ—²ğ˜„ğ˜€ğ—¹ğ—²ğ˜ğ˜ğ—²ğ—¿. ğ—™ğ—¼ğ—¿ ğ—™ğ—¥ğ—˜ğ—˜ â†“\n\nğŸ”— Decoding ML Newsletter: https://lnkd.in/dQpjSXKc\n\nhashtag\n#machinelearning \nhashtag\n#mlops \nhashtag\n#datascience"
  },
  {
    "instruction": "Please write me a LinkedIn Post of about 400 words about fine tuning open source llms: and about fine-tuning vs prompt engineering. Highlight the need to fine tune, why do we need it and the difference between it and prompt engineering.",
    "content": "ğ—ªğ—µğ˜† & ğ˜„ğ—µğ—²ğ—» do you need to ğ—³ğ—¶ğ—»ğ—²-ğ˜ğ˜‚ğ—»ğ—² ğ—¼ğ—½ğ—²ğ—»-ğ˜€ğ—¼ğ˜‚ğ—¿ğ—°ğ—² ğ—Ÿğ—Ÿğ— ğ˜€? What about ğ—³ğ—¶ğ—»ğ—²-ğ˜ğ˜‚ğ—»ğ—¶ğ—»ğ—´ ğ˜ƒğ˜€. ğ—½ğ—¿ğ—¼ğ—ºğ—½ğ˜ ğ—²ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ğ—¶ğ—»ğ—´? \n \nFine-tuning is the process of taking a pre-trained model and further refining it on a specific task. \n \nğ—™ğ—¶ğ—¿ğ˜€ğ˜, ğ—¹ğ—²ğ˜'ğ˜€ ğ—°ğ—¹ğ—®ğ—¿ğ—¶ğ—³ğ˜† ğ˜„ğ—µğ—®ğ˜ ğ—ºğ—²ğ˜ğ—µğ—¼ğ—±ğ˜€ ğ—¼ğ—³ ğ—³ğ—¶ğ—»ğ—²-ğ˜ğ˜‚ğ—»ğ—¶ğ—»ğ—´ ğ—®ğ—» ğ—¼ğ—½ğ—²ğ—»-ğ˜€ğ—¼ğ˜‚ğ—¿ğ—°ğ—² ğ—Ÿğ—Ÿğ—  ğ—²ğ˜…ğ—¶ğ˜€t â†“ \n \n- ğ˜Šğ˜°ğ˜¯ğ˜µğ˜ªğ˜¯ğ˜¶ğ˜¦ğ˜¥ ğ˜±ğ˜³ğ˜¦-ğ˜µğ˜³ğ˜¢ğ˜ªğ˜¯ğ˜ªğ˜¯ğ˜¨: utilize domain-specific data to apply the same pre-training process (next token prediction) on the pre-trained (base) model \n- ğ˜ğ˜¯ğ˜´ğ˜µğ˜³ğ˜¶ğ˜¤ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜§ğ˜ªğ˜¯ğ˜¦-ğ˜µğ˜¶ğ˜¯ğ˜ªğ˜¯ğ˜¨: the pre-trained (base) model is fine-tuned on a Q&A dataset to learn to answer questions \n- ğ˜šğ˜ªğ˜¯ğ˜¨ğ˜­ğ˜¦-ğ˜µğ˜¢ğ˜´ğ˜¬ ğ˜§ğ˜ªğ˜¯ğ˜¦-ğ˜µğ˜¶ğ˜¯ğ˜ªğ˜¯ğ˜¨: the pre-trained model is refined for a specific task, such as toxicity detection, coding, medicine advice, etc. \n- ğ˜™ğ˜“ğ˜ğ˜: It requires collecting human preferences (e.g., pairwise comparisons), which are then used to train a reward model. The reward model is used to fine-tune the LLM via RL techniques such as PPO. \n \nCommon approaches are to take a pre-trained LLM (next-word prediction) and apply instruction & single-task fine-tuning. \n \nğ—ªğ—µğ˜† ğ—±ğ—¼ ğ˜†ğ—¼ğ˜‚ ğ—»ğ—²ğ—²ğ—± ğ˜ğ—¼ ğ—³ğ—¶ğ—»ğ—²-ğ˜ğ˜‚ğ—»ğ—² ğ˜ğ—µğ—² ğ—Ÿğ—Ÿğ— ? \n \nYou do instruction fine-tuning to make the LLM learn to answer your questions. \n \nThe exciting part is when you want to fine-tune your LLM on a single task. \n \nHere is why â†“ \n \nğ˜±ğ˜¦ğ˜³ğ˜§ğ˜°ğ˜³ğ˜®ğ˜¢ğ˜¯ğ˜¤ğ˜¦: it will improve your LLM performance on given use cases (e.g., coding, extracting text, etc.). Mainly, the LLM will specialize in a given task (a specialist will always beat a generalist in its domain) \n \nğ˜¤ğ˜°ğ˜¯ğ˜µğ˜³ğ˜°ğ˜­: you can refine how your model should behave on specific inputs and outputs, resulting in a more robust product \n \nğ˜®ğ˜°ğ˜¥ğ˜¶ğ˜­ğ˜¢ğ˜³ğ˜ªğ˜»ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯: you can create an army of smaller models, where each is specialized on a particular task, increasing the overall system's performance. Usually, when you fine-tune one task, it reduces the performance of the other tasks (known as the alignment tax). Thus, having an expert system of multiple smaller models can improve the overall performance. \n \nğ—ªğ—µğ—®ğ˜ ğ—®ğ—¯ğ—¼ğ˜‚ğ˜ ğ—½ğ—¿ğ—¼ğ—ºğ—½ğ˜ ğ—²ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ğ—¶ğ—»ğ—´ ğ˜ƒğ˜€ ğ—³ğ—¶ğ—»ğ—²-ğ˜ğ˜‚ğ—»ğ—¶ğ—»ğ—´? \n \nğ˜¥ğ˜¢ğ˜µğ˜¢: use prompting when you don't have data available (~2 examples are enough). Fine-tuning needs at least >=100 examples to work. \n \nğ˜¤ğ˜°ğ˜´ğ˜µ: prompting forces you to write long & detailed prompts to achieve your level of performance. You pay per token (API or compute-wise). Thus, when a prompt gets bigger, your costs increase. But, when fine-tuning an LLM, you incorporate all that knowledge inside the model. Hence, you can use smaller prompts with similar performance. \n \nhashtag\n#machinelearning \nhashtag\n#mlops \nhashtag\n#datascience"
  },
  {
    "instruction": "Write me a LinkedIn Post of about 250 - 300 words about  learning MLE and mMLOps. In here you are trying to promovate your MLOps course showcasing its pros.",
    "content": "Want to learn ğ— ğ—Ÿğ—˜ & ğ— ğ—Ÿğ—¢ğ—½ğ˜€ in a ğ˜€ğ˜ğ—¿ğ˜‚ğ—°ğ˜ğ˜‚ğ—¿ğ—²ğ—± ğ˜„ğ—®ğ˜†, for ğ—³ğ—¿ğ—²ğ—², and with ğ—µğ—®ğ—»ğ—±ğ˜€-ğ—¼ğ—» ğ—²ğ˜…ğ—®ğ—ºğ—½ğ—¹ğ—²ğ˜€? \n \nThen you should check out my ğ—§ğ—µğ—² ğ—™ğ˜‚ğ—¹ğ—¹ ğ—¦ğ˜ğ—®ğ—°ğ—¸ ğŸ³-ğ—¦ğ˜ğ—²ğ—½ğ˜€ ğ— ğ—Ÿğ—¢ğ—½ğ˜€ ğ—™ğ—¿ğ—®ğ—ºğ—²ğ˜„ğ—¼ğ—¿ğ—¸ FREE course. \n \n. \n \nIn ğŸ®.ğŸ± ğ—µğ—¼ğ˜‚ğ—¿ğ˜€ ğ—¼ğ—³ ğ—¿ğ—²ğ—®ğ—±ğ—¶ğ—»ğ—´ & ğ˜ƒğ—¶ğ—±ğ—²ğ—¼ ğ—ºğ—®ğ˜ğ—²ğ—¿ğ—¶ğ—®ğ—¹ğ˜€, you will ğ—¹ğ—²ğ—®ğ—¿ğ—» ğ—µğ—¼ğ˜„ ğ˜ğ—¼: \n \n- design a batch-serving architecture \n- use Hopsworks as a feature store \n- design a feature engineering pipeline that reads data from an API \n- build a training pipeline with hyper-parameter tunning \n- use W&B as an ML Platform to track your experiments, models, and metadata \n- implement a batch prediction pipeline \n- use Poetry to build your own Python packages \n- deploy your own private PyPi server \n- orchestrate everything with Airflow \n- use the predictions to code a web app using FastAPI and Streamlit \n- use Docker to containerize your code \n- use Great Expectations to ensure data validation and integrity \n- monitor the performance of the predictions over time \n- deploy everything to GCP \n- build a CI/CD pipeline using GitHub Actions \n- trade-offs & future improvements discussion \n \n...where all the pieces are integrated into a single end-to-end ML system that forecasts hourly energy levels across Denmark. \n \nğ—¬ğ—¼ğ˜‚ ğ—°ğ—®ğ—» ğ—®ğ—°ğ—°ğ—²ğ˜€ğ˜€ ğ˜ğ—µğ—² ğ—°ğ—¼ğ˜‚ğ—¿ğ˜€ğ—² ğ—¼ğ—»: \nâ ğ˜”ğ˜¦ğ˜¥ğ˜ªğ˜¶ğ˜®'ğ˜´ ğ˜›ğ˜‹ğ˜š ğ˜±ğ˜¶ğ˜£ğ˜­ğ˜ªğ˜¤ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯: text tutorials + videos \nâ ğ˜ğ˜ªğ˜µğ˜ğ˜¶ğ˜£: open-source code + docs \n \nThe course is on Medium's TDS publication to make it easily accessible to people worldwide. Thus â†“ \n \n... anyone can learn the fundamentals of MLE & MLOps. \n \nSo, no more excuses. Just go and build your own project ğŸ”¥ \n \nCheck it out â†“ \n \nğŸ”— ğ˜›ğ˜©ğ˜¦ ğ˜ğ˜¶ğ˜­ğ˜­ ğ˜šğ˜µğ˜¢ğ˜¤ğ˜¬ 7-ğ˜šğ˜µğ˜¦ğ˜±ğ˜´ ğ˜”ğ˜“ğ˜–ğ˜±ğ˜´ ğ˜ğ˜³ğ˜¢ğ˜®ğ˜¦ğ˜¸ğ˜°ğ˜³ğ˜¬: \nhttps://lnkd.in/d_GVpZ9X \n \nhashtag\n#machinelearning \nhashtag\n#mlops \nhashtag\n#datascience "
  },
  {
    "instruction": "Write a LinkedIn post explaining a 7-step process on how to fine-tune an open-source LLM to create a real-time financial advisor.",
    "content": "ğŸ³-ğ˜€ğ˜ğ—²ğ—½ğ˜€ on ğ—µğ—¼ğ˜„ to ğ—³ğ—¶ğ—»ğ—²-ğ˜ğ˜‚ğ—»ğ—² an open-source ğ—Ÿğ—Ÿğ—  to create your real-time financial advisor â†“\n\nIn the past weeks, we covered ğ˜„ğ—µğ˜† you must fine-tune an LLM and ğ˜„ğ—µğ—®ğ˜ resources & tools you need.\n\n.\n\nNow, let's see how you can hook all the pieces together into a single fine-tuning module â†“\n\nğŸ­. ğ—Ÿğ—¼ğ—®ğ—± ğ˜ğ—µğ—² ğ—¤&ğ—” ğ—±ğ—®ğ˜ğ—®ğ˜€ğ—²ğ˜\n\nOur Q&A samples have the following structure keys: \"about_me,\" \"user_context,\" \"question,\" and \"answer.\"\n\nFor task-specific fine-tuning, you need only 100-1000 samples. Thus, you can directly load the whole JSON in memory.\n\nAfter you map every sample to a list of Python ğ˜¥ğ˜¢ğ˜µğ˜¢ğ˜¤ğ˜­ğ˜¢ğ˜´ğ˜´ğ˜¦ğ˜´ to validate the structure & type of the ingested instances.\n\nğŸ®. ğ—£ğ—¿ğ—²ğ—½ğ—¿ğ—¼ğ—°ğ—²ğ˜€ğ˜€ ğ˜ğ—µğ—² ğ—¤&ğ—” ğ—±ğ—®ğ˜ğ—®ğ˜€ğ—²ğ˜ ğ—¶ğ—»ğ˜ğ—¼ ğ—½ğ—¿ğ—¼ğ—ºğ—½ğ˜ğ˜€\n\nThe first step is to use ğ˜¶ğ˜¯ğ˜´ğ˜µğ˜³ğ˜¶ğ˜¤ğ˜µğ˜¶ğ˜³ğ˜¦ğ˜¥ to clean every sample by removing redundant characters.\n\nAfter, as every sample consists of multiple fields, you must map it to a single piece of text, known as the prompt.\n\nTo do so, you define a ğ˜—ğ˜³ğ˜°ğ˜®ğ˜±ğ˜µğ˜›ğ˜¦ğ˜®ğ˜±ğ˜­ğ˜¢ğ˜µğ˜¦ class to manage all your prompts. You will use it to map all the sample keys to a prompt using f-strings.\n\nThe last step is to map the list of Python ğ˜¥ğ˜¢ğ˜µğ˜¢ğ˜¤ğ˜­ğ˜¢ğ˜´ğ˜´ğ˜¦ğ˜´ to a HuggingFace dataset and map every sample to a prompt, as discussed above.\n\nğŸ¯. ğ—Ÿğ—¼ğ—®ğ—± ğ˜ğ—µğ—² ğ—Ÿğ—Ÿğ—  ğ˜‚ğ˜€ğ—¶ğ—»ğ—´ ğ—¤ğ—Ÿğ—¼ğ—¥ğ—”\n\nLoad a pretrained Falcon 7B LLM by passing a ğ˜£ğ˜ªğ˜µğ˜´ğ˜¢ğ˜¯ğ˜¥ğ˜£ğ˜ºğ˜µğ˜¦ğ˜´ quantization configuration that loads all the weights on 4 bits.\n\nAfter using LoRA, you freeze the weights of the original Falcon LLM and attach a set of trainable adapters to it.\n\nğŸ°. ğ—™ğ—¶ğ—»ğ—²-ğ˜ğ˜‚ğ—»ğ—¶ğ—»ğ—´\n\nThe ğ˜µğ˜³ğ˜­ package makes this step simple.\n\nYou pass to the ğ˜šğ˜ğ˜›ğ˜›ğ˜³ğ˜¢ğ˜ªğ˜¯ğ˜¦ğ˜³ class the training arguments, the dataset and the model and call the ğ˜µğ˜³ğ˜¢ğ˜ªğ˜¯() method.\n\nOne crucial aspect is configuring an experiment tracker, such as Comet ML, to log all vital metrics & artifacts.\n\nğŸ±. ğ—£ğ˜‚ğ˜€ğ—µ ğ˜ğ—µğ—² ğ—¯ğ—²ğ˜€ğ˜ ğ—ºğ—¼ğ—±ğ—²ğ—¹ ğ˜ğ—¼ ğ˜ğ—µğ—² ğ—ºğ—¼ğ—±ğ—²ğ—¹ ğ—¿ğ—²ğ—´ğ—¶ğ˜€ğ˜ğ—¿ğ˜†\n\nOne of the final steps is to attach a callback to the ğ˜šğ˜ğ˜›ğ˜›ğ˜³ğ˜¢ğ˜ªğ˜¯ğ˜¦ğ˜³ class that runs when the training ends to push the model with the lowest loss to the model registry as the new production candidate.\n\nğŸ². ğ—˜ğ˜ƒğ—®ğ—¹ğ˜‚ğ—®ğ˜ğ—² ğ˜ğ—µğ—² ğ—»ğ—²ğ˜„ ğ—½ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜ğ—¶ğ—¼ğ—» ğ—°ğ—®ğ—»ğ—±ğ—¶ğ—±ğ—®ğ˜ğ—²\n\nEvaluating generative AI models is tricky.\n\nYou can run the LLM on the test set and log the prompts & answers to Comet ML's monitoring system to check them manually.\n\nIf the provided answers are valid, using the model registry dashboard, you will manually release it to replace the old LLM.\n\nğŸ³. ğ——ğ—²ğ—½ğ—¹ğ—¼ğ˜† ğ˜ğ—¼ ğ—•ğ—²ğ—®ğ—º\n\nIt is as easy as wrapping the training & inference functions (or classes) with a Python \"@ğ˜¢ğ˜±ğ˜±.ğ˜³ğ˜¶ğ˜¯()\" decorator."
  },
  {
    "instruction": "Write a LinkedIn post detailing how to generate a synthetic domain-specific Q&A dataset in under 30 minutes for fine-tuning your open-source LLM.",
    "content": "How do you ğ—´ğ—²ğ—»ğ—²ğ—¿ğ—®ğ˜ğ—² a ğ˜€ğ˜†ğ—»ğ˜ğ—µğ—²ğ˜ğ—¶ğ—° ğ—±ğ—¼ğ—ºğ—®ğ—¶ğ—»-ğ˜€ğ—½ğ—²ğ—°ğ—¶ğ—³ğ—¶ğ—° ğ—¤&ğ—” ğ—±ğ—®ğ˜ğ—®ğ˜€ğ—²ğ˜ in <ğŸ¯ğŸ¬ ğ—ºğ—¶ğ—»ğ˜‚ğ˜ğ—²ğ˜€ to ğ—³ğ—¶ğ—»ğ—²-ğ˜ğ˜‚ğ—»ğ—² your ğ—¼ğ—½ğ—²ğ—»-ğ˜€ğ—¼ğ˜‚ğ—¿ğ—°ğ—² ğ—Ÿğ—Ÿğ— ?\n\nThis method is also known as ğ—³ğ—¶ğ—»ğ—²ğ˜ğ˜‚ğ—»ğ—¶ğ—»ğ—´ ğ˜„ğ—¶ğ˜ğ—µ ğ—±ğ—¶ğ˜€ğ˜ğ—¶ğ—¹ğ—¹ğ—®ğ˜ğ—¶ğ—¼ğ—». Here are its 3 ğ˜®ğ˜¢ğ˜ªğ˜¯ ğ˜´ğ˜µğ˜¦ğ˜±ğ˜´ â†“\n\nğ˜ğ˜°ğ˜³ ğ˜¦ğ˜¹ğ˜¢ğ˜®ğ˜±ğ˜­ğ˜¦, ğ˜­ğ˜¦ğ˜µ'ğ˜´ ğ˜¨ğ˜¦ğ˜¯ğ˜¦ğ˜³ğ˜¢ğ˜µğ˜¦ ğ˜¢ ğ˜˜&ğ˜ˆ ğ˜§ğ˜ªğ˜¯ğ˜¦-ğ˜µğ˜¶ğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜¥ğ˜¢ğ˜µğ˜¢ğ˜´ğ˜¦ğ˜µ ğ˜¶ğ˜´ğ˜¦ğ˜¥ ğ˜µğ˜° ğ˜§ğ˜ªğ˜¯ğ˜¦-ğ˜µğ˜¶ğ˜¯ğ˜¦ ğ˜¢ ğ˜§ğ˜ªğ˜¯ğ˜¢ğ˜¯ğ˜¤ğ˜ªğ˜¢ğ˜­ ğ˜¢ğ˜¥ğ˜·ğ˜ªğ˜´ğ˜°ğ˜³ ğ˜“ğ˜“ğ˜”.\n\nğ—¦ğ˜ğ—²ğ—½ ğŸ­: ğ— ğ—®ğ—»ğ˜‚ğ—®ğ—¹ğ—¹ğ˜† ğ—´ğ—²ğ—»ğ—²ğ—¿ğ—®ğ˜ğ—² ğ—® ğ—³ğ—²ğ˜„ ğ—¶ğ—»ğ—½ğ˜‚ğ˜ ğ—²ğ˜…ğ—®ğ—ºğ—½ğ—¹ğ—²ğ˜€\n\nGenerate a few input samples (~3) that have the following structure:\n- ğ˜¶ğ˜´ğ˜¦ğ˜³_ğ˜¤ğ˜°ğ˜¯ğ˜µğ˜¦ğ˜¹ğ˜µ: describe the type of investor (e.g., \"I am a 28-year-old marketing professional\")\n- ğ˜²ğ˜¶ğ˜¦ğ˜´ğ˜µğ˜ªğ˜°ğ˜¯: describe the user's intention (e.g., \"Is Bitcoin a good investment option?\")\n\nğ—¦ğ˜ğ—²ğ—½ ğŸ®: ğ—˜ğ˜…ğ—½ğ—®ğ—»ğ—± ğ˜ğ—µğ—² ğ—¶ğ—»ğ—½ğ˜‚ğ˜ ğ—²ğ˜…ğ—®ğ—ºğ—½ğ—¹ğ—²ğ˜€ ğ˜„ğ—¶ğ˜ğ—µ ğ˜ğ—µğ—² ğ—µğ—²ğ—¹ğ—½ ğ—¼ğ—³ ğ—® ğ˜ğ—²ğ—®ğ—°ğ—µğ—²ğ—¿ ğ—Ÿğ—Ÿğ— \n\nUse a powerful LLM as a teacher (e.g., GPT4, Falcon 180B, etc.) to generate up to +N similar input examples.\n\nWe generated 100 input examples in our use case, but you can generate more.\n\nYou will use the manually filled input examples to do few-shot prompting.\n\nThis will guide the LLM to give you domain-specific samples.\n\nğ˜›ğ˜©ğ˜¦ ğ˜±ğ˜³ğ˜°ğ˜®ğ˜±ğ˜µ ğ˜¸ğ˜ªğ˜­ğ˜­ ğ˜­ğ˜°ğ˜°ğ˜¬ ğ˜­ğ˜ªğ˜¬ğ˜¦ ğ˜µğ˜©ğ˜ªğ˜´:\n\"\"\"\n...\nGenerate 100 more examples with the following pattern:\n\n# USER CONTEXT 1\n...\n\n# QUESTION 1\n...\n\n# USER CONTEXT 2\n...\n\"\"\"\n\nğ—¦ğ˜ğ—²ğ—½ ğŸ¯: ğ—¨ğ˜€ğ—² ğ˜ğ—µğ—² ğ˜ğ—²ğ—®ğ—°ğ—µğ—²ğ—¿ ğ—Ÿğ—Ÿğ—  ğ˜ğ—¼ ğ—´ğ—²ğ—»ğ—²ğ—¿ğ—®ğ˜ğ—² ğ—¼ğ˜‚ğ˜ğ—½ğ˜‚ğ˜ğ˜€ ğ—³ğ—¼ğ—¿ ğ—®ğ—¹ğ—¹ ğ˜ğ—µğ—² ğ—¶ğ—»ğ—½ğ˜‚ğ˜ ğ—²ğ˜…ğ—®ğ—ºğ—½ğ—¹ğ—²ğ˜€\n\nNow, you will have the same powerful LLM as a teacher, but this time, it will answer all your N input examples.\n\nBut first, to introduce more variance, we will use RAG to enrich the input examples with news context.\n\nAfterward, we will use the teacher LLM to answer all N input examples.\n\n...and bam! You generated a domain-specific Q&A dataset with almost 0 manual work.\n\n.\n\nNow, you will use this data to train a smaller LLM (e.g., Falcon 7B) on a niched task, such as financial advising.\n\nThis technique is known as finetuning with distillation because you use a powerful LLM as the teacher (e.g., GPT4, Falcon 180B) to generate the data, which will be used to fine-tune a smaller LLM (e.g., Falcon 7B), which acts as the student.\n\nâœ’ï¸ ğ˜•ğ˜°ğ˜µğ˜¦: To ensure that the generated data is of high quality, you can hire a domain expert to check & refine it.\n\n.\n\nâ†³ To learn more about this technique, check out this article from a0Pau Labarta Bajo a0's RLML newsletter: ğŸ”— a0https://lnkd.in/diGud3Yh\n\n#machinelearning a0#mlops a0#datascience\n"
  },
  {
    "instruction": "Create a LinkedIn post about implementing a configuration/settings system in Python for all your applications using pydantic.",
    "content": "This is my favorite way to ğ—¶ğ—ºğ—½ğ—¹ğ—²ğ—ºğ—²ğ—»ğ˜ a ğ—°ğ—¼ğ—»ğ—³ğ—¶ğ—´ğ˜‚ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—»/ğ˜€ğ—²ğ˜ğ˜ğ—¶ğ—»ğ—´ğ˜€ ğ˜€ğ˜†ğ˜€ğ˜ğ—²ğ—º in ğ—£ğ˜†ğ˜ğ—µğ—¼ğ—» for all my apps â†“ \n \nThe core is based on ğ˜±ğ˜ºğ˜¥ğ˜¢ğ˜¯ğ˜µğ˜ªğ˜¤, a data validation library for Python. \n \nMore precisely, on their ğ˜‰ğ˜¢ğ˜´ğ˜¦ğ˜šğ˜¦ğ˜µğ˜µğ˜ªğ˜¯ğ˜¨ğ˜´ class. \n \nğ—ªğ—µğ˜† ğ˜‚ğ˜€ğ—² ğ˜ğ—µğ—² ğ—½ğ˜†ğ—±ğ—®ğ—»ğ˜ğ—¶ğ—° ğ—•ğ—®ğ˜€ğ—²ğ—¦ğ—²ğ˜ğ˜ğ—¶ğ—»ğ—´ğ˜€ ğ—°ğ—¹ğ—®ğ˜€ğ˜€? \n \n- you can quickly load values from .ğ˜¦ğ˜¯ğ˜· files (or even ğ˜‘ğ˜šğ˜–ğ˜• or ğ˜ ğ˜ˆğ˜”ğ˜“) \n- add default values for the configuration of your application \n- the MOST IMPORTANT one â†’ It validates the type of the loaded variables. Thus, you will always be ensured you use the correct variables to configure your system. \n \nğ—›ğ—¼ğ˜„ ğ—±ğ—¼ ğ˜†ğ—¼ğ˜‚ ğ—¶ğ—ºğ—½ğ—¹ğ—²ğ—ºğ—²ğ—»ğ˜ ğ—¶ğ˜? \n \nIt is pretty straightforward. \n \nYou subclass the ğ˜‰ğ˜¢ğ˜´ğ˜¦ğ˜šğ˜¦ğ˜µğ˜µğ˜ªğ˜¯ğ˜¨ğ˜´ class and define all your settings at the class level. \n \nIt is similar to a Python ğ˜¥ğ˜¢ğ˜µğ˜¢ğ˜¤ğ˜­ğ˜¢ğ˜´ğ˜´ but with an extra layer of data validation and factory methods. \n \nIf you assign a value to the variable, it makes it optional. \n \nIf you leave it empty, providing it in your .ğ™šğ™£ğ™« file is mandatory. \n \nğ—›ğ—¼ğ˜„ ğ—±ğ—¼ ğ˜†ğ—¼ğ˜‚ ğ—¶ğ—»ğ˜ğ—²ğ—´ğ—¿ğ—®ğ˜ğ—² ğ—¶ğ˜ ğ˜„ğ—¶ğ˜ğ—µ ğ˜†ğ—¼ğ˜‚ğ—¿ ğ— ğ—Ÿ ğ—°ğ—¼ğ—±ğ—²? \n \nYou often have a training configuration file (or inference) into a JSON or YAML file (I prefer YAML files as they are easier to read). \n \nYou shouldn't pollute your ğ˜±ğ˜ºğ˜¥ğ˜¢ğ˜¯ğ˜µğ˜ªğ˜¤ settings class with all the hyperparameters related to the module (as they are a lot, A LOT). \n \nAlso, to isolate the application & ML settings, the easiest way is to add the ğ˜µğ˜³ğ˜¢ğ˜ªğ˜¯ğ˜ªğ˜¯ğ˜¨_ğ˜¤ğ˜°ğ˜¯ğ˜§ğ˜ªğ˜¨_ğ˜±ğ˜¢ğ˜µğ˜© in your settings and use a ğ˜›ğ˜³ğ˜¢ğ˜ªğ˜¯ğ˜ªğ˜¯ğ˜¨ğ˜Šğ˜°ğ˜¯ğ˜§ğ˜ªğ˜¨ class to load it independently. \n \nDoing so lets you leverage your favorite way (probably the one you already have in your ML code) of loading a config file for the ML configuration: plain YAML or JSON files, hydra, or other fancier methods. \n \nAnother plus is that you can't hardcode the path anywhere on your system. That is a nightmare when you start using git with multiple people. \n \n. \n \nWhat do you say? Would you start using the ğ˜±ğ˜ºğ˜¥ğ˜¢ğ˜¯ğ˜µğ˜ªğ˜¤ ğ˜‰ğ˜¢ğ˜´ğ˜¦ğ˜šğ˜¦ğ˜µğ˜µğ˜ªğ˜¯ğ˜¨ğ˜´ class in your ML applications? \n \nhashtag\n#machinelearning \nhashtag\n#mlops \nhashtag\n#datascience"
  },
  {
    "instruction": "Describe the process of designing and deploying a Mistral LLM to AWS SageMaker.",
    "content": "This is ğ—µğ—¼ğ˜„ you can ğ—±ğ—²ğ˜€ğ—¶ğ—´ğ—» and ğ—±ğ—²ğ—½ğ—¹ğ—¼ğ˜† a ğ— ğ—¶ğ˜€ğ˜ğ—¿ğ—®ğ—¹ ğ—Ÿğ—Ÿğ—  to ğ—”ğ—ªğ—¦ ğ—¦ğ—®ğ—´ğ—²ğ—ºğ—®ğ—¸ğ—²ğ—¿ â†“\n\nğ—”ğ—¹ğ˜„ğ—®ğ˜†ğ˜€, the ğ—³ğ—¶ğ—¿ğ˜€ğ˜ ğ˜€ğ˜ğ—²ğ—½ is to ğ˜‚ğ—»ğ—±ğ—²ğ—¿ğ˜€ğ˜ğ—®ğ—»ğ—± the ğ—¿ğ—²ğ—¾ğ˜‚ğ—¶ğ—¿ğ—²ğ—ºğ—²ğ—»ğ˜ğ˜€ of your ğ˜€ğ˜†ğ˜€ğ˜ğ—²ğ—º. \n\nğŸ¤” Ask yourself:\n\n- What are my ğ˜¥ğ˜¢ğ˜µğ˜¢ ğ˜±ğ˜³ğ˜°ğ˜¤ğ˜¦ğ˜´ğ˜´ğ˜ªğ˜¯ğ˜¨ ğ˜¯ğ˜¦ğ˜¦ğ˜¥ğ˜´? Consider the diversity of the data formats and complexity.\n\n- How does the ğ˜¸ğ˜°ğ˜³ğ˜¬ğ˜­ğ˜°ğ˜¢ğ˜¥ look? Is it real-time or batch processing? What's the concurrency level?\n\n- What about ğ˜±ğ˜³ğ˜°ğ˜¤ğ˜¦ğ˜´ğ˜´ğ˜ªğ˜¯ğ˜¨ ğ˜¤ğ˜°ğ˜¯ğ˜´ğ˜µğ˜³ğ˜¢ğ˜ªğ˜¯ğ˜µğ˜´? Latency requirements, accuracy versus speed, and, importantly, cost considerations.\n\n- Can it ğ˜´ğ˜¤ğ˜¢ğ˜­ğ˜¦ and ğ˜¢ğ˜¥ğ˜¢ğ˜±ğ˜µ? Your strategy must include scalability and flexibility for future expansion.\n\n.\n\nAlex Vesa wrote an article in Decoding ML where he:\n- made an overview of AWS Sagemaker\n- showed how to design the system requirements and deploy a Mistral LLM using AWS Sagemaker\n\nğ˜ğ˜¦ğ˜³ğ˜¦ ğ˜ªğ˜´ ğ˜¸ğ˜©ğ˜¢ğ˜µ ğ˜ºğ˜°ğ˜¶ ğ˜¸ğ˜ªğ˜­ğ˜­ ğ˜­ğ˜¦ğ˜¢ğ˜³ğ˜¯ â†“\n\nğ—¦ğ—®ğ—´ğ—²ğ—ºğ—®ğ—¸ğ—²ğ—¿ ğ—œğ—»ğ—³ğ—²ğ—¿ğ—²ğ—»ğ—°ğ—² ğ—˜ğ—»ğ—±ğ—½ğ—¼ğ—¶ğ—»ğ˜ ğ—–ğ—¼ğ—ºğ—½ğ—¼ğ—»ğ—²ğ—»ğ˜ğ˜€\n\nğ—¦ğ—®ğ—´ğ—²ğ—ºğ—®ğ—¸ğ—²ğ—¿ ğ—˜ğ—»ğ—±ğ—½ğ—¼ğ—¶ğ—»ğ˜: The scalable and secure API for real-time predictions.\n\nğ—¦ğ—®ğ—´ğ—²ğ—ºğ—®ğ—¸ğ—²ğ—¿ ğ— ğ—¼ğ—±ğ—²ğ—¹: The artifact from training, ready for predictions.\n\nğ—¦ğ—®ğ—´ğ—²ğ—ºğ—®ğ—¸ğ—²ğ—¿ ğ—–ğ—¼ğ—»ğ—³ğ—¶ğ—´ğ˜‚ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—»: Specifies the hardware and software setup.\n\nğ—¦ğ—®ğ—´ğ—²ğ—ºğ—®ğ—¸ğ—²ğ—¿ ğ—œğ—»ğ—³ğ—²ğ—¿ğ—²ğ—»ğ—°ğ—² ğ—–ğ—¼ğ—ºğ—½ğ—¼ğ—»ğ—²ğ—»ğ˜: Manages model deployment to endpoints efficiently.\n\nâ†“â†“â†“\n\nSageMaker's multi-model endpoints offer a dynamic and efficient way to manage and serve multiple models:\n\n- dynamically manages model lifecycles on multi-model endpoints, loading models on demand\n\n- routes additional requests to other instances if one is overloaded\n\n- manages memory by unloading unused models to free up space for new ones\n\n- models can be added or removed by managing them in the S3 bucket without updating the endpoint\n\nDeploying LLMs via AWS SageMaker is a complex task that demands a deep understanding of system requirements and resource management.\n\n.\n\nIn the ğ—®ğ—¿ğ˜ğ—¶ğ—°ğ—¹ğ—², you will ğ—¹ğ—²ğ—®ğ—¿ğ—» how to ğ—±ğ—²ğ—½ğ—¹ğ—¼ğ˜† a ğ— ğ—¶ğ˜€ğ˜ğ—¿ğ—®ğ—¹ ğŸ³ğ—• on ğ—”ğ—ªğ—¦ ğ—¦ğ—®ğ—´ğ—²ğ—ºğ—®ğ—¸ğ—²ğ—¿ using a g5.12xlarge instance.\n\nTo do so, you will ğ—´ğ—¼ ğ˜€ğ˜ğ—²ğ—½-ğ—¯ğ˜†-ğ˜€ğ˜ğ—²ğ—½ ğ˜ğ—µğ—¿ğ—¼ğ˜‚ğ—´ğ—µ the ğ—°ğ—¼ğ—±ğ—² to ğ—¹ğ—²ğ—®ğ—¿ğ—» ğ—µğ—¼ğ˜„ to:\n\n1. Configure Hardware requirements per replica\n2. Prepare a Deployment Service Class\n3. How to deploy the LLM\n4. Cleaning Sagemaker resources\n\nğ—–ğ—µğ—²ğ—°ğ—¸ ğ—¶ğ˜ ğ—¼ğ˜‚ğ˜ ğ—¼ğ—» ğ——ğ—²ğ—°ğ—¼ğ—±ğ—¶ğ—»ğ—´ ğ— ğ—Ÿ\n\nâ†“â†“â†“\n\nğŸ”— ğ˜ ğ˜°ğ˜¶'ğ˜³ğ˜¦ ğ˜¯ğ˜°ğ˜µ ğ˜¥ğ˜ªğ˜¨ğ˜¨ğ˜ªğ˜¯ğ˜¨ ğ˜¥ğ˜¦ğ˜¦ğ˜±ğ˜¦ğ˜³ ğ˜ªğ˜¯ğ˜µğ˜° ğ˜¤ğ˜°ğ˜¯ğ˜¤ğ˜¦ğ˜±ğ˜µğ˜´? ğ˜ğ˜¦ğ˜³ğ˜¦'ğ˜´ ğ˜¸ğ˜©ğ˜¢ğ˜µ ğ˜©ğ˜¢ğ˜±ğ˜±ğ˜¦ğ˜¯ğ˜´... : https://lnkd.in/ee8FeMHa\n\nhashtag\n#machinelearning \nhashtag\n#mlops \nhashtag\n#datascience\n"
  },
  {
    "instruction": "Explain the benefits of using the walrus operator in Python for writing cleaner code.",
    "content": "Here is some ğ—£ğ˜†ğ˜ğ—µğ—¼ğ—» ğ˜€ğ˜†ğ—»ğ˜ğ—®ğ˜… ğ˜€ğ˜‚ğ—´ğ—®ğ—¿ that will help you ğ˜„ğ—¿ğ—¶ğ˜ğ—² ğ—°ğ—¹ğ—²ğ—®ğ—»ğ—²ğ—¿ ğ—°ğ—¼ğ—±ğ—² â†“ \n\nI am talking about the ğ˜¸ğ˜¢ğ˜­ğ˜³ğ˜¶ğ˜´ ğ˜°ğ˜±ğ˜¦ğ˜³ğ˜¢ğ˜µğ˜°ğ˜³ denoted by the `:=` symbol. \n\nIt was introduced in Python 3.8, but I rarely see it used. \n\nThus, as a \"clean code\" freak, I wanted to dedicate a post to it. \n\nğ—ªğ—µğ—®ğ˜ ğ—±ğ—¼ğ—²ğ˜€ ğ˜ğ—µğ—² ğ˜„ğ—®ğ—¹ğ—¿ğ˜‚ğ˜€ ğ—¼ğ—½ğ—²ğ—¿ğ—®ğ˜ğ—¼ğ—¿ ğ—±ğ—¼? \n\nIt's an assignment expression that allows you to assign and return a value in the same expression. \n\nğ—ªğ—µğ˜† ğ˜€ğ—µğ—¼ğ˜‚ğ—¹ğ—± ğ˜†ğ—¼ğ˜‚ ğ˜‚ğ˜€ğ—² ğ—¶ğ˜? \n\nğ˜Šğ˜°ğ˜¯ğ˜¤ğ˜ªğ˜´ğ˜¦ğ˜¯ğ˜¦ğ˜´ğ˜´: It reduces the number of lines needed for variable assignment and checking, making code more concise. \n\nğ˜™ğ˜¦ğ˜¢ğ˜¥ğ˜¢ğ˜£ğ˜ªğ˜­ğ˜ªğ˜µğ˜º: It can enhance readability by keeping related logic close, although this depends on the context and the reader's familiarity with exotic Python syntax. \n\nğ™ƒğ™šğ™§ğ™š ğ™–ğ™§ğ™š ğ™¨ğ™¤ğ™¢ğ™š ğ™šğ™­ğ™–ğ™¢ğ™¥ğ™¡ğ™šğ™¨ \n\nâ†“â†“â†“ \n\n1. Using the walrus operator, you can directly assign the result of the ğ˜­ğ˜¦ğ˜¯() function inside an if statement. \n\n2. Avoid calling the same function twice in a while loop. The benefit is less code and makes everything more readable. \n\n3. Another use case arises in list comprehensions where a value computed in a filtering condition is also needed in the expression body. Before the ğ˜¸ğ˜¢ğ˜­ğ˜³ğ˜¶ğ˜´ ğ˜°ğ˜±ğ˜¦ğ˜³ğ˜¢ğ˜µğ˜°ğ˜³, if you had to apply a function to an item from a list and filter it based on some criteria, you had to refactor it to a standard for loop. \n\n. \n\nWhen writing clean code, the detail matters. \n\nThe details make the difference between a codebase that can be read like a book or one with 10 WTFs / seconds. \n\n. \n\nWhat do you think? Does the walrus operator make the Python code more readable and concise? \n\nhashtag\n#machinelearning \nhashtag\n#mlops \nhashtag\n#datascience \n"
  },
  {
    "instruction": "Detail the three main stages of training an LLM on proprietary data.",
    "content": "You must know these ğŸ¯ ğ—ºğ—®ğ—¶ğ—» ğ˜€ğ˜ğ—®ğ—´ğ—²ğ˜€ of ğ˜ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—®ğ—» ğ—Ÿğ—Ÿğ—  to train your own ğ—Ÿğ—Ÿğ—  on your ğ—½ğ—¿ğ—¼ğ—½ğ—¿ğ—¶ğ—²ğ˜ğ—®ğ—¿ğ˜† ğ—±ğ—®ğ˜ğ—®.\n\n# ğ—¦ğ˜ğ—®ğ—´ğ—² ğŸ­: ğ—£ğ—¿ğ—²ğ˜ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—³ğ—¼ğ—¿ ğ—°ğ—¼ğ—ºğ—½ğ—¹ğ—²ğ˜ğ—¶ğ—¼ğ—»\n\nYou start with a bear foot randomly initialized LLM.\n\nThis stage aims to teach the model to spit out tokens. More concretely, based on previous tokens, the model learns to predict the next token with the highest probability.\n\nFor example, your input to the model is \"The best programming language is ___\", and it will answer, \"The best programming language is Rust.\"\n\nIntuitively, at this stage, the LLM learns to speak.\n\nğ˜‹ğ˜¢ğ˜µğ˜¢: >1 trillion token (~= 15 million books). The data quality doesn't have to be great.\n\n# ğ—¦ğ˜ğ—®ğ—´ğ—² ğŸ®: ğ—¦ğ˜‚ğ—½ğ—²ğ—¿ğ˜ƒğ—¶ğ˜€ğ—²ğ—± ğ—³ğ—¶ğ—»ğ—²-ğ˜ğ˜‚ğ—»ğ—¶ğ—»ğ—´ (ğ—¦ğ—™ğ—§) ğ—³ğ—¼ğ—¿ ğ—±ğ—¶ğ—®ğ—¹ğ—¼ğ—´ğ˜‚ğ—²\n\nYou start with the pretrained model from stage 1.\n\nThis stage teaches the model to respond to the user's questions.\n\nFor example, without this step, when prompted, \"What is the best programming language?\", it has a high probability of creating a series of questions such as: \"What is MLOps? What is MLE? etc.\"\n\nAs the model mimics the training data, you must fine-tune it on Q&A data to align the model to respond to questions instead of predicting the following tokens.\n\nAfter the fine-tuning step, it will respond to \"What is the best programming language?\" with \"Rust\".\n\nğ˜‹ğ˜¢ğ˜µğ˜¢: 10K - 100K Q&A example\n\nğ˜•ğ˜°ğ˜µğ˜¦: After aligning the model to respond to questions, you can further single-task fine-tune the model to specialize the LLM on a specific use case.\n\n# ğ—¦ğ˜ğ—®ğ—´ğ—² ğŸ¯: ğ—¥ğ—Ÿğ—›ğ—™\n\nDemonstration data tells the model what kind of responses to give but doesn't tell the model how good or bad a response is.\n\nThe goal is to align your model with user feedback (what users liked or didn't like) to increase the probability of generating answers that users find helpful.\n\nğ˜™ğ˜“ğ˜ğ˜ ğ˜ªğ˜´ ğ˜´ğ˜±ğ˜­ğ˜ªğ˜µ ğ˜ªğ˜¯ 2:\n\n1. Using the LLM from stage 2, train a reward model to act as a scoring function using (prompt, winning_response, losing_response) samples (= comparison data). \n\nThe model will learn to maximize the difference between these 2. After training, this model outputs rewards for (prompt, response) tuples.\n\nğ˜‹ğ˜¢ğ˜µğ˜¢: 100K - 1M comparisons\n\n2. Use an RL algorithm (e.g., PPO) to fine-tune the LLM from stage 2. Here, you will use the reward model trained above to give a score for every: (prompt, response). \n\nThe RL algorithm will align the LLM to generate prompts with higher rewards, increasing the probability of generating responses that users liked.\n\nğ˜‹ğ˜¢ğ˜µğ˜¢: 10K - 100K prompts"
  },
  {
    "instruction": "Highlight the four pillars necessary to build production ML systems.",
    "content": "The ğŸ° ğ—½ğ—¶ğ—¹ğ—¹ğ—®ğ—¿ğ˜€ to ğ—¯ğ˜‚ğ—¶ğ—¹ğ—± ğ—½ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜ğ—¶ğ—¼ğ—» ğ— ğ—Ÿ ğ˜€ğ˜†ğ˜€ğ˜ğ—²ğ—ºğ˜€ â†“\n\nAs highlighted by Alex Vesa, before building a production-ready system, it is critical to go through a set of questions that will later decide the nature of your ML system architecture.\n\nğ˜ğ˜¦ğ˜³ğ˜¦ ğ˜¢ğ˜³ğ˜¦ ğ˜µğ˜©ğ˜¦ 4 ğ˜±ğ˜ªğ˜­ğ˜­ğ˜¢ğ˜³ğ˜´ ğ˜µğ˜©ğ˜¢ğ˜µ ğ˜ºğ˜°ğ˜¶ ğ˜¢ğ˜­ğ˜¸ğ˜¢ğ˜ºğ˜´ ğ˜©ğ˜¢ğ˜·ğ˜¦ ğ˜µğ˜° ğ˜¤ğ˜°ğ˜¯ğ˜´ğ˜ªğ˜¥ğ˜¦ğ˜³ ğ˜£ğ˜¦ğ˜§ğ˜°ğ˜³ğ˜¦ ğ˜¥ğ˜¦ğ˜´ğ˜ªğ˜¨ğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜¢ğ˜¯ğ˜º ğ˜´ğ˜ºğ˜´ğ˜µğ˜¦ğ˜® â†“\n\nâ” ğ——ğ—®ğ˜ğ—®\n\n- What data types do you have? (e.g., tabular data, images, text, etc.)\n- What does the data look like? (e.g., for text data, is it in a single language or multiple?)\n- How do you collect the data?\n- At what frequency do you have to collect the data?\n- How do you collect labels for the data? (crucial for how you plan to evaluate and monitor the model in production)\n\nâ” ğ—§ğ—µğ—¿ğ—¼ğ˜‚ğ—´ğ—µğ—½ğ˜‚ğ˜\n\n- What are the throughput requirements? You must know at least the throughput's minimum, average, and maximum statistics.\n- How many requests the system must handle simultaneously? (1, 10, 1k, 1 million, etc.)\n\nâ” ğ—Ÿğ—®ğ˜ğ—²ğ—»ğ—°ğ˜†\n\n- What are the latency requirements? (1 millisecond, 10 milliseconds, 1 second, etc.)\n- Throughput vs. latency trade-off\n- Accuracy vs. speed trade-off\n\nâ” ğ—œğ—»ğ—³ğ—¿ğ—®ğ˜€ğ˜ğ—¿ğ˜‚ğ—°ğ˜ğ˜‚ğ—¿ğ—²\n\n- Batch vs. real-time architecture (closely related to the throughput vs. latency trade-off)\n- How should the system scale? (e.g., based on CPU workload, # of requests, queue size, data size, etc.)\n- Cost requirements\n\n.\n\nDo you see how we shifted the focus from model performance towards how it is integrated into a more extensive system?\n\nWhen building production-ready ML, the model's accuracy is no longer the holy grail but a bullet point in a grander scheme.\n\n. \n\nğ—§ğ—¼ ğ˜€ğ˜‚ğ—ºğ—ºğ—®ğ—¿ğ—¶ğ˜‡ğ—², the 4 pillars to keep in mind before designing an ML architecture are:\n- Data\n- Throughput\n- Latency\n- Infrastructure\n\nLet me know in the comments below if you want to dive deeper into each topic. â†´\n\nhashtag\n#machinelearning \nhashtag\n#mlops \nhashtag\n#datascience\n\n.\n\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.\nVizualizaÅ£i traducerea\n"
  },
  {
    "instruction": "Discuss the essentials of fine-tuning an open-source LLM to create a financial advisor.",
    "content": "ğ—ªğ—µğ—®ğ˜ do you need to ğ—³ğ—¶ğ—»ğ—²-ğ˜ğ˜‚ğ—»ğ—² an open-source ğ—Ÿğ—Ÿğ—  to create your own ğ—³ğ—¶ğ—»ğ—®ğ—»ğ—°ğ—¶ğ—®ğ—¹ ğ—®ğ—±ğ˜ƒğ—¶ğ˜€ğ—¼ğ—¿?\n\nThis is the ğ—Ÿğ—Ÿğ—  ğ—³ğ—¶ğ—»ğ—²-ğ˜ğ˜‚ğ—»ğ—¶ğ—»ğ—´ ğ—¸ğ—¶ğ˜ you must know â†“\n\nğ——ğ—®ğ˜ğ—®ğ˜€ğ—²ğ˜\n\nThe key component of any successful ML project is the data.\n\nYou need a 100 - 1000 sample Q&A (questions & answers) dataset with financial scenarios.\n\nThe best approach is to hire experts to create it manually.\n\nBut, for a PoC, that might get expensive & slow.\n\nThe good news is that a method called \"ğ˜ğ˜ªğ˜¯ğ˜¦ğ˜µğ˜¶ğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜¸ğ˜ªğ˜µğ˜© ğ˜¥ğ˜ªğ˜´ğ˜µğ˜ªğ˜­ğ˜­ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯\" exists.\n\nIn a nutshell, this is how it works: \"Use a big & powerful LLM (e.g., GPT4) to generate your fine-tuning data. After, use this data to fine-tune a smaller model (e.g., Falcon 7B).\"\n\nFor specializing smaller LLMs on specific use cases (e.g., financial advisors), this is an excellent method to kick off your project.\n\n\nğ—£ğ—¿ğ—²-ğ˜ğ—¿ğ—®ğ—¶ğ—»ğ—²ğ—± ğ—¼ğ—½ğ—²ğ—»-ğ˜€ğ—¼ğ˜‚ğ—¿ğ—°ğ—² ğ—Ÿğ—Ÿğ— \n\nYou never want to start training your LLM from scratch (or rarely).\n\nWhy? Because you need trillions of tokens & millions of $$$ in compute power.\n\nYou want to fine-tune your LLM on your specific task.\n\nThe good news is that you can find a plethora of open-source LLMs on HuggingFace (e.g., Falcon, LLaMa, etc.)\n\n\nğ—£ğ—®ğ—¿ğ—®ğ—ºğ—²ğ˜ğ—²ğ—¿ ğ—²ğ—³ğ—³ğ—¶ğ—°ğ—¶ğ—²ğ—»ğ˜ ğ—³ğ—¶ğ—»ğ—²-ğ˜ğ˜‚ğ—»ğ—¶ğ—»ğ—´\n\nAs LLMs are big... duh...\n\n... they don't fit on a single GPU.\n\nAs you want only to fine-tune the LLM, the community invented clever techniques that quantize the LLM (to fit on a single GPU) and fine-tune only a set of smaller adapters.\n\nOne popular approach is QLoRA, which can be implemented using HF's `ğ˜±ğ˜¦ğ˜§ğ˜µ` Python package.\n\n\nğ— ğ—Ÿğ—¢ğ—½ğ˜€\n\nAs you want your project to get to production, you have to integrate the following MLOps components:\n\n- experiment tracker to monitor & compare your experiments\n- model registry to version & share your models between the FTI pipelines\n- prompts monitoring to debug & track complex chains\n\nâ†³ Available on ML platforms, such as Comet ML.\n\n\nğ—–ğ—¼ğ—ºğ—½ğ˜‚ğ˜ğ—² ğ—½ğ—¹ğ—®ğ˜ğ—³ğ—¼ğ—¿ğ—º\n\nThe most common approach is to train your LLM on your on-prem Nivida GPUs cluster or rent them on cloud providers such as AWS, Paperspace, etc.\n\nBut what if I told you that there is an easier way?\n\nThere is! It is called serverless.\n\nFor example, Beam is a GPU serverless provider that makes deploying your training pipeline as easy as decorating your Python function with `@ğ˜¢ğ˜±ğ˜±.ğ˜³ğ˜¶ğ˜¯()`.\n\nAlong with ease of deployment, you can easily add your training code to your CI/CD to add the final piece of the MLOps puzzle, called CT (continuous training).\n\n.\n\nâ†³ To see everything in action, check out my FREE ğ—›ğ—®ğ—»ğ—±ğ˜€-ğ—¼ğ—» ğ—Ÿğ—Ÿğ— ğ˜€ ğ—°ğ—¼ğ˜‚ğ—¿ğ˜€ğ—² & give it a â­: ğŸ”— https://lnkd.in/dZgqtf8f"
  },
  {
    "instruction": "Write a brief LinkedIn post introducing resources for starting with MLOps, highlighting a website and a GitHub repository as starting points.",
    "content": "Have you ever wondered where to start learning MLOps?\n.\nYou can find lots of valuable information by checking out the MLOps organization site:\nhttps://ml-ops.org/\nAlso, you can find hundreds of essential references by following this GitHub repository:\nhttps://lnkd.in/dxknAmTK\nNow you have no excuse to start learning MLOps!\nhashtag\n#\nmlops\nhashtag\n#\ncontent\nhashtag\n#\nlearning\n-----\nğŸ’¡ My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n."
  },
  {
    "instruction": "Create a light-hearted LinkedIn post with an amusing image, intended to engage the audience humorously.",
    "content": "Who fell for it raise your hand ğŸ˜‚ğŸ˜‚ğŸ˜‚"
  },
  {
    "instruction": "Write a humorous LinkedIn post sharing an amusing meme related to AWS, crediting the creator.",
    "content": "This is so hilarious ğŸ¤­ğŸ˜‚ğŸ˜‚ğŸ˜‚\nCredit\nCorey Quinn\nhashtag\n#\nAWS\nhashtag\n#\nawscloud\nhashtag\n#\nawscloudpractitioner\nhashtag\n#\nmachinelearning\nhashtag\n#\nnew\nhashtag\n#\nbrad"
  },
  {
    "instruction": "Craft a LinkedIn post discussing the importance of feature stores in ML production systems, highlighting three key aspects.",
    "content": "These are 3 characteristics showing that a feature store is a crucial component within any end-to-end ML production system.\n.\n#ğŸ ğ…ğğšğ­ğ®ğ«ğ ğ¦ğšğ§ğšğ ğğ¦ğğ§ğ­\nFeature management is the ability to discover and share features across teams. This characteristic may save time and computation as multiple features can be shared across models. Also, it helps share knowledge across teams or projects.\n#ğŸ ğ…ğğšğ­ğ®ğ«ğ ğœğ¨ğ¦ğ©ğ®ğ­ğšğ­ğ¢ğ¨ğ§\nFeature computation means you compute your features only once and store them for future uses. In this case, it acts like a data warehouse, and it may save a lot of time and computation, especially when computing specific features consume a lot of resources (usually the case).\n#ğŸ‘ ğ…ğğšğ­ğ®ğ«ğ ğœğ¨ğ§ğ¬ğ¢ğ¬ğ­ğğ§ğœğ²\nFeature consistency refers to the problem of having two separate pipelines for the model between training and production (known as the training-serving skew issue). Using a feature store, you unify the logic of feature creation, using the feature store both in training and production.\n.\nI am curious to know what tools you use for your feature store solutions. Please Let me know if the comments below ğŸ‘‡\nhashtag\n#\nfeaturestore\nhashtag\n#\ndatawarehouse\nhashtag\n#\nmlops\n-----\nğŸ’¡ My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n."
  },
  {
    "instruction": "Write a brief LinkedIn post introducing resources for starting with MLOps, highlighting a website and a GitHub repository as starting points.",
    "content": "Have you ever wondered where to start learning MLOps?\n.\nYou can find lots of valuable information by checking out the MLOps organization site:\nhttps://ml-ops.org/\nAlso, you can find hundreds of essential references by following this GitHub repository:\nhttps://lnkd.in/dxknAmTK\nNow you have no excuse to start learning MLOps!\nhashtag\n#\nmlops\nhashtag\n#\ncontent\nhashtag\n#\nlearning\n-----\nğŸ’¡ My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n."
  },
  {
    "instruction": "Create a light-hearted LinkedIn post with an amusing image, intended to engage the audience humorously.",
    "content": "Who fell for it raise your hand ğŸ˜‚ğŸ˜‚ğŸ˜‚"
  },
  {
    "instruction": "Write a humorous LinkedIn post sharing an amusing meme related to AWS, crediting the creator.",
    "content": "This is so hilarious ğŸ¤­ğŸ˜‚ğŸ˜‚ğŸ˜‚\nCredit\nCorey Quinn\nhashtag\n#\nAWS\nhashtag\n#\nawscloud\nhashtag\n#\nawscloudpractitioner\nhashtag\n#\nmachinelearning\nhashtag\n#\nnew\nhashtag\n#\nbrad"
  },
  {
    "instruction": "Craft a post to inform your network about a recent publication or article related to your field. Provide a brief summary of the article's key points and your personal insights or opinions.",
    "content": "Electromyography has the potential to revolutionize how we interact with computers. Imagine typing without a keyboard, controlling visual interfaces without a mouse, and navigating AR/VR environments much more intuitively. The signals in our wrists are so clear that EMG can detect finger motion of just a millimeter. This video highlights some of the groundbreaking research by the incredible CTRL-Labs team who joined us last year. ğŸ¤© Mind-blowing potential. #innovation #technology #facebook"
  },
  {
    "instruction": "Express gratitude in a post to thank a mentor or colleague for their support and guidance in your professional journey. Share a specific example or memory of how their mentorship has positively impacted your career.",
    "content": "I'm incredibly grateful to have [mentor's or colleague's name] as a mentor. Their guidance and support have been invaluable in my professional growth. I still remember the time when [specific example or memory]. Thank you for everything! #gratitude #mentorship #careerdevelopment"
  },
  {
    "instruction": "Craft a LinkedIn post discussing the importance of feature stores in ML production systems, highlighting three key aspects.",
    "content": "These are 3 characteristics showing that a feature store is a crucial component within any end-to-end ML production system.\n.\n#ğŸ ğ…ğğšğ­ğ®ğ«ğ ğ¦ğšğ§ğšğ ğğ¦ğğ§ğ­\nFeature management is the ability to discover and share features across teams. This characteristic may save time and computation as multiple features can be shared across models. Also, it helps share knowledge across teams or projects.\n#ğŸ ğ…ğğšğ­ğ®ğ«ğ ğœğ¨ğ¦ğ©ğ®ğ­ğšğ­ğ¢ğ¨ğ§\nFeature computation means you compute your features only once and store them for future uses. In this case, it acts like a data warehouse, and it may save a lot of time and computation, especially when computing specific features consume a lot of resources (usually the case).\n#ğŸ‘ ğ…ğğšğ­ğ®ğ«ğ ğœğ¨ğ§ğ¬ğ¢ğ¬ğ­ğğ§ğœğ²\nFeature consistency refers to the problem of having two separate pipelines for the model between training and production (known as the training-serving skew issue). Using a feature store, you unify the logic of feature creation, using the feature store both in training and production.\n.\nI am curious to know what tools you use for your feature store solutions. Please Let me know if the comments below ğŸ‘‡\nhashtag\n#\nfeaturestore\nhashtag\n#\ndatawarehouse\nhashtag\n#\nmlops\n-----\nğŸ’¡ My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n."
  },
  {
    "instruction": "Create a promotional post to introduce your network to a new product or service. Describe the key features, benefits, and how it can address a specific need or problem your audience may have. Include a call-to-action encouraging engagement or further exploration.",
    "content": "Come to see how you can be a part of a place of innovation, with projects like: â¡ï¸ lek3.co, charging electric cars anywhere. Quick and easy. â¡ï¸ 2park.io, smart parking at your fingertips and â¡ï¸ SafeFleet, innovating the telematics industry. We're waiting for you! :) â¡ï¸ â¡ï¸ â¡ï¸ https://lnkd.in/g-sEMiW #iot #design #future #hr #team #smartcities #projects #electriccars #telematics #parks #futurism #charging #software #projecting #safefleet #recruiting #lek3 #lek3co"
  },
  {
    "instruction": "Write a LinkedIn post about the distinction between workflow management tools and orchestration tools in MLOps, explaining their uses and differences.",
    "content": "ğ–ğ¨ğ«ğ¤ğŸğ¥ğ¨ğ° ğ¦ğšğ§ğšğ ğğ¦ğğ§ğ­/ğ¬ğœğ¡ğğğ®ğ¥ğğ«ğ¬ and ğ¨ğ«ğœğ¡ğğ¬ğ­ğ«ğšğ­ğ¢ğ¨ğ§ ğ­ğ¨ğ¨ğ¥ğ¬ are two critical elements of any successful MLOps infrastructure. But what, are they used for, and what are some vital differences between them?\n.\n#ğŸ ğ–ğ¨ğ«ğ¤ğŸğ¥ğ¨ğ° ğ¦ğšğ§ğšğ ğğ¦ğğ§ğ­ ğ­ğ¨ğ¨ğ¥ğ¬ (ğ¨ğ« ğ¬ğœğ¡ğğğ®ğ¥ğğ«ğ¬)\nWorkflow management tools are concerned with when to run jobs and what resources are needed to run those jobs.\nSchedulers are based on job-type abstractions: DAGs or priority queues. The dependencies between multiple jobs are defined within a YAML or Python file. For complex applications, this is essential because, for example, if job A fails, you don't want to execute job B. Another example would be that if job A fails, retry to run it 5 times until you give up.\nSchedulers are used for periodic jobs.\nPopular tools: Airflow, Argo, Prefect, Kubeflow, Metaflow\n#ğŸ ğğ«ğœğ¡ğğ¬ğ­ğ«ğšğ­ğ¢ğ¨ğ§ ğ­ğ¨ğ¨ğ¥ğ¬\nOrchestration tools concern where to get resources to run the jobs and how to scale the resources to run the jobs efficiently.\nOrchestration tools deal with lower-level attractions: machines, instances, or clusters. They manage the hardware resources on top of which you run your system.\nOrchestration tools are responsible for scaling the computational power size to efficiently support the given jobs (e.g., if the number of jobs is more significant than the number of instances, it will spin up more instances).\nOrchestration tools are used for long-running servers that serve clients' requests.\nPopular tools: Kubernetes (K8s)\n.\nThese tools are usually used interchangeably. Schedulers typically run on top of orchestration tools.\nhashtag\n#\ntools\nhashtag\n#\nschedulers\nhashtag\n#\norchestration\n-----\nğŸ’¡ My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n."
  },
  {
    "instruction": "Craft a LinkedIn post emphasizing the necessity of a feature store in ML systems, sharing a succinct, insightful thought.",
    "content": "You don't need a feature store...\nSaid no one after he understood why using one is so essential for any ML system.\nHere is why ğŸ‘‡\nhashtag\n#\nmlops\nhashtag\n#\nml\nhashtag\n#\nfeaturestore"
  },
  {
    "instruction": "Publish a LinkedIn post listing top Python tools for code quality, including links and a note on automating their use with pre-commit hooks.",
    "content": "Top 3 tools to keep your Python code professional without any additional effort ğŸ‘‡\n.\n#ğŸ. ğğ¥ğšğœğ¤\nA tool for automatically formatting your code.\nCheck it out:\nhttps://lnkd.in/dR_RdjT9\n#ğŸ. ğ…ğ¥ğšğ¤ğğŸ–\nA tool that checks and validates your coding style.\nCheck it out:\nhttps://lnkd.in/dBmyuECV\n#ğŸ‘. ğ‘ğğŸğ®ğ«ğ›\nA tool for automatically refurbishing and modernizing your Python codebases.\nCheck it out:\nhttps://lnkd.in/d67J9hbt\n.\nAn important note is that you can automate the trigger of these tools using pre-commit. Thus, these tools will be triggered every time you run a commit.\nhashtag\n#\npython\nhashtag\n#\ncleancode\nhashtag\n#\ncodingstyle\n-----\nğŸ’¡ My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n."
  },
  {
    "instruction": "Write a LinkedIn post presenting a list of essential ML monitoring metrics, aiming to educate followers on effective monitoring practices.",
    "content": "I compiled a list of the top 10 metrics your ML monitoring system should track.\n.\n#ğŸ ğğ©ğğ«ğšğ­ğ¢ğ¨ğ§ğšğ¥ ğ¦ğğ­ğ«ğ¢ğœğ¬\nUsed to monitor the health of your overall software system.\nâ latency\nâ throughput\nâ CPU/GPU utilization\nâ memory utilization\nâ the number of requests your model receives in the last X minutes/hours/days\nâ the number of successful requests\n#ğŸ ğŒğ‹-ğ¬ğ©ğğœğ¢ğŸğ¢ğœ ğ¦ğğ­ğ«ğ¢ğœğ¬\nUsed to monitor the performance of your ML system.\nâ accuracy (extremely useful when you have feedback or natural labels from the user: click rate, upvote, downvote, purchases, bookmarks, views, etc.)\nâ predictions (as predictions are low dimensional various statistics are easy to compute + the distribution of the prediction represents a proxy for the input distribution)\nâ features (feature validation + two-sample tests for drift detection)\nâ raw inputs (these are harder to monitor due to their scattered nature within the infrastructure)\n-----\nğŸ’¡ My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n."
  },
  {
    "instruction": "Compose a LinkedIn post analyzing the most demanding domains in AI/ML, using market size and growth as key metrics.",
    "content": "I compiled a table with the top 8 most demanding domains in AI/ML. I want to understand in which field to specialize further as a freelancer. Thus I wanted to take a data-centric approach in making this decision. I used the market size as the 'demand' metric because where the money flows represents a strong signal of demand in that field.\nNote that the absolute values in $$$ might not be 100% accurate compared as they might be extracted from different years, such as 2020-2021-2022. The most interesting column is ğ‚ğ€ğ†ğ‘ % which shows the potential growth in that field. If there is growth, there is new demand for engineers in that field.\nThe market size within the table consists only of the AI/ML part of that domain (i.e., the entire energy domain is way bigger than 3.82$ Billion). Also, some domains might cross each other, so the Global Machine Learning values won't add to the rest of the fields.\n.\nWhat do you think? Do you believe that this table is relevant? Did I miss any domains? What field in AI/ML do you think has the most potential?\nI am trying to figure out the answers to these questions myself, so feel free to start an open discussion in the comments below ğŸ‘€.\nhashtag\n#\ncarrer\nhashtag\n#\nai\nhashtag\n#\ndemand\n-----\nğŸ’¡ My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n."
  },
  {
    "instruction": "Create a LinkedIn post about evaluating the necessity of additional data for improving a model's performance using learning curves.",
    "content": "Everybody says you can improve your model by training it on more data. Is it true? How can we test this theory before spending money and effort collecting that new data?\n.\nGood news. The solution is quite simple.\nUsing the ğ‹ğğšğ«ğ§ğ¢ğ§ğ  ğ‚ğ®ğ«ğ¯ğ graph, you can quickly see the performance evolution of the model with various amounts of data.\nIt is important to validate this because more data is not the answer to all your problems. ğŸ˜‚\n.\nThe main idea of the ğ‹ğğšğ«ğ§ğ¢ğ§ğ  ğ‚ğ®ğ«ğ¯ğ method is simple:\n1. You create N subsets from your training dataset. Where the first subset is the smallest. You keep increasing the subset size at a given rate until you reach the full dataset size: 1 < 2 < 3 < ... < N.\n2. You train your model on subsample 1.\n3. You test your, model on the test split (which is not divided; it is not good practice to touch your test split).\n4. Save the results.\n5. Repeat steps 2-4 on all your subsets.\n6. Plot the results.\n.\nUsing this mechanism, you can simulate if adding more data will boost your model's performance.\nAlso, you can quickly compare different models and evaluate which one needs more/fewer data to achieve your desired accuracy.\n.\nCheck out how easy it is to do this with Sklearn:\nhttps://lnkd.in/d358Dhhd\n.\nğŸ’¡ My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n."
  },
  {
    "instruction": "Draft a LinkedIn post about framing machine learning problems effectively, using a case study from e-commerce for personalized discounts.",
    "content": "How you frame a\nhashtag\n#\nmachinelearning\nproblem is essential for your future self.\nIt can make your life easy or a living hell.\nğŸ’¡ ğ„ğ±ğšğ¦ğ©ğ¥ğ\nA user just clicked on our e-commerce site that sells tech. We want to welcome them and give them a personalized discount on a hand-picked item.\nHow do we solve this problem using\nhashtag\n#\nML\n?\nAt first sight, we might start building a multiclass classifier that takes input features about the user and predicts the probability that the user might like a particular item.\nBut this decision will make your future life a living hell if we consider it.\n.\nğ–ğ¡ğ²?\nFor every new item in the store, you need to retrain the model to adapt its output to the new inventory.\n.\nğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§\nBuild a binary classifier that takes as input the user's features and information about a specific item. The model's output will reflect only the probability that the user might like the item you used as input.\nIn this scenario, we will have no issue when a new item is added to the store.\n.\nğ–ğ¡ğ²?\nBecause the output is not dependent on the number of items within the store.\nWe will predict the probability for every item and take the most significant out of them.\nIn conclusion, it might be worth taking a deep breath and thinking about the problem before starting to code your solution.\n.\nğŸ’¡ My focus is making\nhashtag\n#\nml\neasy and intuitive to understand. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n."
  },
  {
    "instruction": "Write a LinkedIn post recommending an ML course for building end-to-end applications, highlighting the instructor's expertise.",
    "content": "I just started watching an excellent  ML course on building real-world, end-to-end ML applications.\nIf you ever want to level up your ML knowledge and make it production-ready, I recommend this course by\nPau Labarta Bajo\n.\nHe is a DS and ML professional with 8+ years of experience building production-ready ML systems. Thus, you can get real-world guidance on how to build end-to-end ML applications.\nWithin the course, you will get clear explanations of how to build and design an end-to-end ML application.\nI think it's worth it to check it out.\nP.S. Also, you can follow him on LinkedIn and Twitter, where he often posts content about ML in production. I learned a lot from him.\nhashtag\n#\nml\nhashtag\n#\nmlops\nhashtag\n#\nrealworld\nhashtag\n#\ntutorial\nhashtag\n#\ncourse"
  },
  {
    "instruction": "Create a LinkedIn post celebrating the anniversary of a book on deep learning, offering copies to the audience.",
    "content": "We'll send five of these to lucky people anywhere in the world ğŸŒ\nWe want to celebrate with you - our book just turned 1 - they grow up so fast.\nğŸ‘‰ Simply comment below by adding (@) friends who should get the book. We randomly select your friends and contact them for their address to ship the books.\nğŸ“– About the book:\n\"Why does my neural network not learn?\" by @Frank Hafner and I, will kickstart your friends' deep learning journey!\nWith best practices and extensive experience from applying deep learning in research and development, they will get their own neural networks to train.\nğŸš€ Check it out in more detail, and get your own copy here (make sure to search on your amazon marketplace, we are basically available everywhere) >>\nhttps://lnkd.in/exfkjY9t\nâ˜• And as always, feel free to follow here on LinkedIn.\nhashtag\n#\ndatascience\nhashtag\n#\npython\nhashtag\n#\ndeeplearning\nhashtag\n#\nGiveaway\nhashtag\n#\nbooks\nhashtag\n#\nkdp\nhashtag\n#\nmachinelearning"
  },
  {
    "instruction": "Draft a LinkedIn post on the importance of data distribution shifts awareness in real-world ML applications.",
    "content": "These are 3 data distribution shifts you have to know when building real-world ML applications ğŸ‘‡\nhashtag\n#\nml\nhashtag\n#\ndata\nhashtag\n#\ndistributionshift\nhashtag\n#\nmonitoring"
  },
  {
    "instruction": "Promote an upcoming Q&A session with Paul Iusztin, highlighting the opportunity for the audience to engage and ask anything about MLOps.",
    "content": "Next Tuesday we'll have the 2nd edition of ğ—”ğ˜€ğ—¸ ğ— ğ—®ğ—¿ğ˜ƒğ—²ğ—¹ğ—¼ğ˜‚ğ˜€ ğ— ğ—Ÿğ—¢ğ—½ğ˜€ ğ—”ğ—»ğ˜†ğ˜ğ—µğ—¶ğ—»ğ—´: A live Q&A sessions where we try to answer all your questions ğŸ¤— This time Paul Iusztin will join us, sign up here: https://lnkd.in/eVk7e8k5"
  },
  {
    "instruction": "Announce a new course on building hands-on LLM systems with a focus on good LLMOps principles, highlighting the content and inviting the audience to enroll for free.",
    "content": "If you want to ğ¥ğğšğ«ğ§ for FREE to ğ›ğ®ğ¢ğ¥ğ ğ¡ğšğ§ğğ¬-ğ¨ğ§ ğ‹ğ‹ğŒ ğ¬ğ²ğ¬ğ­ğğ¦ğ¬ using good LLMOps principles, we want to announce that we just ğŸğ¢ğ§ğ¢ğ¬ğ¡ğğ the code & video lessons for the \"ğ‡ğšğ§ğğ¬-ğ¨ğ§ ğ‹ğ‹ğŒğ¬\" ğœğ¨ğ®ğ«ğ¬ğ.\nBy finishing the Hands-On LLMs free course, you will learn how to use the 3-pipeline architecture & LLMOps good practices to design, build, and deploy a real-time financial advisor powered by LLMs & vector DBs.\nWe will primarily focus on the engineering & MLOps aspects.\nThus, by the end of this series, you will know how to build & deploy a real ML system, not some isolated code in Notebooks.\nğŒğ¨ğ«ğ ğ©ğ«ğğœğ¢ğ¬ğğ¥ğ², ğ­ğ¡ğğ¬ğ ğšğ«ğ ğ­ğ¡ğ 3 ğœğ¨ğ¦ğ©ğ¨ğ§ğğ§ğ­ğ¬ ğ²ğ¨ğ® ğ°ğ¢ğ¥ğ¥ ğ¥ğğšğ«ğ§ ğ­ğ¨ ğ›ğ®ğ¢ğ¥ğ:\n1.  a ğ«ğğšğ¥-ğ­ğ¢ğ¦ğ ğ¬ğ­ğ«ğğšğ¦ğ¢ğ§ğ  ğ©ğ¢ğ©ğğ¥ğ¢ğ§ğ (deployed on AWS) that listens to financial news, cleans & embeds the documents, and loads them to a vector DB\n2.  a ğŸğ¢ğ§ğ-ğ­ğ®ğ§ğ¢ğ§ğ  ğ©ğ¢ğ©ğğ¥ğ¢ğ§ğ (deployed as a serverless continuous training) that fine-tunes an LLM on financial data using QLoRA, monitors the experiments using an experiment tracker and saves the best model to a model registry\n3.  an ğ¢ğ§ğŸğğ«ğğ§ğœğ ğ©ğ¢ğ©ğğ¥ğ¢ğ§ğ built in LangChain (deployed as a serverless RESTful API) that loads the fine-tuned LLM from the model registry and answers financial questions using RAG (leveraging the vector DB populated with financial news in real-time)\nWe will also show you how to integrate various serverless tools, such as:\nâ€¢ Comet ML as your ML Platform;\nâ€¢ Qdrant as your vector DB;\nâ€¢ Beam as your infrastructure.\nğ–ğ¡ğ¨ ğ¢ğ¬ ğ­ğ¡ğ¢ğ¬ ğŸğ¨ğ«?\nThe series targets MLE, DE, DS, or SWE who want to learn to engineer LLM systems using LLMOps good principles.\nğ‡ğ¨ğ° ğ°ğ¢ğ¥ğ¥ ğ²ğ¨ğ® ğ¥ğğšğ«ğ§?\nThe series contains 4 hands-on video lessons and the open-source code you can access on GitHub.\nğ‚ğ®ğ«ğ¢ğ¨ğ®ğ¬?\nâ†³ Check it out and support us with a â­:  ğŸ”—\nhttps://lnkd.in/dZgqtf8f\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Explain the benefits and ease of using serverless technology, particularly Beam, for deploying and managing ML models on GPUs.",
    "content": "ğ——ğ—²ğ—½ğ—¹ğ—¼ğ˜†ğ—¶ğ—»ğ—´ & ğ—ºğ—®ğ—»ğ—®ğ—´ğ—¶ğ—»ğ—´ ML models is ğ—µğ—®ğ—¿ğ—±, especially when running your models on GPUs.\nBut ğ˜€ğ—²ğ—¿ğ˜ƒğ—²ğ—¿ğ—¹ğ—²ğ˜€ğ˜€ makes things ğ—²ğ—®ğ˜€ğ˜†.\nUsing\nBeam\nas your serverless provider, deploying & managing ML models can be as easy as â†“\nğ——ğ—²ğ—³ğ—¶ğ—»ğ—² ğ˜†ğ—¼ğ˜‚ğ—¿ ğ—¶ğ—»ğ—³ğ—¿ğ—®ğ˜€ğ˜ğ—¿ğ˜‚ğ—°ğ˜ğ˜‚ğ—¿ğ—² & ğ—±ğ—²ğ—½ğ—²ğ—»ğ—±ğ—²ğ—»ğ—°ğ—¶ğ—²ğ˜€\nIn a few lines of code, you define the application that contains:\n- the requirements of your infrastructure, such as the CPU, RAM, and GPU\n- the dependencies of your application\n- the volumes from where you can load your data and store your artifacts\nğ——ğ—²ğ—½ğ—¹ğ—¼ğ˜† ğ˜†ğ—¼ğ˜‚ğ—¿ ğ—·ğ—¼ğ—¯ğ˜€\nUsing the Beam application, you can quickly decore your Python functions to:\n- run them once on the given serverless application\n- put your task/job in a queue to be processed or even schedule it using a CRON-based syntax\n- even deploy it as a RESTful API endpoint\n.\nAs you can see in the image below, you can have one central function for training or inference, and with minimal effort, you can switch from all these deployment methods.\nAlso, you don't have to bother at all with managing the infrastructure on which your jobs run. You specify what you need, and Beam takes care of the rest.\nBy doing so, you can directly start to focus on your application and stop carrying about the infrastructure.\nThis is the power of serverless!\nâ†³ Check out\nBeam\nto learn more: ğŸ”—\nhttps://lnkd.in/d4-pkCxc\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Share a method for automating the generation of documentation for Python functions using Copilot, highlighting its efficiency and time-saving benefits.",
    "content": "This is how I ğ—´ğ—²ğ—»ğ—²ğ—¿ğ—®ğ˜ğ—²ğ—± ğ—£ğ˜†ğ——ğ—¼ğ—°ğ˜€ for ğŸ­ğŸ¬ğŸ¬ ğ—£ğ˜†ğ˜ğ—µğ—¼ğ—» ğ—³ğ˜‚ğ—»ğ—°ğ˜ğ—¶ğ—¼ğ—»ğ˜€ in <ğŸ­ ğ—µğ—¼ğ˜‚ğ—¿ â†“\nThe most boring programming part is to write PyDocs, so I usually write clean code and let it speak for itself.\nBut, for open-source projects where you have to generate robust documentation, PyDocs are a must.\nThe good news is that now you can automate this process using Copilot.\nYou can see in the video below an example of how easy it is.\nI tested it on more complex functions/classes, and it works well. I chose this example because it fits nicely on one screen.\nOnce I tested Copilot's experience, I will never go back.\nIt is true that, in some cases, you have to make some minor adjustments. But that is still 10000% more efficient than writing it from scratch.\nIf you want more examples, check out our Hands-on LLMs course, where all the PyDocs are generated 99% using Copilot in <1 hour.\nâ†³ Check it out: ğŸ”—\nhttps://lnkd.in/dZgqtf8f\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Detail the necessary components and steps for fine-tuning an open-source LLM to create a financial advisor, emphasizing data, pre-trained models, fine-tuning techniques, and MLOps integration.",
    "content": "ğ—ªğ—µğ—®ğ˜ do you need to ğ—³ğ—¶ğ—»ğ—²-ğ˜ğ˜‚ğ—»ğ—² an open-source ğ—Ÿğ—Ÿğ—  to create your own ğ—³ğ—¶ğ—»ğ—®ğ—»ğ—°ğ—¶ğ—®ğ—¹ ğ—®ğ—±ğ˜ƒğ—¶ğ˜€ğ—¼ğ—¿?\nThis is the ğ—Ÿğ—Ÿğ—  ğ—³ğ—¶ğ—»ğ—²-ğ˜ğ˜‚ğ—»ğ—¶ğ—»ğ—´ ğ—¸ğ—¶ğ˜ you must know â†“\nğ——ğ—®ğ˜ğ—®ğ˜€ğ—²ğ˜\nThe key component of any successful ML project is the data.\nYou need a 100 - 1000 sample Q&A (questions & answers) dataset with financial scenarios.\nThe best approach is to hire a bunch of experts to create it manually.\nBut, for a PoC, that might get expensive & slow.\nThe good news is that a method called \"ğ˜ğ˜ªğ˜¯ğ˜¦ğ˜µğ˜¶ğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜¸ğ˜ªğ˜µğ˜© ğ˜¥ğ˜ªğ˜´ğ˜µğ˜ªğ˜­ğ˜­ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯\" exists.\nIn a nutshell, this is how it works: \"Use a big & powerful LLM (e.g., GPT4) to generate your fine-tuning data. After, use this data to fine-tune a smaller model (e.g., Falcon 7B).\"\nFor specializing smaller LLMs on specific use cases (e.g., financial advisors), this is an excellent method to kick off your project.\nğ—£ğ—¿ğ—²-ğ˜ğ—¿ğ—®ğ—¶ğ—»ğ—²ğ—± ğ—¼ğ—½ğ—²ğ—»-ğ˜€ğ—¼ğ˜‚ğ—¿ğ—°ğ—² ğ—Ÿğ—Ÿğ— \nYou never want to start training your LLM from scratch (or rarely).\nWhy? Because you need trillions of tokens & millions of $$$ in compute power.\nYou want to fine-tune your LLM on your specific task.\nThe good news is that you can find a plethora of open-source LLMs on HuggingFace (e.g., Falcon, LLaMa, etc.)\nğ—£ğ—®ğ—¿ğ—®ğ—ºğ—²ğ˜ğ—²ğ—¿ ğ—²ğ—³ğ—³ğ—¶ğ—°ğ—¶ğ—²ğ—»ğ˜ ğ—³ğ—¶ğ—»ğ—²-ğ˜ğ˜‚ğ—»ğ—¶ğ—»ğ—´\nAs LLMs are big... duh...\n... they don't fit on a single GPU.\nAs you want only to fine-tune the LLM, the community invented clever techniques that quantize the LLM (to fit on a single GPU) and fine-tune only a set of smaller adapters.\nOne popular approach is QLoRA, which can be implemented using HF's `ğ˜±ğ˜¦ğ˜§ğ˜µ` Python package.\nğ— ğ—Ÿğ—¢ğ—½ğ˜€\nAs you want your project to get to production, you have to integrate the following MLOps components:\n- experiment tracker to monitor & compare your experiments\n- model registry to version & share your models between the FTI pipelines\n- prompts monitoring to debug & track complex chains\nâ†³ All of them are available on ML platforms, such as Comet ML ğŸ”—\nhttps://lnkd.in/d7jNQz7m\nğ—–ğ—¼ğ—ºğ—½ğ˜‚ğ˜ğ—² ğ—½ğ—¹ğ—®ğ˜ğ—³ğ—¼ğ—¿ğ—º\nThe most common approach is to train your LLM on your on-prem Nivida GPUs cluster or rent them on cloud providers such as AWS, Paperspace, etc.\nBut what if I told you that there is an easier way?\nThere is! It is called serverless.\nFor example,\nBeam\nis a GPU serverless provider that makes deploying your training pipeline as easy as decorating your Python function with `@ğ˜¢ğ˜±ğ˜±.ğ˜³ğ˜¶ğ˜¯()`.\nAlong with ease of deployment, you can easily add your training code to your CI/CD to add the final piece of the MLOps puzzle, called CT (continuous training).\nâ†³ Beam: ğŸ”—\nhttps://lnkd.in/dedCaMDh\n.\nâ†³ To see all these components in action, check out my FREE ğ—›ğ—®ğ—»ğ—±ğ˜€-ğ—¼ğ—» ğ—Ÿğ—Ÿğ— ğ˜€ ğ—°ğ—¼ğ˜‚ğ—¿ğ˜€ğ—² & give it a â­:  ğŸ”—\nhttps://lnkd.in/dZgqtf8f\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience"
  },
  {
    "instruction": "Celebrate the milestone of surpassing 400 GitHub stars for the Hands-on LLMs course, expressing gratitude towards collaborators and outlining the course's practical approach to leveraging LLMs.",
    "content": "ğ—§ğ—µğ—² ğ—›ğ—®ğ—»ğ—±ğ˜€-ğ—¼ğ—» ğ—Ÿğ—Ÿğ— ğ˜€ FREE ğ—°ğ—¼ğ˜‚ğ—¿ğ˜€ğ—² just ğ—½ğ—®ğ˜€ğ˜€ğ—²ğ—± ğŸ°ğŸ¬ğŸ¬+ ğ—šğ—¶ğ˜ğ—›ğ˜‚ğ—¯ â­ğŸŒŸâ­\nA big ğ—§ğ—µğ—®ğ—»ğ—¸ ğ˜†ğ—¼ğ˜‚!, for everyone who supported the GitHub repo. This means a lot to me.\nAlso, I want to thank Pau Labarta and Alexandru Razvant for this fantastic collaboration and for making this course possible.\n.\nğ˜ğ˜°ğ˜³ ğ˜µğ˜©ğ˜¦ ğ˜±ğ˜¦ğ˜°ğ˜±ğ˜­ğ˜¦ ğ˜¸ğ˜©ğ˜° ğ˜¥ğ˜°ğ˜¯'ğ˜µ ğ˜¬ğ˜¯ğ˜°ğ˜¸, ğ˜©ğ˜¦ğ˜³ğ˜¦ ğ˜ªğ˜´ ğ˜´ğ˜°ğ˜®ğ˜¦ ğ˜¤ğ˜°ğ˜¯ğ˜µğ˜¦ğ˜¹ğ˜µ ğ˜¢ğ˜£ğ˜°ğ˜¶ğ˜µ ğ˜µğ˜©ğ˜¦ ğ˜¤ğ˜°ğ˜¶ğ˜³ğ˜´ğ˜¦ â†“\nğ—§ğ—µğ—² ğ—›ğ—®ğ—»ğ—±ğ˜€-ğ—¢ğ—» ğ—Ÿğ—Ÿğ— ğ˜€ course is not just another demo of how to make a few predictions in a notebook.\nYou'll walk away with a ğ—³ğ˜‚ğ—¹ğ—¹ğ˜† ğ—¼ğ—½ğ—²ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ—®ğ—¹ ğ—½ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜, leveraging Large Language Models (LLMs) to build a chatbot for financial investment advice.\n=== ğ—ªğ—µğ—®ğ˜ ğ—¬ğ—¼ğ˜‚'ğ—¹ğ—¹ ğ—•ğ˜‚ğ—¶ğ—¹ğ—± ===\nWithin the course, you will leverage the ğŸ¯-ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² ğ—®ğ—¿ğ—°ğ—µğ—¶ğ˜ğ—²ğ—°ğ˜ğ˜‚ğ—¿ğ—², as follows:\nğŸ­. ğ—™ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—² ğ—£ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—²: You'll create a system to ingest real-time financial newsâ€”crucial for up-to-date advice.\nğŸ®. ğ—§ğ—¿ğ—®ğ—±ğ—¶ğ—»ğ—´ ğ—£ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—²: You'll fine-tune an LLM to specialize the model in making financial decisions.\nğŸ¯. ğ—œğ—»ğ—³ğ—²ğ—¿ğ—²ğ—»ğ—°ğ—² ğ—£ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—²: You'll combine all the components and deploy the model as a RESTful API, making your application accessible worldwide.\nThese pipelines will be independently developed, deployed, and scaled, ensuring modular and clean code.\nğ˜Šğ˜©ğ˜¦ğ˜¤ğ˜¬ ğ˜ªğ˜µ ğ˜°ğ˜¶ğ˜µ & ğ˜¨ğ˜ªğ˜·ğ˜¦ ğ˜ªğ˜µ ğ˜¢ â­ â†“\nâ†³ğŸ”— Hands-On LLMs course:\nhttps://lnkd.in/dZgqtf8f\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Introduce the three main stages of training an LLM on proprietary data, focusing on pretraining for completion, supervised fine-tuning for dialogue, and reinforcement learning from human feedback for quality improvement.",
    "content": "You must know these ğŸ¯ ğ—ºğ—®ğ—¶ğ—» ğ˜€ğ˜ğ—®ğ—´ğ—²ğ˜€ of ğ˜ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—®ğ—» ğ—Ÿğ—Ÿğ—  to train your own ğ—Ÿğ—Ÿğ—  on your ğ—½ğ—¿ğ—¼ğ—½ğ—¿ğ—¶ğ—²ğ˜ğ—®ğ—¿ğ˜† ğ—±ğ—®ğ˜ğ—®.\n# ğ—¦ğ˜ğ—®ğ—´ğ—² ğŸ­: ğ—£ğ—¿ğ—²ğ˜ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—³ğ—¼ğ—¿ ğ—°ğ—¼ğ—ºğ—½ğ—¹ğ—²ğ˜ğ—¶ğ—¼ğ—»\nYou start with a bear foot randomly initialized LLM.\nThis stage aims to teach the model to spit out tokens. More concretely, based on previous tokens, the model learns to predict the next token with the highest probability.\nFor example, your input to the model is \"The best programming language is ___\", and it will answer, \"The best programming language is Rust.\"\nIntuitively, at this stage, the LLM learns to speak.\nğ˜‹ğ˜¢ğ˜µğ˜¢:  >1 trillion token (~= 15 million books). The data quality doesn't have to be great. Hence, you can scrape data from the internet.\n# ğ—¦ğ˜ğ—®ğ—´ğ—² ğŸ®: ğ—¦ğ˜‚ğ—½ğ—²ğ—¿ğ˜ƒğ—¶ğ˜€ğ—²ğ—± ğ—³ğ—¶ğ—»ğ—²-ğ˜ğ˜‚ğ—»ğ—¶ğ—»ğ—´ (ğ—¦ğ—™ğ—§) ğ—³ğ—¼ğ—¿ ğ—±ğ—¶ğ—®ğ—¹ğ—¼ğ—´ğ˜‚ğ—²\nYou start with the pretrained model from stage 1.\nThis stage aims to teach the model to respond to the user's questions.\nFor example, without this step, when prompting: \"What is the best programming language?\", it has a high probability of creating a series of questions such as: \"What is MLOps? What is MLE? etc.\"\nAs the model mimics the training data, you must fine-tune it on Q&A (questions & answers) data to align the model to respond to questions instead of predicting the following tokens.\nAfter the fine-tuning step, when prompted, \"What is the best programming language?\", it will respond, \"Rust\".\nğ˜‹ğ˜¢ğ˜µğ˜¢: 10K - 100K Q&A example\nğ˜•ğ˜°ğ˜µğ˜¦: After aligning the model to respond to questions, you can further single-task fine-tune the model, on Q&A data, on a specific use case to specialize the LLM.\n# ğ—¦ğ˜ğ—®ğ—´ğ—² ğŸ¯: ğ—¥ğ—²ğ—¶ğ—»ğ—³ğ—¼ğ—¿ğ—°ğ—²ğ—ºğ—²ğ—»ğ˜ ğ—¹ğ—²ğ—®ğ—¿ğ—»ğ—¶ğ—»ğ—´ ğ—³ğ—¿ğ—¼ğ—º ğ—µğ˜‚ğ—ºğ—®ğ—» ğ—³ğ—²ğ—²ğ—±ğ—¯ğ—®ğ—°ğ—¸ (ğ—¥ğ—Ÿğ—›ğ—™)\nDemonstration data tells the model what kind of responses to give but doesn't tell the model how good or bad a response is.\nThe goal is to align your model with user feedback (what users liked or didn't like) to increase the probability of generating answers that users find helpful.\nğ˜™ğ˜“ğ˜ğ˜ ğ˜ªğ˜´ ğ˜´ğ˜±ğ˜­ğ˜ªğ˜µ ğ˜ªğ˜¯ 2:\n1. Using the LLM from stage 2, train a reward model to act as a scoring function using (prompt, winning_response, losing_response) samples (= comparison data). The model will learn to maximize the difference between these 2. After training, this model outputs rewards for (prompt, response) tuples.\nğ˜‹ğ˜¢ğ˜µğ˜¢: 100K - 1M comparisons\n2. Use an RL algorithm (e.g., PPO) to fine-tune the LLM from stage 2. Here, you will use the reward model trained above to give a score for every: (prompt, response). The RL algorithm will align the LLM to generate prompts with higher rewards, increasing the probability of generating responses that users liked.\nğ˜‹ğ˜¢ğ˜µğ˜¢: 10K - 100K prompts\n.\nNote: Post inspired by Chip Huyen's \"RLHF: Reinforcement Learning from Human Feedback\" article: ğŸ”—\nhttps://lnkd.in/dRTFHeFZ\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience"
  },
  {
    "instruction": "Summarize the 8 essential categories of MLOps tools every MLOps engineer should be familiar with, emphasizing the importance of mastering tools from each category for success.",
    "content": "These are the ğŸ´ ğ˜ğ˜†ğ—½ğ—²ğ˜€ of ğ— ğ—Ÿğ—¢ğ—½ğ˜€ ğ˜ğ—¼ğ—¼ğ—¹ğ˜€ that must be in your toolbelt to be a ğ˜€ğ˜‚ğ—°ğ—°ğ—²ğ˜€ğ˜€ğ—³ğ˜‚ğ—¹ ğ— ğ—Ÿğ—¢ğ—½ğ˜€ ğ—²ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ â†“\nIf you are into MLOps, you are aware of the 1000+ tools in the space and think you have to know.\nThe reality is that all of these tools can be boiled down to 8 main categories.\nIf you learn the fundamentals and master one tool from each category, you will be fine.\n.\nBaÅŸak TuÄŸÃ§e Eskili\nand\nMaria Vechtomova\nfrom\nMarvelous MLOps\nwrote an excellent summary highlighting these 8 categories:\n1. ğ™‘ğ™šğ™§ğ™¨ğ™ğ™¤ğ™£ ğ™˜ğ™¤ğ™£ğ™©ğ™§ğ™¤ğ™¡: crucial for the traceability and reproducibility of an ML model deployment or run. Without a version control system, it is difficult to find out what exact code version was responsible for specific runs or errors you might have in production. (ğŸ”§ GitHub, GitLab, etc.)\n2. ğ˜¾ğ™„/ğ˜¾ğ˜¿: automated tests are triggered upon pull request creation & deployment to production should only occur through the CD pipeline (ğŸ”§ GitHub Actions, GitLab CI/CD, Jenkins, etc.)\n3. ğ™’ğ™¤ğ™§ğ™ ğ™›ğ™¡ğ™¤ğ™¬ ğ™¤ğ™§ğ™˜ğ™ğ™šğ™¨ğ™©ğ™§ğ™–ğ™©ğ™ğ™¤ğ™£: manage complex dependencies between different tasks, such as data preprocessing, feature engineering, ML model training (ğŸ”§  Airflow, ZenML, AWS Step Functions, etc.)\n4. ğ™ˆğ™¤ğ™™ğ™šğ™¡ ğ™§ğ™šğ™œğ™ğ™¨ğ™©ğ™§ğ™®: store, version, and share trained ML model artifacts, together with additional metadata (ğŸ”§  Comet ML, W&B, MLFlow, etc.)\n5. ğ˜¿ğ™¤ğ™˜ğ™ ğ™šğ™§ ğ™§ğ™šğ™œğ™ğ™¨ğ™©ğ™§ğ™®: store, version, and share Docker images. Basically, all your code will be wrapped up in Docker images and shared through this registry (ğŸ”§ Docker Hub, ECR, etc.)\n6 & 7. ğ™ˆğ™¤ğ™™ğ™šğ™¡ ğ™©ğ™§ğ™–ğ™ğ™£ğ™ğ™£ğ™œ & ğ™¨ğ™šğ™§ğ™«ğ™ğ™£ğ™œ ğ™ğ™£ğ™›ğ™§ğ™–ğ™¨ğ™©ğ™§ğ™ªğ™˜ğ™©ğ™ªğ™§ğ™š: if on-premise, you will likely have to go with Kubernetes. There are multiple choices if you are on a cloud provider: Azure ML on Azure, Sagemaker on AWS, and Vertex AI on GCP.\n8. ğ™ˆğ™¤ğ™£ğ™ğ™©ğ™¤ğ™§ğ™ğ™£ğ™œ: Monitoring in ML systems goes beyond what is needed for monitoring regular software applications. The distinction lies in that the model predictions can fail even if all typical health metrics appear in good condition. (ğŸ”§  SageMaker, NannyML, Arize, etc.)\nThe secret sauce in MLOps is knowing how to glue all these pieces together while keeping things simple.\n.\nâ†³ To read more about these components, check out the\nMarvelous MLOps\narticle: ğŸ”—\nhttps://lnkd.in/dUHRBzDt\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Outline the steps for implementing a streaming pipeline to populate a vector database for a financial assistant using LLMs, detailing the process from data extraction to embedding and database insertion.",
    "content": "This is ğ—µğ—¼ğ˜„ you can ğ—¶ğ—ºğ—½ğ—¹ğ—²ğ—ºğ—²ğ—»ğ˜ a ğ˜€ğ˜ğ—¿ğ—²ğ—®ğ—ºğ—¶ğ—»ğ—´ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² to populate a ğ˜ƒğ—²ğ—°ğ˜ğ—¼ğ—¿ ğ——ğ—• to do ğ—¥ğ—”ğ—š for a ğ—³ğ—¶ğ—»ğ—®ğ—»ğ—°ğ—¶ğ—®ğ—¹ ğ—®ğ˜€ğ˜€ğ—¶ğ˜€ğ˜ğ—®ğ—»ğ˜ powered by ğ—Ÿğ—Ÿğ— ğ˜€.\nIn a previous post, I covered ğ˜„ğ—µğ˜† you need a streaming pipeline over a batch pipeline when implementing RAG.\nNow, we will focus on the ğ—µğ—¼ğ˜„, aka implementation details â†“\nğŸ All the following steps are wrapped in\nBytewax\nfunctions and connected in a single streaming pipeline.\nğ—˜ğ˜…ğ˜ğ—¿ğ—®ğ—°ğ˜ ğ—³ğ—¶ğ—»ğ—®ğ—»ğ—°ğ—¶ğ—®ğ—¹ ğ—»ğ—²ğ˜„ğ˜€ ğ—³ğ—¿ğ—¼ğ—º ğ—”ğ—¹ğ—½ğ—®ğ—°ğ—®\nYou need 2 types of inputs:\n1. A WebSocket API to listen to financial news in real time. This will be used to listen 24/7 for new data and ingest it as soon as it is available.\n2. A RESTful API to ingest historical data in batch mode. When you deploy a fresh vector DB, you must populate it with data between a given range [date_start; date_end].\nYou wrap the ingested HTML document and its metadata in a `pydantic` NewsArticle model to validate its schema.\nRegardless of the input type, the ingested data is the same. Thus, the following steps are the same for both data inputs â†“\nğ—£ğ—®ğ—¿ğ˜€ğ—² ğ˜ğ—µğ—² ğ—›ğ—§ğ— ğ—Ÿ ğ—°ğ—¼ğ—»ğ˜ğ—²ğ—»ğ˜\nAs the ingested financial news is in HTML, you must extract the text from particular HTML tags.\n`unstructured` makes it as easy as calling `partition_html(document)`, which will recursively return the text within all essential HTML tags.\nThe parsed NewsArticle model is mapped into another `pydantic` model to validate its new schema.\nThe elements of the news article are the headline, summary and full content.\nğ—–ğ—¹ğ—²ğ—®ğ—» ğ˜ğ—µğ—² ğ˜ğ—²ğ˜…ğ˜\nNow we have a bunch of text that has to be cleaned. Again, `unstructured` makes things easy. Calling a few functions we clean:\n- the dashes & bullets\n- extra whitespace & trailing punctuation\n- non ascii chars\n- invalid quotes\nFinally, we standardize everything to lowercase.\nğ—–ğ—µğ˜‚ğ—»ğ—¸ ğ˜ğ—µğ—² ğ˜ğ—²ğ˜…ğ˜\nAs the text can exceed the context window of the embedding model, we have to chunk it.\nYet again, `unstructured` provides a valuable function that splits the text based on the tokenized text and expected input length of the embedding model.\nThis strategy is naive, as it doesn't consider the text's structure, such as chapters, paragraphs, etc. As the news is short, this is not an issue, but LangChain provides a `RecursiveCharacterTextSplitter` class that does that if required.\nğ—˜ğ—ºğ—¯ğ—²ğ—± ğ˜ğ—µğ—² ğ—°ğ—µğ˜‚ğ—»ğ—¸ğ˜€\nYou pass all the chunks through an encoder-only model.\nWe have used `all-MiniLM-L6-v2` from `sentence-transformers`, a small model that can run on a CPU and outputs a 384 embedding.\nBut based on the size and complexity of your data, you might need more complex and bigger models.\nğ—Ÿğ—¼ğ—®ğ—± ğ˜ğ—µğ—² ğ—±ğ—®ğ˜ğ—® ğ—¶ğ—» ğ˜ğ—µğ—² ğ—¤ğ—±ğ—¿ğ—®ğ—»ğ˜ ğ˜ƒğ—²ğ—°ğ˜ğ—¼ğ—¿ ğ——ğ—•\nFinally, you insert the embedded chunks and their metadata into the\nQdrant\nvector DB.\nThe metadata contains the embedded text, the source_url and the publish date.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning"
  },
  {
    "instruction": "Announce the achievement of reaching 1400 subscribers for the \"Decoding ML\" newsletter and preview an upcoming series of articles based on the Hands-on LLMs course.",
    "content": "My ğ—»ğ—²ğ˜„ğ˜€ğ—¹ğ—²ğ˜ğ˜ğ—²ğ—¿ just ğ—½ğ—®ğ˜€ğ˜€ğ—²ğ—± 1400 subscribers ğŸ‰ğŸ‰ğŸ‰  If you are into ğ—Ÿğ—Ÿğ— ğ˜€, ğ˜ƒğ—²ğ—°ğ˜ğ—¼ğ—¿ ğ——ğ—•ğ˜€, and ğ— ğ—Ÿğ—¢ğ—½ğ˜€, you will like the ğ—³ğ˜‚ğ˜ğ˜‚ğ—¿ğ—² ğ˜€ğ—²ğ—¿ğ—¶ğ—²ğ˜€ I am about to ğ˜€ğ˜ğ—®ğ—¿ğ˜ â†“\n.\nâœŒï¸ ğ—™ğ—¶ğ—¿ğ˜€ğ˜, I want to thank everybody who reads my newsletter: \"Decoding ML.\" As it is completely free, your engagement is the only thing that motivates me.\n.\nğŸ”¥ ğ—¦ğ—²ğ—°ğ—¼ğ—»ğ—±ğ—¹ğ˜†, **ğ˜¦ğ˜¹ğ˜¤ğ˜ªğ˜µğ˜¦ğ˜¥ ğ˜·ğ˜°ğ˜ªğ˜¤ğ˜¦** here are my plans for the \"Decoding ML\" newsletter\nUntil now, the weekly articles were randomly picked from various ğ— ğ—Ÿğ—˜, ğ— ğ—Ÿğ—¢ğ—½ğ˜€, ğ——ğ—Ÿ, and ğ—´ğ—²ğ—»ğ—²ğ—¿ğ—®ğ˜ğ—¶ğ˜ƒğ—² ğ—”ğ—œ ğ˜ğ—¼ğ—½ğ—¶ğ—°ğ˜€.\nğ—¦ğ˜ğ—®ğ—¿ğ˜ğ—¶ğ—»ğ—´ ğ˜ğ—µğ—¶ğ˜€ ğ˜„ğ—²ğ—²ğ—¸, based on my ğ—›ğ—®ğ—»ğ—±ğ˜€-ğ—¼ğ—» ğ—Ÿğ—Ÿğ— ğ˜€ ğ—°ğ—¼ğ˜‚ğ—¿ğ˜€ğ—², I will ğ—¹ğ—®ğ˜‚ğ—»ğ—°ğ—µ a ğ˜€ğ—²ğ—¿ğ—¶ğ—²ğ˜€ of ğ˜€ğ—µğ—¼ğ—¿ğ˜ ğ—®ğ—¿ğ˜ğ—¶ğ—°ğ—¹ğ—²ğ˜€ that will teach you how to ğ—±ğ—²ğ˜€ğ—¶ğ—´ğ—», ğ—¯ğ˜‚ğ—¶ğ—¹ğ—±, and ğ—±ğ—²ğ—½ğ—¹ğ—¼ğ˜† an ğ—²ğ—»ğ—±-ğ˜ğ—¼-ğ—²ğ—»ğ—± ğ—Ÿğ—Ÿğ—  ğ˜€ğ˜†ğ˜€ğ˜ğ—²ğ—º for a ğ—³ğ—¶ğ—»ğ—®ğ—»ğ—°ğ—¶ğ—®ğ—¹ ğ—®ğ˜€ğ˜€ğ—¶ğ˜€ğ˜ğ—®ğ—»ğ˜.\nIt will cover topics such as:\n- the 3-pipeline / FTI architecture\n- building your own QA dataset\n- fine-tuning an LLM using QLoRA\n- building a streaming pipeline\n- using a vector DB for RAG\n- gluing everything together using LangChain\n- deploying the solution\nThis is not the course itself. It is just an overview of the most essential aspects.\nBut, if you are too busy to take the whole course, these weekly FREE 5-minute lessons are a great way to learn how to build an end-to-end LLM product seamlessly.\n.\nğŸ‘€ ğ—¦ğ˜‚ğ—¯ğ˜€ğ—°ğ—¿ğ—¶ğ—¯ğ—² to start receiving them in your mail â†“\nâ†³ğŸ”—\nhttps://lnkd.in/dsMR4ivA\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps"
  },
  {
    "instruction": "Discuss the necessity of a streaming pipeline over a batch pipeline for implementing Retrieval Augmented Generation (RAG) in LLM applications, with a focus on real-time data feeding for financial assistant examples.",
    "content": "ğ—ªğ—µğ˜† do you need a ğ˜€ğ˜ğ—¿ğ—²ğ—®ğ—ºğ—¶ğ—»ğ—´ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² ğ—¶ğ—»ğ˜€ğ˜ğ—²ğ—®ğ—± of a ğ—¯ğ—®ğ˜ğ—°ğ—µ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² when implementing ğ—¥ğ—”ğ—š in your ğ—Ÿğ—Ÿğ—  ğ—®ğ—½ğ—½ğ—¹ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€? ğ—ªğ—µğ—®ğ˜ do you need to ğ—¶ğ—ºğ—½ğ—¹ğ—²ğ—ºğ—²ğ—»ğ˜ a ğ˜€ğ˜ğ—¿ğ—²ğ—®ğ—ºğ—¶ğ—»ğ—´ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² for a financial assistant?\nâ†³ ğ—ªğ—µğ˜† ğ—±ğ—¼ ğ˜†ğ—¼ğ˜‚ ğ—»ğ—²ğ—²ğ—± ğ˜ğ—¼ ğ—¯ğ˜‚ğ—¶ğ—¹ğ—± ğ—® ğ˜€ğ˜ğ—¿ğ—²ğ—®ğ—ºğ—¶ğ—»ğ—´ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² ğ—¶ğ—»ğ˜€ğ˜ğ—²ğ—®ğ—± ğ—¼ğ—³ ğ—® ğ—¯ğ—®ğ˜ğ—°ğ—µ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—²?\nThe quality of your RAG implementation is as good as the quality & freshness of your data.\nThus, depending on your use case, you have to ask:\n\"How fresh does my data from the vector DB have to be to provide accurate answers?\"\nBut for the best user experience, the data has to be as fresh as possible, aka real-time data.\nFor example, when implementing a financial assistant, being aware of the latest financial news is critical. A new piece of information can completely change the course of your strategy.\nHence, when implementing RAG, one critical aspect is to have your vector DB synced with all your external data sources in real time.\nA batch pipeline will work if your use case accepts a particular delay (e.g., one hour, one day, etc.).\nBut with tools like Bytewax ğŸ, building streaming applications becomes much more accessible. So why not aim for the best?\nâ†³ ğ—ªğ—µğ—®ğ˜ ğ—±ğ—¼ ğ˜†ğ—¼ğ˜‚ ğ—»ğ—²ğ—²ğ—± ğ˜ğ—¼ ğ—¯ğ˜‚ğ—¶ğ—¹ğ—± ğ—® ğ˜€ğ˜ğ—¿ğ—²ğ—®ğ—ºğ—¶ğ—»ğ—´ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² ğ—³ğ—¼ğ—¿ ğ—® ğ—³ğ—¶ğ—»ğ—®ğ—»ğ—°ğ—¶ğ—®ğ—¹ ğ—®ğ˜€ğ˜€ğ—¶ğ˜€ğ˜ğ—®ğ—»ğ˜?\n- A financial news data source exposed through a web socket (e.g., Alpaca)\n- A Python streaming processing framework. For example,\nBytewax\nğŸ is built in Rust for efficiency and exposes a Python interface for ease of use - you don't need the Java ecosystem to implement real-time pipelines anymore.  â†³ğŸ”—\nhttps://lnkd.in/dWJytkZ5\n- A Python package to process, clean, and chunk documents. `unstructured` offers a rich set of features that makes parsing HTML documents extremely convenient.\n- An encoder-only language model that maps your chunked documents into embeddings. `setence-transformers` is well integrated with HuggingFace and has a huge list of models of various sizes.\n- A vector DB, where to insert your embeddings and their metadata (e.g., the embedded text, the source_url, the creation date, etc.). For example,\nQdrant\nprovides a rich set of features and a seamless experience.  â†³ğŸ”—\nhttps://lnkd.in/d_FA9Bb3\n- A way to deploy your streaming pipeline. Docker + AWS will never disappoint you.\n- A CI/CD pipeline for continuous tests & deployments. GitHub Actions is a great serverless option with a rich ecosystem.\nThis is what you need to build & deploy a streaming pipeline solely in Python ğŸ”¥\n.\nWhat is your experience with batch vs. streaming pipelines? Let me know in the comments â†“\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps"
  },
  {
    "instruction": "Describe how to implement real-time monitoring and metrics in ML systems, detailing the scenarios where ground truth data is available in various timelines and the steps to build a monitoring pipeline.",
    "content": "ğ—›ğ—¼ğ˜„ ğ˜ğ—¼ ğ—®ğ—±ğ—± ğ—¿ğ—²ğ—®ğ—¹-ğ˜ğ—¶ğ—ºğ—² ğ—ºğ—¼ğ—»ğ—¶ğ˜ğ—¼ğ—¿ğ—¶ğ—»ğ—´ ğ—®ğ—»ğ—± ğ—ºğ—²ğ˜ğ—¿ğ—¶ğ—°ğ˜€ to your ML system.\nYour model is exposed to performance degradation after it is deployed to production.\nThat is why you need to monitor it constantly.\nThe most common way to monitor an ML model is to compute its metrics.\nBut for that, you need the ground truth.\nğ—œğ—» ğ—½ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜ğ—¶ğ—¼ğ—», ğ˜†ğ—¼ğ˜‚ ğ—°ğ—®ğ—» ğ—®ğ˜‚ğ˜ğ—¼ğ—ºğ—®ğ˜ğ—¶ğ—°ğ—®ğ—¹ğ—¹ğ˜† ğ—®ğ—°ğ—°ğ—²ğ˜€ğ˜€ ğ˜ğ—µğ—² ğ—´ğ—¿ğ—¼ğ˜‚ğ—»ğ—± ğ˜ğ—¿ğ˜‚ğ˜ğ—µ ğ—¶ğ—» ğŸ¯ ğ—ºğ—®ğ—¶ğ—» ğ˜€ğ—°ğ—²ğ—»ğ—®ğ—¿ğ—¶ğ—¼ğ˜€:\n1. near real-time: you can access it quite quickly\n2. delayed: you can access it after a considerable amount of time (e.g., one month)\n3. never: you have to label the data manually\n.\nğ—™ğ—¼ğ—¿ ğ˜‚ğ˜€ğ—² ğ—°ğ—®ğ˜€ğ—²ğ˜€ ğŸ®. ğ—®ğ—»ğ—± ğŸ¯. ğ˜†ğ—¼ğ˜‚ ğ—°ğ—®ğ—» ğ—¾ğ˜‚ğ—¶ğ—°ğ—¸ğ—¹ğ˜† ğ—°ğ—¼ğ—ºğ—½ğ˜‚ğ˜ğ—² ğ˜†ğ—¼ğ˜‚ğ—¿ ğ—ºğ—¼ğ—»ğ—¶ğ˜ğ—¼ğ—¿ğ—¶ğ—»ğ—´ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² ğ—¶ğ—» ğ˜ğ—µğ—² ğ—³ğ—¼ğ—¹ğ—¹ğ—¼ğ˜„ğ—¶ğ—»ğ—´ ğ˜„ğ—®ğ˜†:\n- store the model predictions and GT as soon as they are available (these 2 will be out of sync -> you can't compute the metrics right away)\n- build a DAG (e.g., using Airflow) that extracts the predictions & GT computes the metrics in batch mode and loads them into another storage (e.g., GCS)\n- use an orchestration tool to run the DAG in the following scenarios:\n1. scheduled: if the GT is available in near real-time (e.g., hourly), then it makes sense to run your monitoring pipeline based on the known frequency\n2. triggered: if the GT is delayed and you don't know when it may come up, then you can implement a webhook to trigger your monitoring pipeline\n- attach a consumer to your storage to use and display the metrics (e.g., trigger alarms and display them in a dashboard)\n.\nIf you want to see how to implement a near real-time monitoring pipeline using Airflow and GCS, check out my article:\nâ†³ğŸ”— ğ˜Œğ˜¯ğ˜´ğ˜¶ğ˜³ğ˜ªğ˜¯ğ˜¨ ğ˜›ğ˜³ğ˜¶ğ˜´ğ˜µğ˜¸ğ˜°ğ˜³ğ˜µğ˜©ğ˜º ğ˜”ğ˜“ ğ˜šğ˜ºğ˜´ğ˜µğ˜¦ğ˜®ğ˜´ ğ˜ğ˜ªğ˜µğ˜© ğ˜‹ğ˜¢ğ˜µğ˜¢ ğ˜ğ˜¢ğ˜­ğ˜ªğ˜¥ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜¢ğ˜¯ğ˜¥ ğ˜™ğ˜¦ğ˜¢ğ˜­-ğ˜›ğ˜ªğ˜®ğ˜¦ ğ˜”ğ˜°ğ˜¯ğ˜ªğ˜µğ˜°ğ˜³ğ˜ªğ˜¯ğ˜¨:\nhttps://lnkd.in/dhqCrGkD\nhashtag\n#\nmlops\nhashtag\n#\nmachinelearning\nhashtag\n#\ndatascience\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Highlight the top 6 ML Platform features crucial for developing and maintaining ML systems, emphasizing their role in ensuring reproducibility, collaboration, and operational efficiency.",
    "content": "ğ—§ğ—¼ğ—½ ğŸ² ğ— ğ—Ÿ ğ—£ğ—¹ğ—®ğ˜ğ—³ğ—¼ğ—¿ğ—º ğ—³ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—²ğ˜€ you must know and use in your ML system.\nHere they are â†“\n#ğŸ­. ğ—˜ğ˜…ğ—½ğ—²ğ—¿ğ—¶ğ—ºğ—²ğ—»ğ˜ ğ—§ğ—¿ğ—®ğ—°ğ—¸ğ—¶ğ—»ğ—´\nIn your ML development phase, you generate lots of experiments.\nTracking and comparing the metrics between them is crucial in finding the optimal model.\n#ğŸ®. ğ— ğ—²ğ˜ğ—®ğ—±ğ—®ğ˜ğ—® ğ—¦ğ˜ğ—¼ğ—¿ğ—²\nIts primary purpose is reproducibility.\nTo know how a model was generated, you need to know:\n- the version of the code\n- the version of the packages\n- hyperparameters/config\n- total compute\n- version of the dataset\n... and more\n#ğŸ¯. ğ—©ğ—¶ğ˜€ğ˜‚ğ—®ğ—¹ğ—¶ğ˜€ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€\nMost of the time, along with the metrics, you must log a set of visualizations for your experiment.\nSuch as:\n- images\n- videos\n- prompts\n- t-SNE graphs\n- 3D point clouds\n... and more\n#ğŸ°. ğ—¥ğ—²ğ—½ğ—¼ğ—¿ğ˜ğ˜€\nYou don't work in a vacuum.\nYou have to present your work to other colleges or clients.\nA report lets you take the metadata and visualizations from your experiment...\n...and create, deliver and share a targeted presentation for your clients or peers.\n#ğŸ±. ğ—”ğ—¿ğ˜ğ—¶ğ—³ğ—®ğ—°ğ˜ğ˜€\nThe most powerful feature out of them all.\nAn artifact is a versioned object that is an input or output for your task.\nEverything can be an artifact, but the most common cases are:\n- data\n- model\n- code\nWrapping your assets around an artifact ensures reproducibility.\nFor example, you wrap your features into an artifact (e.g., features:3.1.2), which you can consume into your ML development step.\nThe ML development step will generate config (e.g., config:1.2.4) and code (e.g., code:1.0.2) artifacts used in the continuous training pipeline.\nDoing so lets you quickly respond to questions such as \"What I used to generate the model?\" and \"What Version?\"\n#ğŸ². ğ— ğ—¼ğ—±ğ—²ğ—¹ ğ—¥ğ—²ğ—´ğ—¶ğ˜€ğ˜ğ—¿ğ˜†\nThe model registry is the ultimate way to make your model accessible to your production ecosystem.\nFor example, in your continuous training pipeline, after the model is trained, you load the weights as an artifact into the model registry (e.g., model:1.2.4).\nYou label this model as \"staging\" under a new version and prepare it for testing. If the tests pass, mark it as \"production\" under a new version and prepare it for deployment (e.g., model:2.1.5).\n.\nAll of these features are used in a mature ML system. What is your favorite one?\nYou can see all these features in action in my ğ—§ğ—µğ—² ğ—™ğ˜‚ğ—¹ğ—¹ ğ—¦ğ˜ğ—®ğ—°ğ—¸ ğŸ³-ğ—¦ğ˜ğ—²ğ—½ğ˜€ ğ— ğ—Ÿğ—¢ğ—½ğ˜€ ğ—™ğ—¿ğ—®ğ—ºğ—²ğ˜„ğ—¼ğ—¿ğ—¸ FREE course. Link in the comments â†“\nhashtag\n#\nmlops\nhashtag\n#\nmachinelearning\nhashtag\n#\ndata\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Explain the concept of chaining prompts to improve the cost-efficiency, accuracy, and debuggability of LLM applications, using analogies to illustrate the benefits of modular prompt design.",
    "content": "This is what you need to know about ğ—°ğ—µğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—½ğ—¿ğ—¼ğ—ºğ—½ğ˜ğ˜€ to ğ—¿ğ—²ğ—±ğ˜‚ğ—°ğ—² ğ—°ğ—¼ğ˜€ğ˜ğ˜€, ğ—¶ğ—»ğ—°ğ—¿ğ—²ğ—®ğ˜€ğ—² ğ—®ğ—°ğ—°ğ˜‚ğ—¿ğ—®ğ—°ğ˜†, ğ—®ğ—»ğ—± ğ—²ğ—®ğ˜€ğ—¶ğ—¹ğ˜† ğ—±ğ—²ğ—¯ğ˜‚ğ—´ your ğ—Ÿğ—Ÿğ—  ğ—®ğ—½ğ—½ğ—¹ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€.\nHere it is â†“\nğ—–ğ—µğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—½ğ—¿ğ—¼ğ—ºğ—½ğ˜ğ˜€ is an intuitive technique that states that you must split your prompts into multiple calls.\nğ—ªğ—µğ˜†? ğ—Ÿğ—²ğ˜'ğ˜€ ğ˜‚ğ—»ğ—±ğ—²ğ—¿ğ˜€ğ˜ğ—®ğ—»ğ—± ğ˜ğ—µğ—¶ğ˜€ ğ˜„ğ—¶ğ˜ğ—µ ğ˜€ğ—¼ğ—ºğ—² ğ—®ğ—»ğ—®ğ—¹ğ—¼ğ—´ğ—¶ğ—²ğ˜€.\nWhen cooking, you are following a recipe split into multiple steps. You want to move to the next step only when you know what you have done so far is correct.\nâ†³ You want every prompt to be simple & focused.\nAnother analogy is between reading all the code in one monolith/god class and using DRY to separate the logic between multiple modules.\nâ†³ You want to understand & debug every prompt easily.\n.\nChaining prompts is a ğ—½ğ—¼ğ˜„ğ—²ğ—¿ğ—³ğ˜‚ğ—¹ ğ˜ğ—¼ğ—¼ğ—¹ ğ—³ğ—¼ğ—¿ ğ—¯ğ˜‚ğ—¶ğ—¹ğ—±ğ—¶ğ—»ğ—´ ğ—® ğ˜€ğ˜ğ—®ğ˜ğ—²ğ—³ğ˜‚ğ—¹ ğ˜€ğ˜†ğ˜€ğ˜ğ—²ğ—º where you must take different actions depending on the current state.\nIn other words, you control what happens between 2 chained prompts.\nğ˜‰ğ˜ºğ˜±ğ˜³ğ˜°ğ˜¥ğ˜¶ğ˜¤ğ˜µğ˜´ ğ˜°ğ˜§ ğ˜¤ğ˜©ğ˜¢ğ˜ªğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜±ğ˜³ğ˜°ğ˜®ğ˜±ğ˜µğ˜´:\n- increase in accuracy\n- reduce the number of tokens -> lower costs (skips steps of the workflow when not needed)\n- avoid context limitations\n- easier to include a human-in-the-loop -> easier to control, moderate, test & debug\n- use external tools/plugins (web search, API, databases, calculator, etc.)\n.\nğ—˜ğ˜…ğ—®ğ—ºğ—½ğ—¹ğ—²\nYou want to build a virtual assistant to respond to customer service queries.\nInstead of adding in one single prompt the system message, all the available products, and the user inquiry, you can split it into the following:\n1. Use a prompt to extract the products and categories of interest.\n2. Enrich the context only with the products of interest.\n3. Call the LLM for the final answer.\nYou can evolve this example by adding another prompt that classifies the nature of the user inquiry. Based on that, redirect it to billing, technical support, account management, or a general LLM (similar to the complex system of GPT-4).\n.\nğ—§ğ—¼ ğ˜€ğ˜‚ğ—ºğ—ºğ—®ğ—¿ğ—¶ğ˜‡ğ—²:\nInstead of writing a giant prompt that includes multiple steps:\nSplit the god prompt into multiple modular prompts that let you keep track of the state externally and orchestrate the program.\nIn other words, you want modular prompts that you can combine easily (same as in writing standard functions/classes)\n.\nTo ğ—®ğ˜ƒğ—¼ğ—¶ğ—± ğ—¼ğ˜ƒğ—²ğ—¿ğ—²ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ğ—¶ğ—»ğ—´, use this technique when your prompt contains >= instruction.\nYou can leverage the DRY principle from software -> one prompt = one instruction.\nâ†³ Tools to chain prompts: LangChain\nâ†³ Tools to monitor and debug prompts: Comet LLMOps Tools\nLinks in the comments â†“\nhashtag\n#\nmachinelearning\nhashtag\n#\ngenerativeai\nhashtag\n#\ndeeplearning\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Promote the Hands-on LLMs course, emphasizing its comprehensive approach to building production-ready applications with LLMs and detailing the key components students will learn to develop and deploy.",
    "content": "Want to ğ˜€ğ˜ğ—®ğ—¿ğ˜ ğ—¹ğ—²ğ—®ğ—¿ğ—»ğ—¶ğ—»ğ—´ to ğ—¯ğ˜‚ğ—¶ğ—¹ğ—± ğ—½ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜ğ—¶ğ—¼ğ—»-ğ—¿ğ—²ğ—®ğ—±ğ˜† applications using ğ—Ÿğ—Ÿğ— ğ˜€? Then, I want to let you know that Pau and I ğ˜€ğ˜ğ—®ğ—¿ğ˜ğ—²ğ—± ğ—¿ğ—²ğ—¹ğ—²ğ—®ğ˜€ğ—¶ğ—»ğ—´ ğ˜ğ—µğ—² ğ˜ƒğ—¶ğ—±ğ—²ğ—¼ ğ—¹ğ—²ğ—°ğ˜ğ˜‚ğ—¿ğ—²ğ˜€ for the FREE ğ—›ğ—®ğ—»ğ—±ğ˜€-ğ—¢ğ—» ğ—Ÿğ—Ÿğ— ğ˜€ ğ—°ğ—¼ğ˜‚ğ—¿ğ˜€ğ—².\nThe Hands-On LLMs course is not just another demo of how to make a few predictions in a notebook.\nYou'll walk away with a ğ—³ğ˜‚ğ—¹ğ—¹ğ˜† ğ—¼ğ—½ğ—²ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ—®ğ—¹ ğ—½ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜, leveraging Large Language Models (LLMs) to build a chatbot for financial investment advice.\n=== ğ—ªğ—µğ—®ğ˜ ğ—¬ğ—¼ğ˜‚'ğ—¹ğ—¹ ğ—•ğ˜‚ğ—¶ğ—¹ğ—± ===\nWithin the course, you will leverage the ğŸ¯-ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² ğ—®ğ—¿ğ—°ğ—µğ—¶ğ˜ğ—²ğ—°ğ˜ğ˜‚ğ—¿ğ—², as follows:\nğŸ­. ğ—™ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—² ğ—£ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—²: You'll create a system to ingest real-time financial newsâ€”crucial for up-to-date advice.\nğŸ®. ğ—§ğ—¿ğ—®ğ—±ğ—¶ğ—»ğ—´ ğ—£ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—²: You'll fine-tune an LLM to specialize the model in making financial decisions.\nğŸ¯. ğ—œğ—»ğ—³ğ—²ğ—¿ğ—²ğ—»ğ—°ğ—² ğ—£ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—²: You'll combine all the components and deploy the model as a RESTful API, making your application accessible worldwide.\nThese pipelines will be independently developed, deployed, and scaled, ensuring modular and clean code.\nCheck it out â†“\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Discuss the advantages of Retrieval Augmented Generation (RAG) in LLM-powered applications, including overcoming limitations such as outdated knowledge, hallucinations, and lack of source references.",
    "content": "ğ—¥ğ—”ğ—š: ğ˜„ğ—µğ—®ğ˜ problems does it solve, and ğ—µğ—¼ğ˜„ it's integrated into ğ—Ÿğ—Ÿğ— -ğ—½ğ—¼ğ˜„ğ—²ğ—¿ğ—²ğ—± ğ—®ğ—½ğ—½ğ—¹ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€?\nLet's find out â†“\nRAG is a popular strategy when building LLMs to add external data to your prompt.\n=== ğ—£ğ—¿ğ—¼ğ—¯ğ—¹ğ—²ğ—º ===\nWorking with LLMs has 3 main issues:\n1. The world moves fast\nAn LLM learns an internal knowledge base. However, the issue is that its knowledge is limited to its training dataset.\nThe world moves fast. New data flows on the internet every second. Thus, the model's knowledge base can quickly become obsolete.\nOne solution is to fine-tune the model every minute or day...\nIf you have some billions to spend around, go for it.\n2. Hallucinations\nAn LLM  is full of testosterone and likes to be blindly confident.\nEven if the answer looks 100% legit, you can never fully trust it.\n3. Lack of reference links\nIt is hard to trust the response of the LLM if we can't see the source of its decisions.\nEspecially for important decisions (e.g., health, financials)\n=== ğ—¦ğ—¼ğ—¹ğ˜‚ğ˜ğ—¶ğ—¼ğ—» ===\nâ†’ Surprize! It is RAG.\n1. Avoid fine-tuning\nUsing RAG, you use the LLM as a reasoning engine and the external knowledge base as the main memory (e.g., vector DB).\nThe memory is volatile, so you can quickly introduce or remove data.\n2. Avoid hallucinations\nBy forcing the LLM to answer solely based on the given context, the LLM will provide an answer as follows:\n-  use the external data to respond to the user's question if it contains the necessary insights\n- \"I don't know\" if not\n3. Add reference links\nUsing RAG, you can easily track the source of the data and highlight it to the user.\n=== ğ—›ğ—¼ğ˜„ ğ—±ğ—¼ğ—²ğ˜€ ğ—¥ğ—”ğ—š ğ˜„ğ—¼ğ—¿ğ—¸? ===\nLet's say we want to use RAG to build a financial assistant.\nğ˜ğ˜©ğ˜¢ğ˜µ ğ˜¥ğ˜° ğ˜¸ğ˜¦ ğ˜¯ğ˜¦ğ˜¦ğ˜¥?\n- a data source with historical and real-time financial news (e.g. Alpaca)\n- a stream processing engine (e.g., Bytewax - ğŸ”—\nhttps://lnkd.in/dWJytkZ5\n)\n- an encoder-only model for embedding the documents (e.g., pick one from `sentence-transformers`)\n- a vector DB (e.g., Qdrant - ğŸ”—\nhttps://lnkd.in/d_FA9Bb3\n)\nğ˜ğ˜°ğ˜¸ ğ˜¥ğ˜°ğ˜¦ğ˜´ ğ˜ªğ˜µ ğ˜¸ğ˜°ğ˜³ğ˜¬?\nâ†³ On the feature pipeline side:\n1. using Bytewax, you ingest the financial news and clean them\n2. you chunk the news documents and embed them\n3. you insert the embedding of the docs along with their metadata (e.g., the initial text, source_url, etc.) to Qdrant\nâ†³ On the inference pipeline side:\n4. the user question is embedded (using the same embedding model)\n5. using this embedding, you extract the top K most similar news documents from Qdrant\n6. along with the user question, you inject the necessary metadata from the extracted top K documents into the prompt template (e.g., the text of documents & its source_url)\n7. you pass the whole prompt to the LLM for the final answer\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience"
  },
  {
    "instruction": "Introduce The Full Stack 7-Steps MLOps Framework course in a LinkedIN post, a comprehensive, free resource for learning MLE and MLOps with hands-on examples, covering the entire lifecycle of ML system development and deployment.",
    "content": "Want to learn ğ— ğ—Ÿğ—˜ & ğ— ğ—Ÿğ—¢ğ—½ğ˜€ in a ğ˜€ğ˜ğ—¿ğ˜‚ğ—°ğ˜ğ˜‚ğ—¿ğ—²ğ—± ğ˜„ğ—®ğ˜†, for ğ—³ğ—¿ğ—²ğ—², and with ğ—µğ—®ğ—»ğ—±ğ˜€-ğ—¼ğ—» ğ—²ğ˜…ğ—®ğ—ºğ—½ğ—¹ğ—²ğ˜€?\nThen you should check out my ğ—§ğ—µğ—² ğ—™ğ˜‚ğ—¹ğ—¹ ğ—¦ğ˜ğ—®ğ—°ğ—¸ ğŸ³-ğ—¦ğ˜ğ—²ğ—½ğ˜€ ğ— ğ—Ÿğ—¢ğ—½ğ˜€ ğ—™ğ—¿ğ—®ğ—ºğ—²ğ˜„ğ—¼ğ—¿ğ—¸ FREE course.\nIn ğŸ®.ğŸ± ğ—µğ—¼ğ˜‚ğ—¿ğ˜€ ğ—¼ğ—³ ğ—¿ğ—²ğ—®ğ—±ğ—¶ğ—»ğ—´ & ğ˜ƒğ—¶ğ—±ğ—²ğ—¼ ğ—ºğ—®ğ˜ğ—²ğ—¿ğ—¶ğ—®ğ—¹ğ˜€, you will ğ—¹ğ—²ğ—®ğ—¿ğ—» ğ—µğ—¼ğ˜„ ğ˜ğ—¼:\n- design a batch-serving architecture\n- use Hopsworks as a feature store\n- design a feature engineering pipeline that reads data from an API\n- build a training pipeline with hyper-parameter tunning\n- use W&B as an ML Platform to track your experiments, models, and metadata\n- implement a batch prediction pipeline\n- use Poetry to build your own Python packages\n- deploy your own private PyPi server\n- orchestrate everything with Airflow\n- use the predictions to code a web app using FastAPI and Streamlit\n- use Docker to containerize your code\n- use Great Expectations to ensure data validation and integrity\n- monitor the performance of the predictions over time\n- deploy everything to GCP\n- build a CI/CD pipeline using GitHub Actions\n- trade-offs & future improvements discussion\nâ€¦where all the pieces are integrated into a single end-to-end ML system that forecasts hourly energy levels across Denmark.\nğ—¬ğ—¼ğ˜‚ ğ—°ğ—®ğ—» ğ—®ğ—°ğ—°ğ—²ğ˜€ğ˜€ ğ˜ğ—µğ—² ğ—°ğ—¼ğ˜‚ğ—¿ğ˜€ğ—² ğ—¼ğ—»:\nâ ğ˜”ğ˜¦ğ˜¥ğ˜ªğ˜¶ğ˜®'ğ˜´ ğ˜›ğ˜‹ğ˜š ğ˜±ğ˜¶ğ˜£ğ˜­ğ˜ªğ˜¤ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯: text tutorials + videos\nâ ğ˜ğ˜ªğ˜µğ˜ğ˜¶ğ˜£: open-source code + docs\nI published the course on Medium's TDS publication to make it accessible to as many people as people. Thus â†“\n... anyone can learn the fundamentals of MLE & MLOps.\nSo no more excuses. Just go and build your own project ğŸ”¥\nCheck it out â†“\nâ†³ğŸ”— ğ˜›ğ˜©ğ˜¦ ğ˜ğ˜¶ğ˜­ğ˜­ ğ˜šğ˜µğ˜¢ğ˜¤ğ˜¬ 7-ğ˜šğ˜µğ˜¦ğ˜±ğ˜´ ğ˜”ğ˜“ğ˜–ğ˜±ğ˜´ ğ˜ğ˜³ğ˜¢ğ˜®ğ˜¦ğ˜¸ğ˜°ğ˜³ğ˜¬:\nhttps://lnkd.in/daShNdjw\nhashtag\n#\nmachinelearn"
  },
  {
    "instruction": "Explain the 3-pipeline architecture for ML systems in A LinkedIN post, highlighting its benefits for structure, modularity, collaboration, and efficient transition from development to production in MLOps practices.",
    "content": "We all know how ğ—ºğ—²ğ˜€ğ˜€ğ˜† ğ— ğ—Ÿ ğ˜€ğ˜†ğ˜€ğ˜ğ—²ğ—ºğ˜€ can get. That is where the ğŸ¯-ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² ğ—®ğ—¿ğ—°ğ—µğ—¶ğ˜ğ—²ğ—°ğ˜ğ˜‚ğ—¿ğ—² ğ—¸ğ—¶ğ—°ğ—¸ğ˜€ ğ—¶ğ—».\nThe 3-pipeline design is a way to bring structure & modularity to your ML system and improve your MLOps processes.\nThis is how â†“\n=== ğ—£ğ—¿ğ—¼ğ—¯ğ—¹ğ—²ğ—º ===\nDespite advances in MLOps tooling, transitioning from prototype to production remains challenging.\nIn 2022, only 54% of the models get into production. Auch.\nSo what happens?\nSometimes the model is not mature enough, sometimes there are some security risks, but most of the time...\n...the architecture of the ML system is built with research in mind, or the ML system becomes a massive monolith that is extremely hard to refactor from offline to online.\nSo, good processes and a well-defined architecture are as crucial as good tools and models.\n=== ğ—¦ğ—¼ğ—¹ğ˜‚ğ˜ğ—¶ğ—¼ğ—» ===\nğ˜›ğ˜©ğ˜¦ 3-ğ˜±ğ˜ªğ˜±ğ˜¦ğ˜­ğ˜ªğ˜¯ğ˜¦ ğ˜¢ğ˜³ğ˜¤ğ˜©ğ˜ªğ˜µğ˜¦ğ˜¤ğ˜µğ˜¶ğ˜³ğ˜¦.\nFirst, let's understand what the 3-pipeline design is.\nIt is a mental map that helps you simplify the development process and split your monolithic ML pipeline into 3 components:\n1. the feature pipeline\n2. the training pipeline\n3. the inference pipeline\n...also known as the Feature/Training/Inference (FTI) architecture.\n.\n#ğŸ­. The feature pipeline transforms your data into features & labels, which are stored and versioned in a feature store.\n#ğŸ®. The training pipeline ingests a specific version of the features & labels from the feature store and outputs the trained models, which are stored and versioned inside a model registry.\n#ğŸ¯. The inference pipeline takes a given version of the features and trained models and outputs the predictions to a client.\n.\nThis is why the 3-pipeline design is so beautiful:\n- it is intuitive\n- it brings structure, as on a higher level, all ML systems can be reduced to these 3 components\n- it defines a transparent interface between the 3 components, making it easier for multiple teams to collaborate\n- the ML system has been built with modularity in mind since the beginning\n- the 3 components can easily be divided between multiple teams (if necessary)\n- every component can use the best stack of technologies available for the job\n- every component can be deployed, scaled, and monitored independently\n- the feature pipeline can easily be either batch, streaming or both\nBut the most important benefit is that...\n...by following this pattern, you know 100% that your ML model will move out of your Notebooks into production.\n.\nWhat do you think about the 3-pipeline architecture? Have you used it?\nIf you want to know more about the 3-pipeline design, I recommend this awesome article from\nHopsworks\nâ†“\nâ†³ğŸ”— From MLOps to ML Systems with Feature/Training/Inference Pipelines:\nhttps://lnkd.in/dRnhHDdg\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\nğŸ’¡ Follow me for daily lessons about MLE and MLOps."
  },
  {
    "instruction": "For a LinkedIn update, highlight the collaboration with Alexandru RÄƒzvanÈ› in developing the Hands-on LLMs course, focusing on a financial assistant project. Invite feedback and discussions on GitHub, stressing the course's practical approach.",
    "content": "Want to learn how to ğ—³ğ—¶ğ—»ğ—²-ğ˜ğ˜‚ğ—»ğ—² ğ—®ğ—» ğ—Ÿğ—Ÿğ— , build a ğ˜€ğ˜ğ—¿ğ—²ğ—®ğ—ºğ—¶ğ—»ğ—´ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—², use a ğ˜ƒğ—²ğ—°ğ˜ğ—¼ğ—¿ ğ——ğ—•, build a ğ—³ğ—¶ğ—»ğ—®ğ—»ğ—°ğ—¶ğ—®ğ—¹ ğ—¯ğ—¼ğ˜ and ğ—±ğ—²ğ—½ğ—¹ğ—¼ğ˜† ğ—²ğ˜ƒğ—²ğ—¿ğ˜†ğ˜ğ—µğ—¶ğ—»ğ—´ using serverless solutions?\nThen maybe you know that I,\nPau Labarta Bajo\nand\nAlexandru RÄƒzvanÈ› ğŸ‘‹\n(one of the best MLEs I know out there in the wild) are working on our ğ—›ğ—®ğ—»ğ—±ğ˜€-ğ—¼ğ—» ğ—Ÿğ—Ÿğ— ğ˜€ ğ—³ğ—¿ğ—²ğ—² ğ—°ğ—¼ğ˜‚ğ—¿ğ˜€ğ—². If not, now you know.\nâ†’ The course will teach you how to build a ğ—³ğ—¶ğ—»ğ—®ğ—»ğ—°ğ—¶ğ—®ğ—¹ ğ—®ğ˜€ğ˜€ğ—¶ğ˜€ğ˜ğ—®ğ—»ğ˜ ğ—½ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜ powered by ğ—Ÿğ—Ÿğ— ğ˜€ leveraging the ğ— ğ—Ÿğ—¢ğ—½ğ˜€ ğŸ¯-ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² ğ—±ğ—²ğ˜€ğ—¶ğ—´ğ—».\nThe ğ—¼ğ˜‚ğ˜ğ—°ğ—¼ğ—ºğ—² of the product is a ğ—±ğ—²ğ—½ğ—¹ğ—¼ğ˜†ğ—²ğ—± ğ—½ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜ that you can already show off.\n... and not another Notebook.\n.\nAs the course is still a ğ˜„ğ—¼ğ—¿ğ—¸ ğ—¶ğ—» ğ—½ğ—¿ğ—¼ğ—´ğ—¿ğ—²ğ˜€ğ˜€, we want to ğ—¸ğ—²ğ—²ğ—½ ğ˜†ğ—¼ğ˜‚ ğ˜‚ğ—½ğ—±ğ—®ğ˜ğ—²ğ—± on our progress â†“\nâ†³ Thus, we opened up the ğ—±ğ—¶ğ˜€ğ—°ğ˜‚ğ˜€ğ˜€ğ—¶ğ—¼ğ—» ğ˜ğ—®ğ—¯ under the course's GitHub Repository, where we will ğ—¸ğ—²ğ—²ğ—½ ğ˜†ğ—¼ğ˜‚ ğ˜‚ğ—½ğ—±ğ—®ğ˜ğ—²ğ—± with everything is happening.\n.\nAlso, if you have any ğ—¶ğ—±ğ—²ğ—®ğ˜€, ğ˜€ğ˜‚ğ—´ğ—´ğ—²ğ˜€ğ˜ğ—¶ğ—¼ğ—»ğ˜€, ğ—¾ğ˜‚ğ—²ğ˜€ğ˜ğ—¶ğ—¼ğ—»ğ˜€ or want to ğ—°ğ—µğ—®ğ˜, we encourage you to ğ—°ğ—¿ğ—²ğ—®ğ˜ğ—² ğ—® \"ğ—»ğ—²ğ˜„ ğ—±ğ—¶ğ˜€ğ—°ğ˜‚ğ˜€ğ˜€ğ—¶ğ—¼ğ—»\".\nâ†“ We want the course to fill your real needs â†“\nâ†³ Hence, if your suggestion fits well with our hands-on course direction, we will consider implementing it.\n.\nCheck it out and leave a â­ if you like what you see:\nâ†³ğŸ”— Hands-on LLMs Course:\nhttps://lnkd.in/dKRmRgfZ\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Share insights on LinkedIn about prioritizing ML fundamentals over chasing the latest trends. Emphasize the value of a solid foundation for quickly adapting to new technologies in the ML and MLOps landscape.",
    "content": "If anyone told you that ğ— ğ—Ÿ or ğ— ğ—Ÿğ—¢ğ—½ğ˜€ is ğ—²ğ—®ğ˜€ğ˜†, they were ğ—¿ğ—¶ğ—´ğ—µğ˜.\nHere is a simple trick that I learned the hard way â†“\nIf you are in this domain, you already know that everything changes fast:\n- a new tool every month\n- a new model every week\n- a new project every day\nYou know what I did? I stopped caring about all these changes and switched my attention to the real gold.\nWhich is â†’ \"ğ—™ğ—¼ğ—°ğ˜‚ğ˜€ ğ—¼ğ—» ğ˜ğ—µğ—² ğ—³ğ˜‚ğ—»ğ—±ğ—®ğ—ºğ—²ğ—»ğ˜ğ—®ğ—¹ğ˜€.\"\n.\nLet me explain â†“\nWhen you constantly chase the latest models (aka FOMO),  you will only have a shallow understanding of that new information (except if you are a genius or already deep into that niche).\nBut the joke's on you. In reality, most of what you think you need to know, you don't.\nSo you won't use what you learned and forget most of it after 1-2 months.\nWhat a waste of time, right?\n.\nBut...\nIf you master the fundamentals of the topic, you want to learn.\nFor example, for deep learning, you have to know:\n- how models are built\n- how they are trained\n- groundbreaking architectures (Resnet, UNet, Transformers, etc.)\n- parallel training\n- deploying a model, etc.\n...when in need (e.g., you just moved on to a new project), you can easily pick up the latest research.\nThus, after you have laid the foundation, it is straightforward to learn SoTA approaches when needed (if needed).\nMost importantly, what you learn will stick with you, and you will have the flexibility to jump from one project to another quickly.\n.\nI am also guilty. I used to FOMO into all kinds of topics until I was honest with myself and admitted I am no Leonardo Da Vinci.\nBut here is what I did and worked well:\n- building projects\n- replicating the implementations of famous papers\n- teaching the subject I want to learn\n... and most importantly, take my time to relax and internalize the information.\n.\nTo conclude:\n- learn ahead only the fundamentals\n- learn the latest trend only when needed\nWhat is your learning strategy? Let me know in the comments â†“\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Introduce the Composite design pattern on LinkedIn as essential for ML engineers, particularly for structuring efficient ML pipelines. Explain its benefits in simplifying complex projects by combining processing steps.",
    "content": "This is my ğ—³ğ—®ğ˜ƒğ—¼ğ—¿ğ—¶ğ˜ğ—² ğ—±ğ—²ğ˜€ğ—¶ğ—´ğ—» ğ—½ğ—®ğ˜ğ˜ğ—²ğ—¿ğ—» that you must know as an ML engineer.\nMost ML engineers completely ignore software design patterns, but let me explain why you should know this one for your machine learning projects â†“\nI am talking about Composite.\nThe Composite pattern is a structural design pattern that helps you compose objects in a tree-like structure.\nLet me explain by starting with the problem.\nğ—£ğ—¿ğ—¼ğ—¯ğ—¹ğ—²ğ—º\nLet's say that you want to build an ML pipeline that performs object detection + tracking.\nYou can easily divide it into smaller pipelines, such as:\n1. preprocessing\n2. training | inference\n3. postprocessing\nAlso, these 3 pipelines, in their turn, are split into smaller components.\nLet's say that to speed up the ML pipeline. You want to run everything in parallel if possible.\nThus, depending on the use case, it would be best to have a module to compose components sequentially or in parallel.\nâŒ If you don't think this through, your code can quickly transform into spaghetti.\nğ—¦ğ—¼ğ—¹ğ˜‚ğ˜ğ—¶ğ—¼ğ—»\nâœ… Now, the Composite design pattern kicks in.\n-> ğ˜›ğ˜©ğ˜ªğ˜´ ğ˜ªğ˜´ ğ˜©ğ˜°ğ˜¸ ğ˜ºğ˜°ğ˜¶ ğ˜¤ğ˜¢ğ˜¯ ğ˜ªğ˜®ğ˜±ğ˜­ğ˜¦ğ˜®ğ˜¦ğ˜¯ğ˜µ ğ˜µğ˜©ğ˜¦ ğ˜”ğ˜“ ğ˜±ğ˜ªğ˜±ğ˜¦ğ˜­ğ˜ªğ˜¯ğ˜¦ ğ˜¢ğ˜£ğ˜°ğ˜·ğ˜¦ ğ˜¶ğ˜´ğ˜ªğ˜¯ğ˜¨ ğ˜µğ˜©ğ˜¦ ğ˜Šğ˜°ğ˜®ğ˜±ğ˜°ğ˜´ğ˜ªğ˜µğ˜¦ ğ˜±ğ˜¢ğ˜µğ˜µğ˜¦ğ˜³ğ˜¯:\n1. Define a standard interface for all the transformations. Let's call it \"Transformation.\"\n2. We create an abstract class called \"AtomicTransformation\" that inherits the \"Transformation\" interface for an atomic transformation.\n3. We implement an abstract class called \"CompositeTransformation\" for running multiple transformations. This class inherits the \"Transformation\" interface but also inputs a list of \"Transformation\" objects as input.\n4. Depending on how you want to call a sequence of transformations, you can inherit the \"CompositeTransformation\" interface and implement classes for:\n- \"SequenceTransformations\"\n- \"ParallelTransformations,\"\n- \"DistributedTransformations,\" etc.\n5. Now, when you want to implement a granular transformation (e.g., normalize the image). You implement the \"AtomicTransformation\" interface.\n6. When you want to glue multiple transformations together, you leverage the \"CompositeTransformation\" classes.\n7. When you call a \"CompositeTransformation\" under the hood, it calls the list of \"Transformation\" objects until it hits an \"AtomicTransformation\" object which will do the actual transformation.\nNote that because both the \"AtomicTransformation\" and \"CompositeTransformation\" inherit the \"Transformation\" interface, you can use them interchangeably, like LEGOs.\nThat is powerful.\nThat is why we all love Sklearn and their \"Pipeline\" interface ğŸ”¥\n.\nIf you want to know how to apply other software design patterns in MLE, I left in the comments an interesting article â†“\nhashtag\n#\nmachinelearning\nhashtag\n#\ndesignpattern\nhashtag\n#\nmlops\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Discuss on LinkedIn the importance of maintaining an updated vector database for RAG in LLM applications. Outline the components of a streaming pipeline essential for real-time updates, emphasizing the role of this pipeline in enhancing application performance.",
    "content": "To successfully use ğ—¥ğ—”ğ—š in your ğ—Ÿğ—Ÿğ—  ğ—®ğ—½ğ—½ğ—¹ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€, your ğ˜ƒğ—²ğ—°ğ˜ğ—¼ğ—¿ ğ——ğ—• must constantly be updated with the latest data.\nHere is how you can implement a ğ˜€ğ˜ğ—¿ğ—²ğ—®ğ—ºğ—¶ğ—»ğ—´ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² to keep your vector DB in sync with your datasets â†“\n.\nğ—¥ğ—”ğ—š is a popular strategy when building LLMs to add context to your prompt about your private datasets.\nLeveraging your domain data using RAG provides 2 significant benefits:\n- you don't need to fine-tune your model as often (or at all)\n- avoid hallucinations\n.\nOn the ğ—¯ğ—¼ğ˜ ğ˜€ğ—¶ğ—±ğ—², to implement RAG, you have to:\n3. Embed the user's question using an embedding model (e.g., BERT). Use the embedding to query your vector DB and find the most similar vectors using a distance function (e.g., cos similarity).\n4. Get the top N closest vectors and their metadata.\n5. Attach the extracted top N vectors metadata + the chat history to the input prompt.\n6. Pass the prompt to the LLM.\n7. Insert the user question + assistant answer to the chat history.\n.\nBut the question is, ğ—µğ—¼ğ˜„ do you ğ—¸ğ—²ğ—²ğ—½ ğ˜†ğ—¼ğ˜‚ğ—¿ ğ˜ƒğ—²ğ—°ğ˜ğ—¼ğ—¿ ğ——ğ—• ğ˜‚ğ—½ ğ˜ğ—¼ ğ—±ğ—®ğ˜ğ—² ğ˜„ğ—¶ğ˜ğ—µ ğ˜ğ—µğ—² ğ—¹ğ—®ğ˜ğ—²ğ˜€ğ˜ ğ—±ğ—®ğ˜ğ—®?\nâ†³ You need a real-time streaming pipeline.\nHow do you implement it?\nYou need 2 components:\nâ†³ A streaming processing framework. For example,Â BytewaxÂ is built in Rust for efficiency and exposes a Python interface for ease of use - you don't need Java to implement real-time pipelines anymore.\nğŸ”— Bytewax:\nhttps://lnkd.in/dbJDDvKB\nâ†³ A vector DB. For example,Â QdrantÂ provides a rich set of features and a seamless experience.\nğŸ”— Qdrant:\nhttps://qdrant.tech/\n.\nHere is an example of how to implement a streaming pipeline for financial news â†“\nğŸ­. Financial news data source (e.g., Alpaca):\nTo populate your vector DB, you need a historical API (e.g., RESTful API) to add data to your vector DB in batch mode between a desired [start_date, end_date] range. You can tweak the number of workers to parallelize this step as much as possible.\nâ†’ You run this once in the beginning.\nYou need the data exposed under a web socket to ingest news in real time. So, you'll be able to listen to the news and ingest it in your vector DB as soon as they are available.\nâ†’ Listens 24/7 for financial news.\nğŸ®. Build the streaming pipeline using Bytewax:\nImplement 2 input connectors for the 2 different types of APIs: RESTful API & web socket.\nThe rest of the steps can be shared between both connectors â†“\n- Clean financial news documents.\n- Chunk the documents.\n- Embed the documents (e.g., using Bert).\n- Insert the embedded documents + their metadata to the vector DB (e.g., Qdrant).\nğŸ¯-ğŸ³. When the users ask a financial question, you can leverage RAG with an up-to-date vector DB to search for the latest news in the industry.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning"
  },
  {
    "instruction": "On LinkedIn, announce the expansion to Twitter (X) for sharing content, reflecting on the platform choice to reach a broader audience. Invite followers to connect on Twitter, highlighting the move as a step towards accessible content distribution.",
    "content": "After 1 year, I finally decided to ğ˜€ğ˜ğ—®ğ—¿ğ˜ ğ—½ğ—¼ğ˜€ğ˜ğ—¶ğ—»ğ—´ ğ—¼ğ—» ğ—§ğ˜„ğ—¶ğ˜ğ˜ğ—²ğ—¿ or, as others like to call it, ğ—«.\nI took this decision because everybody has a different way of reading and interacting with their socials.\n...and I want everyone to enjoy my content on their favorite platform.\nIt took me a while to make this decision as I was not a Twitter user, but despite what people say, I started using it lately and enjoyed it.\nThus... It just made sense to start posting there, but I must warn you I don't have any followers ğŸ‘€\nI even bought that stu*** blue ticker to see that I am serious about this ğŸ˜‚\nSo...\nIf you like my content and you are a Twitter/X person â†“\nFollow me on Twitter/X:\nâ†³ ğŸ”—\nhttps://lnkd.in/d5ad8YSD\nâ†³ handler: @ğ—¶ğ˜‚ğ˜€ğ˜‡ğ˜ğ—¶ğ—»ğ—½ğ—®ğ˜‚ğ—¹\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience"
  },
  {
    "instruction": "Explain the integration of GitHub Actions and Docker for CI/CD pipelines in a LinkedIn post. Highlight their benefits for ML/MLOps projects, including automation, cost-effectiveness, and simplicity of setup.",
    "content": "This is how you can build a CI/CD pipeline using GitHub Actions and Docker in just a few lines of code.\nAs an ML/MLOps engineer, you should master serving models by building CI/CD pipelines.\nThe good news is that GitHub Actions + Docker simplifies building a CI/CD pipeline.\n.\nğ—ªğ—µğ˜†?\n- you can easily trigger jobs when merging various branches\n- the CI/CD jobs run on GitHub's VMs (free)\n- easy to implement: copy & paste pre-made templates + adding credentials\n.\nğ—™ğ—¼ğ—¿ ğ—²ğ˜…ğ—®ğ—ºğ—½ğ—¹ğ—², ğ˜ğ—µğ—¶ğ˜€ ğ—¶ğ˜€ ğ—µğ—¼ğ˜„ ğ˜†ğ—¼ğ˜‚ ğ—°ğ—®ğ—» ğ—¯ğ˜‚ğ—¶ğ—¹ğ—± ğ—® ğ—–ğ—œ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² ğ—¶ğ—» ğŸ¯ ğ˜€ğ—¶ğ—ºğ—½ğ—¹ğ—² ğ˜€ğ˜ğ—²ğ—½ğ˜€:\n#1. The CI pipeline is triggered when you merge your new feature branch into the main branch.\n#2. You log into the Docker Registry (or any other compatible registry such as ECR).\n#3. You build the image. Run your tests (if you have any), and if the tests pass, you push the image into the registry.\n.\nğ—§ğ—¼ ğ—¶ğ—ºğ—½ğ—¹ğ—²ğ—ºğ—²ğ—»ğ˜ ğ˜ğ—µğ—²ğ—º ğ˜‚ğ˜€ğ—¶ğ—»ğ—´ ğ—šğ—¶ğ˜ğ—›ğ˜‚ğ—¯ ğ—”ğ—°ğ˜ğ—¶ğ—¼ğ—»ğ˜€, ğ˜†ğ—¼ğ˜‚ ğ—µğ—®ğ˜ƒğ—² ğ˜ğ—¼:\n- Dockerize your code\n- search \"CI Template GitHub Actions\" on Google\n- copy-paste the template\n- add your Docker Registry credentials\n...and bam... you are done.\nEasy right? The steps are similar when building your CD pipeline (deploying the new image to production).\nIf you want to see how I used GitHub Actions to build & deploy an ML system to GCP, check out my article from the comments â†“\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ninfrastructure\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "On LinkedIn, outline methods for evaluating LLM outputs, focusing on structured answers, using LLM as a classifier, and generating multiple answers for selection. Stress the importance of automating this process for efficiency.",
    "content": "Here are 3 techniques you must know to evaluate your LLMs quickly.\nManually testing the output of your LLMs is a tedious and painful process â†’ you need to automate it.\nIn generative AI, most of the time, you cannot leverage standard metrics.\nThus, the real question is, how do you evaluate the outputs of an LLM?\nDepending on your problem, here is what you can do â†“\n#ğŸ­. ğ—¦ğ˜ğ—¿ğ˜‚ğ—°ğ˜ğ˜‚ğ—¿ğ—²ğ—± ğ—®ğ—»ğ˜€ğ˜„ğ—²ğ—¿ğ˜€ - ğ˜†ğ—¼ğ˜‚ ğ—¸ğ—»ğ—¼ğ˜„ ğ—²ğ˜…ğ—®ğ—°ğ˜ğ—¹ğ˜† ğ˜„ğ—µğ—®ğ˜ ğ˜†ğ—¼ğ˜‚ ğ˜„ğ—®ğ—»ğ˜ ğ˜ğ—¼ ğ—´ğ—²ğ˜\nEven if you use an LLM to generate text, you can ask it to generate a response in a structured format (e.g., JSON) that can be parsed.\nYou know exactly what you want (e.g., a list of products extracted from the user's question).\nThus, you can easily compare the generated and ideal answers using classic approaches.\nFor example, when extracting the list of products from the user's input, you can do the following:\n- check if the LLM outputs a valid JSON structure\n- use a classic method to compare the generated and real answers\n#ğŸ®. ğ—¡ğ—¼ \"ğ—¿ğ—¶ğ—´ğ—µğ˜\" ğ—®ğ—»ğ˜€ğ˜„ğ—²ğ—¿ (ğ—².ğ—´., ğ—´ğ—²ğ—»ğ—²ğ—¿ğ—®ğ˜ğ—¶ğ—»ğ—´ ğ—±ğ—²ğ˜€ğ—°ğ—¿ğ—¶ğ—½ğ˜ğ—¶ğ—¼ğ—»ğ˜€, ğ˜€ğ˜‚ğ—ºğ—ºğ—®ğ—¿ğ—¶ğ—²ğ˜€, ğ—²ğ˜ğ—°.)\nWhen generating sentences, the LLM can use different styles, words, etc. Thus, traditional metrics (e.g., BLUE score) are too rigid to be useful.\nYou can leverage another LLM to test the output of our initial LLM. The trick is in what questions to ask.\nWhen testing LLMs, you won't have a big testing split size as you are used to. A set of 10-100 tricky examples usually do the job (it won't be costly).\nHere, we have another 2 sub scenarios:\nâ†³ ğŸ®.ğŸ­ ğ—ªğ—µğ—²ğ—» ğ˜†ğ—¼ğ˜‚ ğ—±ğ—¼ğ—»'ğ˜ ğ—µğ—®ğ˜ƒğ—² ğ—®ğ—» ğ—¶ğ—±ğ—²ğ—®ğ—¹ ğ—®ğ—»ğ˜€ğ˜„ğ—²ğ—¿ ğ˜ğ—¼ ğ—°ğ—¼ğ—ºğ—½ğ—®ğ—¿ğ—² ğ˜ğ—µğ—² ğ—®ğ—»ğ˜€ğ˜„ğ—²ğ—¿ ğ˜ğ—¼ (ğ˜†ğ—¼ğ˜‚ ğ—±ğ—¼ğ—»'ğ˜ ğ—µğ—®ğ˜ƒğ—² ğ—´ğ—¿ğ—¼ğ˜‚ğ—»ğ—± ğ˜ğ—¿ğ˜‚ğ˜ğ—µ)\nYou don't have access to an expert to write an ideal answer for a given question to compare it to.\nBased on the initial prompt and generated answer, you can compile a set of questions and pass them to an LLM. Usually, these are Y/N questions that you can easily quantify and check the validity of the generated answer.\nThis is known as \"Rubric Evaluation\"\nFor example:\n\"\"\"\n- Is there any disagreement between the response and the context? (Y or N)\n- Count how many questions the user asked. (output a number)\n...\n\"\"\"\nThis strategy is intuitive, as you can ask the LLM any question you are interested in as long it can output a quantifiable answer (Y/N or a number).\nâ†³ ğŸ®.ğŸ®. ğ—ªğ—µğ—²ğ—» ğ˜†ğ—¼ğ˜‚ ğ—±ğ—¼ ğ—µğ—®ğ˜ƒğ—² ğ—®ğ—» ğ—¶ğ—±ğ—²ğ—®ğ—¹ ğ—®ğ—»ğ˜€ğ˜„ğ—²ğ—¿ ğ˜ğ—¼ ğ—°ğ—¼ğ—ºğ—½ğ—®ğ—¿ğ—² ğ˜ğ—µğ—² ğ—¿ğ—²ğ˜€ğ—½ğ—¼ğ—»ğ˜€ğ—² ğ˜ğ—¼ (ğ˜†ğ—¼ğ˜‚ ğ—µğ—®ğ˜ƒğ—² ğ—´ğ—¿ğ—¼ğ˜‚ğ—»ğ—± ğ˜ğ—¿ğ˜‚ğ˜ğ—µ)\nWhen you have access to an answer manually created by a group of experts, things are easier.\nYou will use an LLM to compare the generated and ideal answers based on semantics, not structure.\nFor example:\n\"\"\"\n(A) The submitted answer is a subset of the expert answer and entirely consistent.\n...\n(E) The answers differ, but these differences don't matter.\n\"\"\"\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning"
  },
  {
    "instruction": "Celebrate reaching 10k+ followers on LinkedIn by expressing gratitude and a commitment to continue sharing valuable ML & MLOps content. Highlight the milestone as an unexpected achievement and thank the community for their support.",
    "content": "I wasn't expecting this, but somehow it happened.\nI recently hit 10k+ followers on LinkedIn.\nI will be honest with you guys. That was my goal for the end of 2023.\nBut I don't mind ğŸ˜‚ This was a great surprise for me.\nI have to thank you guys for following me  ğŸ™\nThis motivates me to create more and better content to help you decode ML & MLOps concepts.\nHave a fantastic day, and see you tomorrow ğŸ”¥\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndata"
  },
  {
    "instruction": "Discuss the top 6 ML Platform features that enhance a mature ML system in a LinkedIn post. Highlight experiment tracking, metadata storage, visualizations, reports, artifacts, and model registry as essential components.",
    "content": "ğ—§ğ—¼ğ—½ ğŸ² ğ— ğ—Ÿ ğ—£ğ—¹ğ—®ğ˜ğ—³ğ—¼ğ—¿ğ—º ğ—³ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—²ğ˜€ you must know and use in your ML system.\nHere they are â†“\n#ğŸ­. ğ—˜ğ˜…ğ—½ğ—²ğ—¿ğ—¶ğ—ºğ—²ğ—»ğ˜ ğ—§ğ—¿ğ—®ğ—°ğ—¸ğ—¶ğ—»ğ—´\nIn your ML development phase, you generate lots of experiments.\nTracking and comparing the metrics between them is crucial in finding the optimal model.\n#ğŸ®. ğ— ğ—²ğ˜ğ—®ğ—±ğ—®ğ˜ğ—® ğ—¦ğ˜ğ—¼ğ—¿ğ—²\nIts primary purpose is reproducibility.\nTo know how a model was generated, you need to know:\n- the version of the code\n- the version of the packages\n- hyperparameters/config\n- total compute\n- version of the dataset\n... and more\n#ğŸ¯. ğ—©ğ—¶ğ˜€ğ˜‚ğ—®ğ—¹ğ—¶ğ˜€ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€\nMost of the time, along with the metrics, you must log a set of visualizations for your experiment.\nSuch as:\n- images\n- videos\n- prompts\n- t-SNE graphs\n- 3D point clouds\n... and more\n#ğŸ°. ğ—¥ğ—²ğ—½ğ—¼ğ—¿ğ˜ğ˜€\nYou don't work in a vacuum.\nYou have to present your work to other colleges or clients.\nA report lets you take the metadata and visualizations from your experiment...\n...and create, deliver and share a targeted presentation for your clients or peers.\n#ğŸ±. ğ—”ğ—¿ğ˜ğ—¶ğ—³ğ—®ğ—°ğ˜ğ˜€\nThe most powerful feature out of them all.\nAn artifact is a versioned object that is an input or output for your task.\nEverything can be an artifact, but the most common cases are:\n- data\n- model\n- code\nWrapping your assets around an artifact ensures reproducibility.\nFor example, you wrap your features into an artifact (e.g., features:3.1.2), which you can consume into your ML development step.\nThe ML development step will generate config (e.g., config:1.2.4) and code (e.g., code:1.0.2) artifacts used in the continuous training pipeline.\nDoing so lets you quickly respond to questions such as \"What I used to generate the model?\" and \"What Version?\"\n#ğŸ². ğ— ğ—¼ğ—±ğ—²ğ—¹ ğ—¥ğ—²ğ—´ğ—¶ğ˜€ğ˜ğ—¿ğ˜†\nThe model registry is the ultimate way to make your model accessible to your production ecosystem.\nFor example, in your continuous training pipeline, after the model is trained, you load the weights as an artifact into the model registry (e.g., model:1.2.4).\nYou label this model as \"staging\" under a new version and prepare it for testing. If the tests pass, mark it as \"production\" under a new version and prepare it for deployment (e.g., model:2.1.5).\n.\nAll of these features are used in a mature ML system. What is your favorite one?\nYou can see all these features in action in my ğ—§ğ—µğ—² ğ—™ğ˜‚ğ—¹ğ—¹ ğ—¦ğ˜ğ—®ğ—°ğ—¸ ğŸ³-ğ—¦ğ˜ğ—²ğ—½ğ˜€ ğ— ğ—Ÿğ—¢ğ—½ğ˜€ ğ—™ğ—¿ğ—®ğ—ºğ—²ğ˜„ğ—¼ğ—¿ğ—¸ FREE course. Link in the comments â†“\nhashtag\n#\nmlops\nhashtag\n#\nmachinelearning\nhashtag\n#\ndata\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Introduce the capability of embedding spreadsheets in Streamlit applications using Mito in a LinkedIn post. Highlight its benefits for data exploration and analysis within Streamlit dashboards, simplifying data manipulation.",
    "content": "Here is how you can ğ—²ğ—ºğ—¯ğ—²ğ—± ğ—® ğ˜€ğ—½ğ—¿ğ—²ğ—®ğ—±ğ˜€ğ—µğ—²ğ—²ğ˜ ğ—±ğ—¶ğ—¿ğ—²ğ—°ğ˜ğ—¹ğ˜† ğ—¶ğ—»ğ˜ğ—¼ ğ˜†ğ—¼ğ˜‚ğ—¿ ğ—¦ğ˜ğ—¿ğ—²ğ—®ğ—ºğ—¹ğ—¶ğ˜ ğ—®ğ—½ğ—½ğ—¹ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—» with just ğ—® ğ—³ğ—²ğ˜„ ğ—¹ğ—¶ğ—»ğ—²ğ˜€ ğ—¼ğ—³ ğ—°ğ—¼ğ—±ğ—².\nFor sure, you heard about\nTry Mito\n.\nğ˜ğ˜§ ğ˜¯ğ˜°ğ˜µ, ğ˜©ğ˜¦ğ˜³ğ˜¦ ğ˜ªğ˜´ ğ˜¢ ğ˜²ğ˜¶ğ˜ªğ˜¤ğ˜¬ ğ˜³ğ˜¦ğ˜¤ğ˜¢ğ˜±:\n\"Mito is a way to incorporate an Excel-like component into your beloved notebooks to explore data without writing any Python code (it writes it for you based on your interactions).\"\n.\nRecently, they introduced the same experience, but this time for Streamlit.\nNow you can add a spreadsheet in your dashboard with just a few lines of code:\n\"\"\"\nfrom mitosheet.streamlit.v1 import spreadsheet\n# ... rest of your streamlit app\nspreadsheet()\n\"\"\"\n.\nğ—›ğ—²ğ—¿ğ—² ğ—¶ğ˜€ ğ˜„ğ—µğ—®ğ˜ ğ—¶ğ˜ ğ—°ğ—®ğ—» ğ—±ğ—¼:\n- Import, clean, and transform datasets into a format required by the rest of the Streamlit app.\n- Do flexible data exploration and analysis.\n- Create Python scripts using Mito's code-gen capabilities.\n.\nAccess the docs in the comments â†“\nhashtag\n#\nmachinelearning\nhashtag\n#\ndatascience\nhashtag\n#\ndata\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Announce the launch of a personal site dedicated to MLE & MLOps resources on LinkedIn. Invite feedback on the work-in-progress site, aiming to create a central hub for learning and sharing knowledge in the field.",
    "content": "Looking for a hub where to ğ—¹ğ—²ğ—®ğ—¿ğ—» ğ—®ğ—¯ğ—¼ğ˜‚ğ˜ ğ— ğ—Ÿ ğ—²ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ğ—¶ğ—»ğ—´ ğ—®ğ—»ğ—± ğ— ğ—Ÿğ—¢ğ—½ğ˜€ ğ—³ğ—¿ğ—¼ğ—º ğ—¿ğ—²ğ—®ğ—¹-ğ˜„ğ—¼ğ—¿ğ—¹ğ—± ğ—²ğ˜…ğ—½ğ—²ğ—¿ğ—¶ğ—²ğ—»ğ—°ğ—²?\nThen, I want to let you know that I just launched my personal site, where I will constantly aggregate my:\n- courses\n- articles\n- talks\n...and more\nâ†’ Sweet part: Everything will revolve around MLE & MLOps\nIt is still a work in progress...\nBut please check it out and let me know what you think â†“\nYour opinion is deeply appreciated ğŸ™\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Discuss the strategic advantage of deploying ML models using a batch architecture initially in a LinkedIn post. Emphasize the approach for quick value delivery and feedback collection, positioning it as a foundation for more complex deployment strategies.",
    "content": "Why serving an ML model using a batch architecture is so powerful?\nWhen you first start deploying your ML model, you want an initial end-to-end flow as fast as possible.\nDoing so lets you quickly provide value, get feedback, and even collect data.\n.\nBut here is the catch...\nSuccessfully serving an ML model is tricky as you need many iterations to optimize your model to work in real-time:\n- low latency\n- high throughput\nInitially, serving your model in batch mode is like a hack.\nBy storing the model's predictions in dedicated storage, you automatically move your model from offline mode to a real-time online model.\nThus, you no longer have to care for your model's latency and throughput. The consumer will directly load the predictions from the given storage.\nğ“ğ¡ğğ¬ğ ğšğ«ğ ğ­ğ¡ğ ğ¦ğšğ¢ğ§ ğ¬ğ­ğğ©ğ¬ ğ¨ğŸ ğš ğ›ğšğ­ğœğ¡ ğšğ«ğœğ¡ğ¢ğ­ğğœğ­ğ®ğ«ğ:\n- extracts raw data from a real data source\n- clean, validate, and aggregate the raw data within a feature pipeline\n- load the cleaned data into a feature store\n- experiment to find the best model + transformations using the data from the feature store\n- upload the best model from the training pipeline into the model registry\n- inside a batch prediction pipeline, use the best model from the model registry to compute the predictions\n- store the predictions in some storage\n- the consumer will download the predictions from the storage\n- repeat the whole process hourly, daily, weekly, etc. (it depends on your context)\n.\nğ˜›ğ˜©ğ˜¦ ğ˜®ğ˜¢ğ˜ªğ˜¯ ğ˜¥ğ˜°ğ˜¸ğ˜¯ğ˜´ğ˜ªğ˜¥ğ˜¦ of deploying your model in batch mode is that the predictions will have a level of lag.\nFor example, in a recommender system, if you make your predictions daily, it won't capture a user's behavior in real-time, and it will update the predictions only at the end of the day.\nMoving to other architectures, such as request-response or streaming, will be natural after your system matures in batch mode.\n.\nSo remember, when you initially deploy your model, using a batch mode architecture will be your best shot for a good user experience.\nLet me know in the comments what your usual strategy to serve models is â†“\nhashtag\n#\ndata\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Share on LinkedIn how diffusion models are trained by learning to predict noise added to images. Highlight the process's significance in differentiating between image information and noise, contributing to the model's ability to generate accurate images.",
    "content": "Your quick guide on how diffusion models learn to predict your favorite images.\n.\nğ—¤ğ˜‚ğ—¶ğ—°ğ—¸ ğ—¿ğ—²ğ—ºğ—¶ğ—»ğ—±ğ—²ğ—¿!\nA diffusion model takes a noisy image as input and outputs the noise level from the image.\nAt inference time, you take the input image and subtract the predicted noise from it.\nAlso, A diffusion model is parameterized by a timestamp T that reflects the diffusion process from T to 0.\nThus, for different timestamps, it predicts different levels of noise.\nWhen the timestamp is near T, the model expects noisier images.\nAs it approaches 0, the expected noise level in the image is reduced.\n.\nğ—›ğ—²ğ—¿ğ—² ğ—¶ğ˜€ ğ˜ğ—µğ—² ğ˜ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—¹ğ—¼ğ—¼ğ—½ ğŸ‘‡\n1. Sample a training image from the dataset.\n1. Sample timestamp t from the interval [0, T], which determines the noise level.\n2. Sample the noise.\n3. Add the noise to the image based on the sample timestamp t.\n4. Pass it through the diffusion model, which predicts the noise from the image.\n5. Use an MSE loss to compare the predicted noise with the true one.\n6. Use backpropagation to update the model.\n6. Repeat!\nFollowing this training strategy, the model learns to differentiate between the actual information from an image (e.g., the features of a cat) and the noise.\n.\nTo summarize...\nTo train a diffusion model you:\n- add noise to an image based on timestamp t\n- the models learn to predict the noise from timestamp t\n- you use MSE as a loss to compare the real noise with the predicted noise\nhashtag\n#\nmachinelearning\nhashtag\n#\ngenerativeai\nhashtag\n#\nstablediffusion\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "On LinkedIn, share a personal experience with Hopsworks while developing The Full Stack 7-Steps MLOps Framework. Praise its features, documentation, and the support received, underscoring the platform's contribution to the course's success.",
    "content": "Don't know what ğ—™ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—² ğ—¦ğ˜ğ—¼ğ—¿ğ—² to use?\nI recommend you check out\nHopsworks\n.\nI had an excellent time using it while implementing ğ—§ğ—µğ—² ğ—™ğ˜‚ğ—¹ğ—¹ ğ—¦ğ˜ğ—®ğ—°ğ—¸ ğŸ³-ğ—¦ğ˜ğ—²ğ—½ğ˜€ ğ— ğ—Ÿğ—¢ğ—½ğ˜€ ğ—™ğ—¿ğ—®ğ—ºğ—²ğ˜„ğ—¼ğ—¿ğ—¸ free course.\nThey provide:\n- an intuitive Python package to interface with their platform\n- a robust documentation\n- all the features you need from a feature store\nThey are still a fast-growing company, so the tool is evolving, making it more robust & feature-rich.\nWhile developing the course, I didn't know the team or have any connections with them, but they were kind enough to approach me and send me a gift.\nI love the logo & branding on their items. ğŸ”¥\nI guess I have no other option but to drink this weekend ğŸ˜‚\nThank you,\nHopsworks\n!\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\nfeaturestore\n-----\nğŸ’¡ Follow me if you want to level up in designing ML systems using MLOps good practices."
  },
  {
    "instruction": "Describe integrating a real-time streaming data source into an ML system on LinkedIn. Detail the infrastructure and processes required, emphasizing the approach's scalability and efficiency in enhancing system capabilities.",
    "content": "ğ—¨ğ—»ğ—¶ğ—³ğ˜† ğ—¯ğ—®ğ˜ğ—°ğ—µ ğ—®ğ—»ğ—± ğ˜€ğ˜ğ—¿ğ—²ğ—®ğ—ºğ—¶ğ—»ğ—´ ğ— ğ—Ÿ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—²ğ˜€\nWhat happens if you want to introduce a real-time/streaming data source into your system?\nYou cry. Just kidding. It is a lot easier than it sounds.\nLet's get some context.\nUntil now, you used only a static data source to train your model & compute your features.\nBut you find out that your business wants to use real-time news feeds as features for your model.\nğ—ªğ—µğ—®ğ˜ ğ—±ğ—¼ ğ˜†ğ—¼ğ˜‚ ğ—±ğ—¼?\nYou have to implement 2 ğ˜®ğ˜¢ğ˜ªğ˜¯ ğ˜±ğ˜ªğ˜±ğ˜¦ğ˜­ğ˜ªğ˜¯ğ˜¦ğ˜´ ğ˜§ğ˜°ğ˜³ ğ˜ºğ˜°ğ˜¶ğ˜³ ğ˜¯ğ˜¦ğ˜¸ ğ˜´ğ˜µğ˜³ğ˜¦ğ˜¢ğ˜®ğ˜ªğ˜¯ğ˜¨ ğ˜ªğ˜¯ğ˜±ğ˜¶ğ˜µ ğ˜´ğ˜°ğ˜¶ğ˜³ğ˜¤ğ˜¦:\n#ğŸ­. One that will quickly transform the raw data into features and make them accessible into the feature store to be used by the production services.\n#ğŸ®. One that will store the raw data in the static raw data source (e.g., a warehouse) so it will be used later for experimentation and research.\nBefore ingesting into your system, the real-time data source might need an extra processing step to standardize and adapt the data format to your interface.\nA standard strategy for:\n#ğŸ­. Kafka as your streaming platform\n#ğŸ®. Flink/Kafka Streams as your streaming processing units\nFor step #2. most of the time, you will have access to out-of-the-box data connectors that quickly load the real-time data into your static data storage (e.g., from Kafka to an S3 bucket or Big Query data warehouse).\nTo conclude...\nTo add a streaming data source to your current infrastructure, you need the following:\n- Kafka\n- Flink/Kafka Streams\n- to move your streaming data source into your static one\n- to quickly compute features and load them into the feature store\nThus, it isn't hardâ€”just a lot of infrastructure to set up.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndata\n-----\nğŸ’¡ Follow me if you want to level up in designing ML systems using MLOps good practices."
  },
  {
    "instruction": "Reflect on participating in the Let's Talk AI podcast in a LinkedIn post. Share insights from the discussion on ML engineering and MLOps, highlighting topics like AI system design, job roles, and the use of Airflow in automation.",
    "content": "Are you into MLOps and ML Engineering?\nI was honored to pour my MLE & MLOps wisdom into the podcast ğ˜“ğ˜¦ğ˜µ'ğ˜´ ğ˜›ğ˜¢ğ˜­ğ˜¬ ğ˜ˆğ˜ hosted.\nI had a great time talking with\nThomas Bustos\n, where he had some fantastic questions about:\n- building and engineering AI systems\n- finding your niche in AI\n- different ML job positions\n- Airflow for automating ML\n- deploying multiple versions and communicating effectively\n- explaining technical complexity to customers\n... and more\nIf this sounds like something you are interested in, check it out ğŸ‘‡\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndata\n-----\nğŸ’¡ Follow me if you want to level up in designing ML systems using MLOps good practices."
  },
  {
    "instruction": "Discuss the importance of a model registry in decoupling experimental and production pipelines of ML systems on LinkedIn. Explain how it facilitates smooth transitions from development to deployment, enhancing system reliability.",
    "content": "ğ—¦ğ˜‚ğ—½ğ—²ğ—¿ğ—°ğ—µğ—®ğ—¿ğ—´ğ—² ğ˜†ğ—¼ğ˜‚ğ—¿ ğ— ğ—Ÿ ğ˜€ğ˜†ğ˜€ğ˜ğ—²ğ—º: ğ˜‚ğ˜€ğ—² ğ—® ğ—ºğ—¼ğ—±ğ—²ğ—¹ ğ—¿ğ—²ğ—´ğ—¶ğ˜€ğ˜ğ—¿ğ˜†\nA model registry is the holy grail of any production-ready ML system.\nThe model registry is the critical component that decouples your offline pipeline (experimental/research phase) from your production pipeline.\nğ—–ğ—¼ğ—ºğ—½ğ˜‚ğ˜ğ—² ğ—¢ğ—³ğ—³ğ—¹ğ—¶ğ—»ğ—² ğ—™ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—²ğ˜€\nUsually, when training your model, you use a static data source.\nUsing a feature engineering pipeline, you compute the necessary features used to train the model.\nThese features will be stored inside a features store.\nAfter processing your data, your training pipeline creates the training & testing splits and starts training the model.\nThe output of your training pipeline is the trained weights, also known as the model artifact.\nğ—›ğ—²ğ—¿ğ—² ğ—¶ğ˜€ ğ˜„ğ—µğ—²ğ—¿ğ—² ğ˜ğ—µğ—² ğ—ºğ—¼ğ—±ğ—²ğ—¹ ğ—¿ğ—²ğ—´ğ—¶ğ˜€ğ˜ğ—¿ğ˜† ğ—¸ğ—¶ğ—°ğ—¸ğ˜€ ğ—¶ğ—» ğŸ‘‡\nThis artifact will be pushed into the model registry under a new version that can easily be tracked.\nSince this point, the new model artifact version can be pulled by any serving strategy:\n#1. batch\n#2. request-response\n#3. streaming\nYour inference pipeline doesnâ€™t care how the model artifact was generated. It just has to know what model to use and how to transform the data into features.\nNote that this strategy is independent of the type of model & hardware you use:\n- classic model (Sklearn, XGboost),\n- distributed system (Spark),\n- deep learning model (PyTorch)\nTo summarize...\nUsing a model registry is a simple and effective method to:\n-> detach your experimentation from your production environment,\nregardless of what framework or hardware you use.\nTo learn more, check out my practical & detailed example of how to use a model registry in my article: ğ˜ˆ ğ˜ğ˜¶ğ˜ªğ˜¥ğ˜¦ ğ˜µğ˜° ğ˜‰ğ˜¶ğ˜ªğ˜­ğ˜¥ğ˜ªğ˜¯ğ˜¨ ğ˜Œğ˜§ğ˜§ğ˜¦ğ˜¤ğ˜µğ˜ªğ˜·ğ˜¦ ğ˜›ğ˜³ğ˜¢ğ˜ªğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜—ğ˜ªğ˜±ğ˜¦ğ˜­ğ˜ªğ˜¯ğ˜¦ğ˜´ ğ˜§ğ˜°ğ˜³ ğ˜”ğ˜¢ğ˜¹ğ˜ªğ˜®ğ˜¶ğ˜® ğ˜™ğ˜¦ğ˜´ğ˜¶ğ˜­ğ˜µğ˜´\nLink in the comments ğŸ‘‡\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndata\n-----\nğŸ’¡ Follow me if you want to level up in designing ML systems using MLOps good practices."
  },
  {
    "instruction": "Express pride in Romania's contributions to mathematics in a LinkedIn post. Share personal reflections on the country's achievements, emphasizing the impact on national pride and personal inspiration.",
    "content": "I usually donâ€™t care about someone's nationality, but damn, I am proud of my country Romania \uD83C\uDF89\\nhashtag\\n#\\nmathematics\\nhashtag\\n#\\nromania"
  },
  {
    "instruction": "On LinkedIn, share a strategy for managing information overload by describing a personalized 'second brain' system. Highlight the process from collection to storage, emphasizing the importance of organizing information for productivity and learning.",
    "content": "I never forget anything. Said no one but only your second brain.\nAfter 6+ months of refinement, this is my second brain strategy ğŸ‘‡\nTiago's Forte book inspired me, but I adapted his system to my needs.\n.\n#ğŸ¬. ğ—–ğ—¼ğ—¹ğ—¹ğ—²ğ—°ğ˜\nThis is where you are bombarded with information from all over the place.\n#ğŸ­. ğ—§ğ—µğ—² ğ—šğ—¿ğ—®ğ˜ƒğ—²ğ˜†ğ—®ğ—¿ğ—±\nThis is where I save everything that looks interesting.\nI won't use 90% of what is here, but it satisfied my urge to save that \"cool article\" I saw on LinkedIn.\nTools: Mostly Browser Bookmarks, but I rarely use GitHub stars, Medium lists, etc.\n#ğŸ®. ğ—§ğ—µğ—² ğ—•ğ—¼ğ—®ğ—¿ğ—±\nHere, I start converging the information and planning what to do next.\nTools: Notion\n#ğŸ¯. ğ—§ğ—µğ—² ğ—™ğ—¶ğ—²ğ—¹ğ—±\nHere is where I express myself through learning, coding, writing, etc.\nTools: whatever you need to express yourself.\n2 & 3 are iterative processes. Thus I often bounce between them until the information is distilled.\n#ğŸ°. ğ—§ğ—µğ—² ğ—ªğ—®ğ—¿ğ—²ğ—µğ—¼ğ˜‚ğ˜€ğ—²\nHere is where I take the distilled information and write it down for cold storage.\nTools: Notion, Google Drive\n.\nWhen I want to search for a piece of information, I start from the Warehouse and go backward until I find what I need.\nAs a minimalist, I  kept my tools to a minimum. I primarily use only: Brave, Notion, and Google Drive.\nYou don't need 100+ tools to be productive. They just want to take your money from you.\nSo remember...\nYou have to:\n- collect\n- link\n- plan\n- distill\n- store\nWhat is your second brain strategy? Leave your thoughts in the comments.\nhashtag\n#\nproductivity\nhashtag\n#\nsecondbrain\nhashtag\n#\nmachinelearning\n-----\nğŸ’¡ Follow me for weekly insights about designing ML systems."
  },
  {
    "instruction": "Invite LinkedIn followers to consider the implications of searching for oneself on ChatGPT. Spark a discussion on the role of generative AI in personal and professional discovery, questioning its potential in hiring and dating contexts.",
    "content": "Have you ever searched for yourself on ChatGPT?\nDo you think this is the next generation of hiring and dating?\nThis is what it returned after prompting: \"You are a private detective. Tell me everything you know about Paul Iusztin.\"\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\nchatgpt\n-----\nğŸ’¡ Follow me for weekly insights about designing ML systems."
  },
  {
    "instruction": "On LinkedIn, introduce the combination of FastAPI and Streamlit as essential tools for ML engineers. Discuss their synergy in enabling Python-centric development for deploying ML models across various architectures.",
    "content": "The Perfect DUO: FastAPI + Streamlit\n2 tools you should know as an ML Engineer\nHere are 2 reasons why FastAPI & Streamlit should be in your MLE stack ğŸ‘‡\n#ğŸ­. ğ—£ğ˜†ğ˜ğ—µğ—¼ğ—», ğ—£ğ˜†ğ˜ğ—µğ—¼ğ—», ğ—£ğ˜†ğ˜ğ—µğ—¼ğ—»!\nAs an MLE, Python is your magic wand.\nUsing FastAPI & Streamlit, you can build full-stack web apps using solely Python.\n#ğŸ®. ğ—˜ğ˜…ğ˜ğ—¿ğ—²ğ—ºğ—²ğ—¹ğ˜† ğ—³ğ—¹ğ—²ğ˜…ğ—¶ğ—¯ğ—¹ğ—²\nUsing FastAPI & Streamlit, you can deploy an ML model in almost any scenario.\n<< ğ˜‰ğ˜¢ğ˜µğ˜¤ğ˜© >>\nExpose the predictions from any storage, such as S3 or Redis, using FastAPI as REST endpoints.\nVisualize the predictions using Streamlit by calling the FastAPI REST endpoints.\n<< ğ˜™ğ˜¦ğ˜²ğ˜¶ğ˜¦ğ˜´ğ˜µ-ğ˜™ğ˜¦ğ˜´ğ˜±ğ˜°ğ˜¯ğ˜´ğ˜¦ >>\nWrap your model using FastAPI and expose its functionalities as REST endpoints.\nYet again... visualize the predictions using Streamlit by calling the FastAPI REST endpoints.\n<< ğ˜šğ˜µğ˜³ğ˜¦ğ˜¢ğ˜® >>\nWrap your model using FastAPI and expose it as REST endpoints.\nBut this time, the REST endpoints will be called from a Flink or Kafka Streams microservice.\n.\nUsing this tech stack won't be the most optimal solution in 100% use cases,\n... but in most cases:\n- it will get the job done\n- you can quickly prototype almost any ML application.\n.\nSo remember...\nYou should learn FastAPI & Streamlit because:\n- Python all the way!\n- you can quickly deploy a model in almost any architecture scenario\nDo you use FastAPI & Streamlit?\nTo learn more, check out my Medium article ğ˜ğ˜¢ğ˜´ğ˜µğ˜ˆğ˜—ğ˜ ğ˜¢ğ˜¯ğ˜¥ ğ˜šğ˜µğ˜³ğ˜¦ğ˜¢ğ˜®ğ˜­ğ˜ªğ˜µ: ğ˜›ğ˜©ğ˜¦ ğ˜—ğ˜ºğ˜µğ˜©ğ˜°ğ˜¯ ğ˜‹ğ˜¶ğ˜° ğ˜ ğ˜°ğ˜¶ ğ˜”ğ˜¶ğ˜´ğ˜µ ğ˜’ğ˜¯ğ˜°ğ˜¸ ğ˜ˆğ˜£ğ˜°ğ˜¶ğ˜µ. Link in the comments ğŸ‘‡\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\npython\n-----\nğŸ’¡ Follow me if you want to level up in designing and productionizing ML systems."
  },
  {
    "instruction": "Craft a LinkedIn post sharing how to efficiently use GitHub Actions and Docker to create CI/CD pipelines for ML/MLOps projects. Highlight the ease of setup and the benefits of automation and scalability offered by these tools.",
    "content": "Writing your own ML models is history.\nThe true value is in your data, how you prepare it, and your computer power.\nTo demonstrate my statement. Here is how you can write a Python script to train your LLM at scale in under 5 minutes â†“\n#ğŸ­. Load your data in JSON format and convert it into a Hugging Dataset\n#ğŸ®. Use Huggingface to load the LLM and pass it to the SFTTrainer, along with the tokenizer and training & evaluation datasets.\n#ğŸ¯. Wrap your training script with a serverless solution, such as Beam, which quickly lets you access a cluster of GPUs to train large models.\nğŸš¨ As you can see, the secret ingredients are not the LLM but:\n- the amount of data\n- the quality of data\n- how you process the data\n- $$$ for compute power\n- the ability to scale the system\n.\nğŸ’¡ My advice\nâ†³ If you don't plan to become an ML researcher, shift your focus from the latest models to your data and infrastructure.\n.\nğ—¡ğ—¼ğ˜ğ—²: Integrating serverless services, such as Beam, makes the deployment of your training pipeline fast & seamless, leaving you to focus only on the last piece of the puzzle: your data.\nâ†³ğŸ”— Check out Beam's docs to find out more:\nhttps://lnkd.in/dtu2MWSp\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Initiate a conversation on LinkedIn about the distinct roles within ML engineering, drawing from personal observations. Encourage sharing of experiences to explore how these roles intersect and contribute to the broader field of AI and ML.",
    "content": "ğ—ğ—¼ğ—¯ ğ—¿ğ—¼ğ—¹ğ—²ğ˜€ tell you there is just ğ—¼ğ—»ğ—² ğ˜ğ˜†ğ—½ğ—² ğ—¼ğ—³ ğ— ğ—Ÿ ğ—²ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ğ—¶ğ—»ğ—´, but there are ğ—®ğ—°ğ˜ğ˜‚ğ—®ğ—¹ğ—¹ğ˜† ğŸ¯\nHere they are â†“\nThese are the 3 ML engineering personas I found while working with different teams in the industry:\n#ğŸ­. ğ—¥ğ—²ğ˜€ğ—²ğ—®ğ—¿ğ—°ğ—µğ—²ğ—¿ğ˜€ ğ˜‚ğ—»ğ—±ğ—²ğ—¿ğ—°ğ—¼ğ˜ƒğ—²ğ—¿\nThey like to stay in touch with the latest papers, understand the architecture of models, optimize them, run experiments, etc.\nThey are great at picking the best models but not that great at writing clean code and scaling the solution.\n#ğŸ®. ğ—¦ğ—ªğ—˜ ğ˜‚ğ—»ğ—±ğ—²ğ—¿ğ—°ğ—¼ğ˜ƒğ—²ğ—¿\nThey pretend they read papers but don't (maybe only when they have to). They are more concerned with writing modular code and data quality than the latest hot models. Usually, these are the \"data-centric\" people.\nThey are great at writing clean code & processing data at scale but lack deep mathematical skills to develop complex DL solutions.\n#ğŸ¯. ğ— ğ—Ÿğ—¢ğ—½ğ˜€ ğ—³ğ—¿ğ—²ğ—®ğ—¸ğ˜€\nThey ultimately don't care about the latest research & hot models. They are more into the latest MLOps tools and building ML systems. They love to automate everything and use as many tools as possible.\nGreat at scaling the solution and building ML pipelines, but not great at running experiments & tweaking ML models. They love to treat the ML model as a black box.\n.\nI started as #1. , until I realized I hated it - now I am a mix of:\nâ†’ #ğŸ­. 20%\nâ†’ #ğŸ®. 40%\nâ†’ #ğŸ¯. 40%\nBut that doesn't mean one is better - these types are complementary.\nA great ML team should have at least one of each persona.\nWhat do you think? Did I get it right?\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Share a comparison between ML development environments and continuous training setups on LinkedIn, focusing on their distinct purposes and designs. Highlight the strategic importance of each in the lifecycle of ML system deployment.",
    "content": "What is the ğ—±ğ—¶ğ—³ğ—³ğ—²ğ—¿ğ—²ğ—»ğ—°ğ—² between your ğ— ğ—Ÿ ğ—±ğ—²ğ˜ƒğ—²ğ—¹ğ—¼ğ—½ğ—ºğ—²ğ—»ğ˜ and ğ—°ğ—¼ğ—»ğ˜ğ—¶ğ—»ğ˜‚ğ—¼ğ˜‚ğ˜€ ğ˜ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—²ğ—»ğ˜ƒğ—¶ğ—¿ğ—¼ğ—»ğ—ºğ—²ğ—»ğ˜ğ˜€?\nThey might do the same thing, but their design is entirely different â†“\nğ— ğ—Ÿ ğ——ğ—²ğ˜ƒğ—²ğ—¹ğ—¼ğ—½ğ—ºğ—²ğ—»ğ˜ ğ—˜ğ—»ğ˜ƒğ—¶ğ—¿ğ—¼ğ—»ğ—ºğ—²ğ—»ğ˜\nAt this point, your main goal is to ingest the raw and preprocessed data through versioned artifacts (or a feature store), analyze it & generate as many experiments as possible to find the best:\n- model\n- hyperparameters\n- augmentations\nBased on your business requirements, you must maximize some specific metrics, find the best latency-accuracy trade-offs, etc.\nYou will use an experiment tracker to compare all these experiments.\nAfter you settle on the best one, the output of your ML development environment will be:\n- a new version of the code\n- a new version of the configuration artifact\nHere is where the research happens. Thus, you need flexibility.\nThat is why we decouple it from the rest of the ML systems through artifacts (data, config, & code artifacts).\nğ—–ğ—¼ğ—»ğ˜ğ—¶ğ—»ğ˜‚ğ—¼ğ˜‚ğ˜€ ğ—§ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—˜ğ—»ğ˜ƒğ—¶ğ—¿ğ—¼ğ—»ğ—ºğ—²ğ—»ğ˜\nHere is where you want to take the data, code, and config artifacts and:\n- train the model on all the required data\n- output a staging versioned model artifact\n- test the staging model artifact\n- if the test passes, label it as the new production model artifact\n- deploy it to the inference services\nA common strategy is to build a CI/CD pipeline that (e.g., using GitHub Actions):\n- builds a docker image from the code artifact (e.g., triggered manually or when a new artifact version is created)\n- start the training pipeline inside the docker container that pulls the feature and config artifacts and outputs the staging model artifact\n- manually look over the training report -> If everything went fine, manually trigger the testing pipeline\n- manually look over the testing report -> if everything worked fine (e.g., the model is better than the previous one), manually trigger the CD pipeline that deploys the new model to your inference services\nNote how the model registry quickly helps you to decouple all the components.\nAlso, because training and testing metrics are not always black & white, it is tough to 100% automate the CI/CD pipeline.\nThus, you need a human in the loop when deploying ML models.\nTo conclude...\nThe ML development environment is where you do your research to find better models:\n- ğ˜ªğ˜¯ğ˜±ğ˜¶ğ˜µ: data artifact\n- ğ˜°ğ˜¶ğ˜µğ˜±ğ˜¶ğ˜µ: code & config artifacts\nThe continuous training environment is used to train & test the production model at scale:\n- ğ˜ªğ˜¯ğ˜±ğ˜¶ğ˜µ: data, code, config artifacts\n- ğ˜°ğ˜¶ğ˜µğ˜±ğ˜¶ğ˜µ: model artifact\nThis is not a fixed solution, as ML systems are still an open question. I would love to see your opinion in the comments. â†“\n.\nBut if you want to see this strategy in action,\nCheck out my ğ—§ğ—µğ—² ğ—™ğ˜‚ğ—¹ğ—¹ ğ—¦ğ˜ğ—®ğ—°ğ—¸ ğŸ³-ğ—¦ğ˜ğ—²ğ—½ğ˜€ ğ— ğ—Ÿğ—¢ğ—½ğ˜€ ğ—™ğ—¿ğ—®ğ—ºğ—²ğ˜„ğ—¼ğ—¿ğ—¸ FREE Course.\nLink in the comments â†“\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning"
  },
  {
    "instruction": "Discuss the versatility of transforming data with a feature store in an ML production system on LinkedIn. Highlight three innovative approaches to data transformation that enhance model performance and system efficiency.",
    "content": "These are 3 ways you didn't know about how you can transform your data when using a feature store.\nA feature store helps you quickly solve the training serving skew issue by offering you a consistent way to transform your data into features between the training and inference pipelines.\nThe issue boils down to WHEN you do the transformation.\nWhen using a feature store, there are 3 main ways you can transform your data:\nğŸ. ğğğŸğ¨ğ«ğ ğ¬ğ­ğ¨ğ«ğ¢ğ§ğ  ğ­ğ¡ğ ğğšğ­ğš ğ¢ğ§ ğ­ğ¡ğ ğŸğğšğ­ğ®ğ«ğ ğ¬ğ­ğ¨ğ«ğ\nIn the feature engineering pipeline, you do everything: clean, validate, aggregate, reduce, and transform your data.\nEven if this is the most intuitive way of doing things, it is the worse.\nğŸŸ¢ ultra-low latency\nğŸ”´ hard to do EDA on transformed data\nğŸ”´ store duplicated/redundant data\nğŸ. ğ’ğ­ğ¨ğ«ğ ğ­ğ¡ğ ğ­ğ«ğšğ§ğ¬ğŸğ¨ğ«ğ¦ğšğ­ğ¢ğ¨ğ§ ğ¢ğ§ ğ²ğ¨ğ®ğ« ğ©ğ¢ğ©ğğ¥ğ¢ğ§ğ ğ¨ğ« ğ¦ğ¨ğğğ¥ ğ©ğ«ğ-ğ©ğ«ğ¨ğœğğ¬ğ¬ğ¢ğ§ğ  ğ¥ğšğ²ğğ«ğ¬\nIn the feature engineering pipeline, you perform only the cleaning, validation, aggregations, and reduction steps.\nLater, by incorporating all your transformations into your pipeline object or pre-processing layers, you automatically save them along your model.\nThus, you can input your cleaned data into your pipeline, and it will know how to handle it.\nğŸŸ¢ store only cleaned data\nğŸŸ¢ easily explore your data\nğŸ”´ the transformations are done on the client\nğŸ‘. ğ˜ğ¨ğ® ğšğ­ğ­ğšğœğ¡ ğ­ğ¨ ğğ¯ğğ«ğ² ğœğ¥ğğšğ§ğğ ğğšğ­ğš ğ¬ğ¨ğ®ğ«ğœğ ğš ğ”ğƒğ… ğ­ğ«ğšğ§ğ¬ğŸğ¨ğ«ğ¦ğšğ­ğ¢ğ¨ğ§\nThis is similar to solution 2., but instead of attaching the transformation directly to your model, you attached them as a UDF to the feature store.\nfeature = cleaned data source + UDF\nSo when you request a feature, the feature store will automatically trigger the UDF on a server and return it.\nğŸŸ¢ store only cleaned data\nğŸŸ¢ easily explore your data\nğŸŸ¢ the transformations are done on the server\nğŸŸ¢ scalable (using Spark)\nğŸ”´ hard to implement\nAs a recap,\nThere are 3 ways you can perform your transformations to solve the train serving skew when using a feature store.\nWhat method do you think is the best?\n.\nâ†³ To see method #ğŸ®. in action\nCheck out my ğ˜ˆ ğ˜ğ˜¶ğ˜ªğ˜¥ğ˜¦ ğ˜µğ˜° ğ˜‰ğ˜¶ğ˜ªğ˜­ğ˜¥ğ˜ªğ˜¯ğ˜¨ ğ˜Œğ˜§ğ˜§ğ˜¦ğ˜¤ğ˜µğ˜ªğ˜·ğ˜¦ ğ˜›ğ˜³ğ˜¢ğ˜ªğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜—ğ˜ªğ˜±ğ˜¦ğ˜­ğ˜ªğ˜¯ğ˜¦ğ˜´ ğ˜§ğ˜°ğ˜³ ğ˜”ğ˜¢ğ˜¹ğ˜ªğ˜®ğ˜¶ğ˜® ğ˜™ğ˜¦ğ˜´ğ˜¶ğ˜­ğ˜µğ˜´ article\nLink in the comments â†“\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Outline a LinkedIn post explaining the method of chaining prompts to build a financial assistant using LLMs. Emphasize the systematic approach for enhancing reliability and user experience in LLM-powered applications.",
    "content": "ğŸ³ ğ˜€ğ˜ğ—²ğ—½ğ˜€ on how to ğ—°ğ—µğ—®ğ—¶ğ—» your ğ—½ğ—¿ğ—¼ğ—ºğ—½ğ˜ğ˜€ to build a production-ready ğ—³ğ—¶ğ—»ğ—®ğ—»ğ—°ğ—¶ğ—®ğ—¹ ğ—®ğ˜€ğ˜€ğ—¶ğ˜€ğ˜ğ—®ğ—»ğ˜ using ğ—Ÿğ—Ÿğ— ğ˜€ â†“\nWhen building LLM applications, you frequently have to divide your application into multiple steps & prompts, which are known as \"chaining prompts\".\nHere are 7 standard steps when building a financial assistant using LLMs (or any other assistant) â†“\nğ—¦ğ˜ğ—²ğ—½ ğŸ­: Check if the user's question is safe using OpenAI's Moderation API\nIf the user's query is safe, move to ğ—¦ğ˜ğ—²ğ—½ ğŸ® â†“\nğ—¦ğ˜ğ—²ğ—½ ğŸ®: Query your proprietary data (e.g., financial news) to enrich the prompt with fresh data & additional context.\nTo do so, you have to:\n- use an LM to embed the user's input\n- use the embedding to query your proprietary data stored in a vector DB\nğ˜•ğ˜°ğ˜µğ˜¦: You must use the same LM model to embed:\n- the data that will be stored in the vector DB\n- the user's question used to query the vector DB\nğ—¦ğ˜ğ—²ğ—½ ğŸ¯: Build the prompt using:\n- a predefined template\n- the user's question\n- extracted financial news as context\n- your conversation history as context\nğ—¦ğ˜ğ—²ğ—½ ğŸ°: Call the LLM\nğ—¦ğ˜ğ—²ğ—½ ğŸ±: Check if the assistant's answer is safe using the OpenAI's Moderation API.\nIf the assistant's answer is safe, move to ğ—¦ğ˜ğ—²ğ—½ ğŸ± â†“\nğ—¦ğ˜ğ—²ğ—½ ğŸ²: Use an LLM to check if the final answer is satisfactory.\nTo do so, you build a prompt using the following:\n- a validation predefined template\n- the user's initial question\n- the assistants answer\nThe LLM has to give a \"yes\" or \"no\" answer.\nThus, if it answers \"yes,\" we show the final answer to the user. Otherwise, we will return a predefined response, such as:\n\"Sorry, we couldn't answer your question because we don't have enough information.\"\nğ—¦ğ˜ğ—²ğ—½ ğŸ³: Add the user's question and assistant's answer to a history cache. Which will be used to enrich the following prompts with the current conversation.\nJust to remind you, the assistant should support a conversation. Thus, it needs to know what happened in the previous questions.\nâ†’ In practice, you usually keep only the latest N (question, answer) tuples or a conversation summary to keep your context length under control.\n.\nâ†³ If you want to see this strategy in action, check out our new FREE Hands-on LLMs course (work in progress) & give it a â­ to stay updated with its latest progress.\nLink in the comments â†“\nhashtag\n#\nmachinelearning\nhashtag\n#\ngenerativeai\nhashtag\n#\ndeeplearning\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "On LinkedIn, share strategies for ensuring the reliability and accuracy of LLM outputs. Discuss four methods that can be implemented to monitor and improve the quality of LLM responses, enhancing overall system performance.",
    "content": "Here are 4 ways to monitor and check the output prompts of any LLM to increase the reliability and accuracy of your system.\n#ğŸ­. ğ—¢ğ—½ğ—²ğ—»ğ—”ğ—£ğ—œ ğ— ğ—¼ğ—±ğ—²ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—”ğ—£ğ—œ\nYou can check whether the LLM's answer is harmful with a simple API call. It classifies the prompt as hate, harassment, self-harm, sexual, and violence.\nYou don't want your LLM to become a bully without knowing it.\n#ğŸ®. ğ—Ÿğ—Ÿğ— ğ—¢ğ—½ğ˜€: ğ— ğ—¼ğ—»ğ—¶ğ˜ğ—¼ğ—¿ ğ˜ğ—µğ—² ğ—½ğ—¿ğ—¼ğ—ºğ—½ğ˜ğ˜€\nOne part of LLMOps is to monitor, track, and see the lineage of all the prompts that come into & out of your system.\nYou can easily do that with Comet ML's LLMOps features. Link in the comments â†“\n#ğŸ¯. ğ—¨ğ˜€ğ—² ğ˜ğ—µğ—² ğ˜€ğ—®ğ—ºğ—² ğ—Ÿğ—Ÿğ—  ğ˜ğ—¼ ğ—°ğ—¹ğ—®ğ˜€ğ˜€ğ—¶ğ—³ğ˜† ğ˜ğ—µğ—² ğ—¼ğ˜‚ğ˜ğ—½ğ˜‚ğ˜ ğ—®ğ˜€ ğ˜€ğ—®ğ˜ğ—¶ğ˜€ğ—³ğ˜†ğ—¶ğ—»ğ—´ ğ—¼ğ—¿ ğ—»ğ—¼ğ˜\nAlong with generating text, an LLM can also be used as a classifier (without additional training).\nAfter all, outputting a class can still be considered text generation, right?\nTo do so, you have to:\n- write a system prompt: \"You are an assistant that evaluates ... respond with \"Y\" if the output is sufficient and \"N\" otherwise.\n- add the user question\n- add the LLM answer\n- add the additional context used by the LLM to generate the answers (e.g., a set of product information)\nâ†³ concatenate everything and pass it to the same LLM...\n... and vualÃ¡, you've built a monitoring system that constantly classifies the LLM's answers between satisfying or not.\n#ğŸ°. ğ—šğ—²ğ—»ğ—²ğ—¿ğ—®ğ˜ğ—² ğ—ºğ—¼ğ—¿ğ—² ğ—®ğ—»ğ˜€ğ˜„ğ—²ğ—¿ğ˜€ ğ—®ğ—»ğ—± ğ˜‚ğ˜€ğ—² ğ˜ğ—µğ—² ğ˜€ğ—®ğ—ºğ—² ğ—Ÿğ—Ÿğ—  ğ˜ğ—¼ ğ—½ğ—¶ğ—°ğ—¸ ğ˜ğ—µğ—² ğ—¯ğ—²ğ˜€ğ˜ ğ—®ğ—»ğ˜€ğ˜„ğ—²ğ—¿\nQuite self-explanatory.\nAnother option is letting the user pick the best option - a popular strategy for generating stuff.\nA big downside to this strategy is that it adds extra costs.\n.\nSo remember...\nThere are 4 ways to parse your LLM's outputs:\n1. use the OpenAI Moderation API\n2. log them to Comet ML\n3. build a Y/N satisfying classifier\n4. generate more options and pick the best\nHave you used any of these options? Let me know â†“\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning\n-----\nğŸ’¡  Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Summarize key ML monitoring resources in a LinkedIn post, following an extensive review of articles and materials. Highlight three standout sources that provide deep insights and practical examples for effectively monitoring ML systems.",
    "content": "In the last month, I read 100+ ML monitoring articles.\nI trimmed them for you to 3 key resources:\n1. A series of excellent articles made by\nArize AI\nthat will make you understand what ML monitoring is all about.\nâ†³ğŸ”—\nhttps://lnkd.in/dDVWRujh\n2. The\nEvidently AI\nBlog, where you can find answers to all your questions regarding ML monitoring.\nâ†³ğŸ”—\nhttps://lnkd.in/du35hWp2\n3. The monitoring hands-on examples hosted by\nDataTalksClub\nwill teach you how to implement an ML monitoring system.\nâ†³ğŸ”—\nhttps://lnkd.in/d4ziHhxH\nAfter wasting a lot of time reading other resources...\nUsing these 3 resources is a solid start for learning about monitoring ML systems.\nHave you tried them?\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Promote the adoption of Terraform for infrastructure management in ML projects on LinkedIn. Share personal savings and efficiencies gained from using Terraform, positioning it as a crucial tool for cost-effective and scalable ML deployments.",
    "content": "I was uselessly spending 1000$ dollars every month on cloud machines until I started using this tool ğŸ‘‡\nTerraform!\n.\nğ…ğ¢ğ«ğ¬ğ­, ğ¥ğğ­'ğ¬ ğ®ğ§ğğğ«ğ¬ğ­ğšğ§ğ ğ°ğ¡ğ² ğ°ğ ğ§ğğğ ğ“ğğ«ğ«ğšğŸğ¨ğ«ğ¦.\nWhen you want to deploy a software application, there are two main steps:\n1. Provisioning infrastructure\n2. Deploying applications\nA regular workflow would be that before deploying your applications or building your CI/CD pipelines, you manually go and spin up your, let's say, AWS machines.\nInitially, this workflow should be just fine, but there are two scenarios when it could get problematic.\n#1. Your infrastructure gets too big and complicated. Thus, it is cumbersome and might yield bugs in manually replicating it.\n#2. In the world of AI, there are many cases when you want to spin up a GPU machine to train your models, and afterward, you don't need it anymore. Thus, if you forget to close it, you will end up uselessly paying a lot of $$$.\nWith Terraform, you can solve both of these issues.\n.\nSo...\nğ–ğ¡ğšğ­ ğ¢ğ¬ ğ“ğğ«ğ«ğšğŸğ¨ğ«ğ¦?\nIt sits on the provisioning infrastructure layer as a: \"infrastructure as code\" tool that:\n- is declarative (you focus on the WHAT, not on the HOW)\n- automates and manages your infrastructure\n- is open source\nYeah... yeah... that sounds fancy. But ğ°ğ¡ğšğ­ ğœğšğ§ ğˆ ğğ¨ ğ°ğ¢ğ­ğ¡ ğ¢ğ­?\nLet's take AWS as an example, where you have to:\n- create a VPC\n- create AWS users and permissions\n- spin up EC2 machines\n- install programs (e.g., Docker)\n- create a K8s cluster\nUsing Terraform...\nYou can do all that just by providing a configuration file that reflects the state of your infrastructure.\nBasically, it helps you create all the infrastructure you need programmatically. Isn't that awesome?\n.\nIf you want to quickly understand Terraform enough to start using it in your own projects,\nâ†³ check out my 7-minute read article: ğ˜šğ˜µğ˜°ğ˜± ğ˜”ğ˜¢ğ˜¯ğ˜¶ğ˜¢ğ˜­ğ˜­ğ˜º ğ˜Šğ˜³ğ˜¦ğ˜¢ğ˜µğ˜ªğ˜¯ğ˜¨ ğ˜ ğ˜°ğ˜¶ğ˜³ ğ˜ˆğ˜ğ˜š ğ˜ğ˜¯ğ˜§ğ˜³ğ˜¢ğ˜´ğ˜µğ˜³ğ˜¶ğ˜¤ğ˜µğ˜¶ğ˜³ğ˜¦. ğ˜œğ˜´ğ˜¦ ğ˜›ğ˜¦ğ˜³ğ˜³ğ˜¢ğ˜§ğ˜°ğ˜³ğ˜®!\nLink from the comments â†“\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\nsoftwareengineering\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Introduce the concept of retry policies for enhancing code reliability in ML systems on LinkedIn. Highlight the use of the Tenacity Python package as a simple yet powerful solution for implementing customizable retry strategies.",
    "content": "One strategy that makes the ğ—±ğ—¶ğ—³ğ—³ğ—²ğ—¿ğ—²ğ—»ğ—°ğ—² ğ—¯ğ—²ğ˜ğ˜„ğ—²ğ—²ğ—» ğ—´ğ—¼ğ—¼ğ—± ğ—°ğ—¼ğ—±ğ—² ğ—®ğ—»ğ—± ğ—´ğ—¿ğ—²ğ—®ğ˜ ğ—°ğ—¼ğ—±ğ—² is adding ğ—¿ğ—²ğ˜ğ—¿ğ˜† ğ—½ğ—¼ğ—¹ğ—¶ğ—°ğ—¶ğ—²ğ˜€.\nTo manually implement them can get tedious and complicated.\nRetry policies are a must when you:\n- make calls to an external API\n- read from a queue, etc.\n.\nğ—¨ğ˜€ğ—¶ğ—»ğ—´ ğ˜ğ—µğ—² ğ—§ğ—²ğ—»ğ—®ğ—°ğ—¶ğ˜ğ˜† ğ—£ğ˜†ğ˜ğ—µğ—¼ğ—» ğ—½ğ—®ğ—°ğ—¸ğ—®ğ—´ğ—²...\nğ˜ ğ˜°ğ˜¶ ğ˜¤ğ˜¢ğ˜¯ ğ˜²ğ˜¶ğ˜ªğ˜¤ğ˜¬ğ˜­ğ˜º ğ˜¥ğ˜¦ğ˜¤ğ˜°ğ˜³ğ˜¢ğ˜µğ˜¦ ğ˜ºğ˜°ğ˜¶ğ˜³ ğ˜§ğ˜¶ğ˜¯ğ˜¤ğ˜µğ˜ªğ˜°ğ˜¯ğ˜´ ğ˜¢ğ˜¯ğ˜¥ ğ˜¢ğ˜¥ğ˜¥ ğ˜¤ğ˜¶ğ˜´ğ˜µğ˜°ğ˜®ğ˜ªğ˜»ğ˜¢ğ˜£ğ˜­ğ˜¦ ğ˜³ğ˜¦ğ˜µğ˜³ğ˜º ğ˜±ğ˜°ğ˜­ğ˜ªğ˜¤ğ˜ªğ˜¦ğ˜´, ğ˜´ğ˜¶ğ˜¤ğ˜© ğ˜¢ğ˜´:\n1. Add fixed and random wait times between multiple retries.\n2. Add a maximum number of attempts or computation time.\n3. Retry only when specific errors are thrown (or not thrown).\n... as you can see, you easily compose these policies between them.\nThe cherry on top is that you can access the statistics of the retries of a specific function:\n\"\nprint(raise_my_exception.retry.statistics)\n\"\n.\nWhat is your current strategy for adding retry policies to your Python code?\nhashtag\n#\nmachinelearning\nhashtag\n#\npython\nhashtag\n#\nsoftwareengineering\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps."
  }
]
